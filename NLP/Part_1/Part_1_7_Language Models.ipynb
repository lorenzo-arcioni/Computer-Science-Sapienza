{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.di.uniroma1.it/sites/all/themes/sapienza_bootstrap/logo.png' width=\"200\"/> \n",
    "\n",
    "\n",
    "# **Part_1_7_Language Models**\n",
    " \n",
    "In the evolving field of Natural Language Processing (NLP), **Language Models** are foundational in generating, understanding, and analyzing text. From classic statistical models like `n-grams` to advanced Large Language Models (`LLMs`) that power modern conversational AI, language models shape applications across search engines, chatbots, summarization, translation, and more. This tutorial focuses on **n-grams**, providing a foundational understanding of their structure and applications in language modeling, with hands-on exercises to illustrate their role in text generation and probability-based modeling.\n",
    " \n",
    "### **Objectives:**\n",
    "\n",
    "By the end of this notebook, Parham will:\n",
    "1. Gain an understanding of **n-gram language models**, their structure, and their role in basic text generation and probability-based language modeling.\n",
    "2. Explore the theoretical and practical aspects of **n-grams**. \n",
    "3. Dive into implementation of **n-grams**, and smoothing techniques.\n",
    "\n",
    "\n",
    "### **References**: \n",
    "- [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "- [https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/](https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/)\n",
    "- [https://tallinzen.net/media/readings/slp3_chapter3_ngrams.pdf](https://tallinzen.net/media/readings/slp3_chapter3_ngrams.pdf)\n",
    "\n",
    "### **Tutors**:\n",
    "- Professor Stefano Farali\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: Stefano.faralli@uniroma1.it\n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/stefano-faralli-b1183920/) \n",
    "- Professor Iacopo Masi\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: masi@di.uniroma1.it  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/iacopomasi/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/iacopomasi)  \n",
    "    \n",
    "### **Contributors**:\n",
    "- Parham Membari\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: p.membari96@gmail.com\n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/p-mem/)\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**:  [GitHub](https://github.com/parham075)\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ec/Medium_logo_Monogram.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Medium**: [Medium](https://medium.com/@p.membari96)\n",
    "\n",
    "### **Table of Contents:**\n",
    "1. Import Libraries\n",
    "2. Introduction to Language Modeling\n",
    "3. N-gram Models\n",
    "4. Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/p/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/p/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import spacy\n",
    "from loguru import logger\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import words\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling is the way of determining the probability of any sequence of words. Language modeling is used in various applications such as Speech Recognition, Spam filtering, etc. Language modeling is the key aim behind implementing many state-of-the-art Natural Language Processing models.\n",
    "\n",
    "### Methods of Language Modelling\n",
    "Two methods of Language Modeling:\n",
    "\n",
    "- **Statistical Language Modelling**: Statistical Language Modeling, or Language Modeling, is the development of probabilistic models that can predict the next word in the sequence given the words that precede. Examples such as N-gram language modeling.\n",
    "\n",
    "- **Neural Language Modeling**: Neural network methods are achieving better results than classical methods both on standalone language models and when models are incorporated into larger models on challenging tasks like speech recognition and machine translation. A way of performing a neural language model is through word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview and Theory\n",
    "\n",
    "An **N-gram** is a contiguous sequence of `n` items from a given text or speech sample, where the items can be letters, words, or even base pairs depending on the application. Typically, N-grams are extracted from a large corpus, providing insights into text patterns and dependencies.\n",
    "\n",
    "For instance, N-grams can be:\n",
    "- **Unigrams**: Individual words like “This\", “article\", “is\", “on\", and “NLP”\n",
    "- **Bigrams**: Word pairs like “This article\", “article is\", “is on\", and “on NLP”\n",
    "\n",
    "An **N-gram language model** estimates the likelihood of a word given a specific context or history. For example, a bigram model estimates the probability of each word given the previous word. The model's goal is to predict the next word, capturing dependencies and patterns in language sequences.\n",
    "\n",
    "**Calculating N-gram Probabilities**\n",
    "\n",
    "For example, in the sentence **“This article is on...”**, if we want to predict the probability that the next word is “NLP\", this can be represented as:\n",
    "$$p(\\text{“NLP”} | \\text{“This”}, \\text{“article”}, \\text{“is”}, \\text{“on”})$$\n",
    "This probability is part of a conditional probability chain that models the probability of each word in a sentence based on its predecessors.\n",
    "\n",
    "To generalize, the conditional probability of the n-th word given the preceding `n-1` words can be written as:\n",
    "$$P(W) = p(w_n | w_1, w_2, ..., w_{n-1})$$\n",
    "Using the **chain rule of probability**, this probability of a word sequence w_1, w_2, ..., w_n can be expanded as:\n",
    "$$P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, ..., w_{i-1})$$\n",
    "\n",
    "**Markov Assumptions and Simplified Models:**\n",
    "\n",
    "In practice, language models often simplify this calculation by applying **Markov assumptions**. This assumption posits that the probability of a word depends only on a limited history of previous words, rather than the entire sequence. Specifically, in an k-gram model, we assume that each word depends only on the previous `k` words.\n",
    "\n",
    "- For a **unigram model** (where `k = 0`), each word is considered independently:\n",
    "  $$P(w_1, w_2, ..., w_n) \\approx \\prod_{i=1}^{n} P(w_i)$$\n",
    "\n",
    "- For a **bigram model** (where `k = 1`), each word depends only on the immediately preceding word:\n",
    "  $$P(w_i | w_1, w_2, ..., w_{i-1}) \\approx P(w_i | w_{i-1})$$\n",
    "\n",
    "**Examples for unigram and bigram:**\n",
    "\n",
    "Consider a simple sentence: **\"The cat sat.\"**\n",
    "\n",
    "- **Unigram Model Example:**\n",
    "\n",
    "    Given the unigram formula:\n",
    "    $$P(\\text{\"The cat sat\"}) \\approx P(\\text{\"The\"}) \\times P(\\text{\"cat\"}) \\times P(\\text{\"sat\"})$$\n",
    "\n",
    "    Suppose we have the following unigram probabilities (estimated from a large text corpus):\n",
    "    $$ P(\\text{\"The\"}) = 0.3 $$\n",
    "    $$ P(\\text{\"cat\"}) = 0.1 $$\n",
    "    $$ P(\\text{\"sat\"}) = 0.05 $$\n",
    "\n",
    "    Then, the probability of the sentence \"The cat sat\" under the unigram model is:\n",
    "    $$P(\\text{\"The cat sat\"}) \\approx 0.3 \\times 0.1 \\times 0.05 = 0.0015$$\n",
    "\n",
    "- **Bigram Model Example**\n",
    "    Using the bigram formula:\n",
    "    $$P(\\text{\"The cat sat\"}) \\approx P(\\text{\"The\"}) \\times P(\\text{\"cat\"} | \\text{\"The\"}) \\times P(\\text{\"sat\"} | \\text{\"cat\"})$$\n",
    "\n",
    "    Assume the following bigram probabilities based on prior occurrences in a corpus:\n",
    "\n",
    "\n",
    "    $$ P(\\text{\"The\"}) = 0.3 \\quad \\text{(probability of \"The\" starting a sentence)} $$\n",
    "    $$ P(\\text{\"cat\"} | \\text{\"The\"}) = 0.4 \\quad \\text{(probability of \"cat\" following \"The\")} $$\n",
    "    $$ P(\\text{\"sat\"} | \\text{\"cat\"}) = 0.3 \\quad \\text{(probability of \"sat\" following \"cat\")} $$\n",
    "\n",
    "    Then, the probability of the sentence \"The cat sat\" under the bigram model is:\n",
    "    $$P(\\text{\"The cat sat\"}) \\approx 0.3 \\times 0.4 \\times 0.3 = 0.036$$\n",
    "\n",
    "\n",
    "\n",
    "By applying these assumptions, we make the model computationally feasible while still capturing relevant language patterns. This approach allows us to approximate word dependencies and make educated predictions, forming the basis of applications like autocomplete and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of N-Gram language modeling in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word: of\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(' '.join(reuters.words()))\n",
    "\n",
    "# Create trigrams\n",
    "tri_grams = list(trigrams(words))\n",
    "\n",
    "# Build a trigram model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurrence\n",
    "for w1, w2, w3 in tri_grams:\n",
    "    model[(w1, w2)][w3] += 1\n",
    "\n",
    "# Transform the counts into probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(w1, w2):\n",
    "    \"\"\"\n",
    "    Predicts the next word based on the previous two words using the trained trigram model.\n",
    "    Args:\n",
    "    w1 (str): The first word.\n",
    "    w2 (str): The second word.\n",
    "\n",
    "    Returns:\n",
    "    str: The predicted next word.\n",
    "    \"\"\"\n",
    "    next_word = model[w1, w2]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return \"No prediction available\"\n",
    "\n",
    "# Example usage\n",
    "print(\"Next Word:\", predict_next_word('the', 'cat'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating language models is essential for understanding how well they perform on tasks like text prediction, machine translation, and conversational AI. In language modeling, three fundamental metrics are widely used to assess the effectiveness and reliability of a model: **entropy**, **cross-entropy**, and **perplexity**. These metrics offer insight into a model’s accuracy, confidence, and the extent to which it captures linguistic patterns.\n",
    "\n",
    "#### 1. Entropy\n",
    "\n",
    "**Entropy**, introduced by Claude Shannon, is a measure of the unpredictability or “information content” within a given set of text. In the context of language models, entropy quantifies how much information is required to represent a sequence of words, based on the probability distribution of possible word sequences.\n",
    "\n",
    "The entropy `H(P)` of a probability distribution  `P` over possible words  `X` is calculated as:\n",
    "\n",
    "$$H(P) = -\\sum_{X} P(X) \\cdot \\log(P(X))$$\n",
    "\n",
    "where:\n",
    "- `P(X)`  is the probability of each word X in the vocabulary,\n",
    "- `log(P(X))` represents the information content of `X`, and\n",
    "- `H(P)` is always greater than or equal to zero, with higher values indicating higher uncertainty and complexity.\n",
    "\n",
    "In language modeling, lower entropy implies that the model has more certainty and requires fewer bits to represent the text. Higher entropy indicates greater unpredictability, suggesting that the text may contain more complex or diverse content.\n",
    "\n",
    "#### 2. Cross-Entropy\n",
    "\n",
    "**Cross-entropy** measures how well a language model’s predicted probability distribution aligns with the actual distribution of words in the test data. In other words, it evaluates how effectively the model represents unseen text based on its training.\n",
    "\n",
    "Cross-entropy  `H(P, Q)` between the actual probability distribution `P`  and the model's predicted distribution `Q`  is defined as:\n",
    "\n",
    "$$H(P, Q) = -\\sum_{X} P(X) \\cdot \\log(Q(X))$$\n",
    "\n",
    "This metric captures the “surprise” of the model when encountering new text. If the model’s predictions closely align with the real data, cross-entropy is low, indicating that the model successfully captures the language structure. Conversely, high cross-entropy suggests that the model has not accurately represented the test data. Cross-entropy is always greater than or equal to entropy, reflecting that the model's uncertainty cannot be less than the inherent uncertainty in the actual data.\n",
    "\n",
    "#### 3. Perplexity\n",
    "\n",
    "**Perplexity** is a metric commonly used to assess the performance of language models by measuring how “confused” or “uncertain” a model feels when predicting the next word in a sequence. Perplexity is the exponentiated form of cross-entropy, making it a more interpretable value that reflects how well the model can predict words in a sequence. Lower perplexity indicates better model performance.\n",
    "\n",
    "For a test set of  `N`  words, perplexity `PP(W)` is calculated as:\n",
    "\n",
    "$$PP(W) = 2^{H(P, Q)} = \\prod_{i=1}^{N} P(w_i \\mid w_{1}, \\dots, w_{i-1})^{-\\frac{1}{N}}$$\n",
    "\n",
    "where:\n",
    "- `P(w_i | w_{1}, ..., w_{i-1})` is the probability of word `w_i` given its preceding words,\n",
    "- `N`  is the total number of words in the test set.\n",
    "\n",
    "For example, consider predicting the sentence “Natural Language Processing.” If the model assigns probabilities to each word in the sequence based on preceding context, these probabilities are used to compute perplexity. A lower perplexity value indicates that the model is less “puzzled” about the sequence, signaling higher prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability and Smoothing Techniques\n",
    "\n",
    "For an n-gram model, the probability of a word  `w_n`  given its preceding words can be expressed as the conditional probability  $$P(w_n \\mid w_1, w_2, \\dots, w_{n-1})$$ \n",
    "This probability is calculated by dividing the count of the specific n-gram by the count of the (n-1)-gram prefix that precedes it:\n",
    "\n",
    "$$\n",
    "P(w_n \\mid w_{n-1}, \\dots, w_1) = \\frac{\\text{Count}(w_1, w_2, \\dots, w_n)}{\\text{Count}(w_1, w_2, \\dots, w_{n-1})}\n",
    "$$\n",
    "\n",
    "For example, in a bigram model (where  n = 2 \\)), the probability of a word  `w_2` following a word  `w_1` is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_2 \\mid w_1) = \\frac{\\text{Count}(w_1, w_2)}{\\text{Count}(w_1)}\n",
    "$$\n",
    "\n",
    ">\n",
    ">**Example:**\n",
    ">\n",
    "> Suppose we have the following sentence in our corpus:\n",
    ">\n",
    ">`“The cat sat on the mat.”`\n",
    ">\n",
    ">In a bigram model, we calculate the probability of each word following its immediate predecessor. Let's calculate the probability of the word “sat” following “cat.”\n",
    ">\n",
    "> - Count the Bigram (“cat”, “sat”): In this corpus, the sequence “cat sat” appears once, so Count(“cat sat”) = 1.\n",
    "> - Count the Unigram (“cat”): The word “cat” appears once as well, so Count(“cat”) = 1.\n",
    "> Using the formula for bigram probability:\n",
    "> $$P(\\text{sat} \\mid \\text{cat}) = \\frac{\\text{Count}(\\text{cat sat})}{\\text{Count}(\\text{cat})} = \\frac{1}{1} = 1.0$$\n",
    "\n",
    "This approach helps estimate the likelihood of a word appearing after a given context based on observed patterns in the data. However, if an n-gram is absent from the training data, this method assigns it a probability of zero, which can cause issues in tasks such as text generation and prediction.\n",
    "\n",
    "### Smoothing Techniques\n",
    "\n",
    "To handle zero probabilities and improve the generalization of the model, smoothing techniques adjust probability estimates for n-grams that are not present in the training data. Several common smoothing methods are used in language modeling:\n",
    "\n",
    "- **Laplace (Add-One) Smoothing**: Adds 1 to each count, ensuring that no probability is zero.\n",
    "- **Additive (Add-k) Smoothing**: Generalizes Laplace by adding a small constant  k \\), allowing for flexible adjustments.\n",
    "- **Good-Turing Smoothing**: Adjusts counts of observed events based on the counts of rare and unseen events, useful for infrequent n-grams.\n",
    "- **Kneser-Ney Smoothing**: Considers the diversity of contexts in which a word appears, providing a more sophisticated smoothing approach.\n",
    "\n",
    "Each of these techniques provides a way to handle unseen n-grams, improving the robustness and accuracy of language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Laplace (Add-One) Smoothing**\n",
    "\n",
    "Laplace Smoothing adds 1 to each count to ensure that no probability is zero, even for unseen bigrams. Here’s the formula and code:\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(w_2 \\mid w_1) = \\frac{\\text{Count}(w_1, w_2) + 1}{\\text{Count}(w_1) + V}\n",
    "$$\n",
    "\n",
    "where  `V` is the vocabulary size. To set up the example, let’s assume we have the following bigram counts and vocabulary for the phrase:\n",
    "\n",
    "`\"the cat sat on the mat\"`\n",
    "\n",
    "Then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample bigram counts\n",
    "bigram_counts = {\n",
    "    (\"the\", \"cat\"): 3,\n",
    "    (\"cat\", \"sat\"): 2,\n",
    "    (\"sat\", \"on\"): 1,\n",
    "    (\"on\", \"the\"): 1,\n",
    "    (\"the\", \"mat\"): 1\n",
    "}\n",
    "\n",
    "# Unigram counts (counts of individual words)\n",
    "unigram_counts = {\n",
    "    \"the\": 5,\n",
    "    \"cat\": 2,\n",
    "    \"sat\": 1,\n",
    "    \"on\": 1,\n",
    "    \"mat\": 1\n",
    "}\n",
    "\n",
    "# Total vocabulary size\n",
    "vocab_size = len(unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Smoothing (P(cat | the)): 0.4\n"
     ]
    }
   ],
   "source": [
    "def laplace_smoothing(w1, w2, bigram_counts, unigram_counts, vocab_size):\n",
    "    # Apply Laplace smoothing\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    unigram_count = unigram_counts.get(w1, 0)\n",
    "    smoothed_prob = (bigram_count + 1) / (unigram_count + vocab_size)\n",
    "    return smoothed_prob\n",
    "\n",
    "# Example\n",
    "print(\"Laplace Smoothing (P(cat | the)):\", laplace_smoothing(\"the\", \"cat\", bigram_counts, unigram_counts, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Additive Smoothing (Add-k)**\n",
    "\n",
    "Additive Smoothing generalizes Laplace by adding a constant  `k` , allowing more flexible adjustments. If  `k = 0.1` , the formula becomes:\n",
    "\n",
    "$$\n",
    "P_{\\text{Add-k}}(w_2 \\mid w_1) = \\frac{\\text{Count}(w_1, w_2) + k}{\\text{Count}(w_1) + k \\cdot V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additive Smoothing (P(cat | the)): 0.5636363636363636\n"
     ]
    }
   ],
   "source": [
    "def additive_smoothing(w1, w2, bigram_counts, unigram_counts, vocab_size, k=0.1):\n",
    "    # Apply Additive smoothing with parameter k\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    unigram_count = unigram_counts.get(w1, 0)\n",
    "    smoothed_prob = (bigram_count + k) / (unigram_count + k * vocab_size)\n",
    "    return smoothed_prob\n",
    "\n",
    "# Example with k=0.1\n",
    "print(\"Additive Smoothing (P(cat | the)):\", additive_smoothing(\"the\", \"cat\", bigram_counts, unigram_counts, vocab_size, k=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Good-Turing Smoothing**\n",
    "\n",
    "Good-Turing Smoothing estimates probabilities by adjusting counts of observed events based on the counts of rare and unseen events. Here’s how we calculate the adjusted count  `C^*` :\n",
    "\n",
    "$$\n",
    "C^* = (r+1) \\frac{N_{r+1}}{N_r}\n",
    "$$\n",
    "\n",
    "where  `r`  is the count of the n-gram,  `N_r`  is the number of n-grams with frequency  `r` , and  `N_{r+1}`  is the number with frequency  `r+1`.\n",
    "\n",
    "For simplicity, let’s define a function for Good-Turing smoothing using our bigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good-Turing Smoothing: {('the', 'cat'): 0.375, ('cat', 'sat'): 0.375, ('sat', 'on'): 0.08333333333333333, ('on', 'the'): 0.08333333333333333, ('the', 'mat'): 0.08333333333333333}\n"
     ]
    }
   ],
   "source": [
    "def good_turing_smoothing(bigram_counts):\n",
    "    # Calculate Good-Turing adjusted counts\n",
    "    count_of_counts = Counter(bigram_counts.values())\n",
    "    adjusted_counts = {}\n",
    "    \n",
    "    for bigram, count in bigram_counts.items():\n",
    "        if count + 1 in count_of_counts and count in count_of_counts:\n",
    "            adjusted_count = (count + 1) * (count_of_counts[count + 1] / count_of_counts[count])\n",
    "        else:\n",
    "            adjusted_count = count\n",
    "        adjusted_counts[bigram] = adjusted_count\n",
    "    \n",
    "    # Calculate probabilities with adjusted counts\n",
    "    total_bigrams = sum(adjusted_counts.values())\n",
    "    smoothed_probs = {bigram: count / total_bigrams for bigram, count in adjusted_counts.items()}\n",
    "    return smoothed_probs\n",
    "\n",
    "# Example\n",
    "print(\"Good-Turing Smoothing:\", good_turing_smoothing(bigram_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Kneser-Ney Smoothing**\n",
    "\n",
    "Kneser-Ney Smoothing is a more complex technique that takes into account the diversity of contexts in which a word appears. Here’s the formula:\n",
    "\n",
    "$$\n",
    "P_{\\text{KN}}(w_2 \\mid w_1) = \\max \\left( \\text{Count}(w_1, w_2) - d, 0 \\right) / \\text{Count}(w_1) + \\lambda(w_1) \\cdot P_{\\text{continuation}}(w_2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  `d` is a discount constant,\n",
    "-  `lambda(w_1)`  ensures the probabilities sum to 1,\n",
    "-  `P_continuation(w_2)`  is based on the probability of  `w_2`  appearing in new contexts.\n",
    "\n",
    "For simplicity, we’ll set a discount  `d` and calculate a continuation probability based on unique bigrams in which a word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney Smoothing (P(cat | the)): 0.51\n"
     ]
    }
   ],
   "source": [
    "def kneser_ney_smoothing(w1, w2, bigram_counts, unigram_counts, vocab_size, discount=0.75):\n",
    "    # Discounted probability for bigram\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    unigram_count = unigram_counts.get(w1, 0)\n",
    "    continuation_count = sum(1 for (prev_word, next_word) in bigram_counts if next_word == w2)\n",
    "    \n",
    "    # Discounted probability for seen bigrams\n",
    "    p_continuation = continuation_count / len(bigram_counts)\n",
    "    p_discounted = max(bigram_count - discount, 0) / unigram_count if unigram_count > 0 else 0\n",
    "    \n",
    "    # Normalization constant\n",
    "    lambda_w1 = (discount / unigram_count) * len([bigram for bigram in bigram_counts if bigram[0] == w1]) if unigram_count > 0 else 0\n",
    "    \n",
    "    # Final smoothed probability\n",
    "    smoothed_prob = p_discounted + lambda_w1 * p_continuation\n",
    "    return smoothed_prob\n",
    "\n",
    "# Example\n",
    "print(\"Kneser-Ney Smoothing (P(cat | the)):\", kneser_ney_smoothing(\"the\", \"cat\", bigram_counts, unigram_counts, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Closing Thoughts  \n",
    "\n",
    "Throughout this notebook, Parham has undertaken the following activities:  \n",
    "- Learned about methods if Language Modeling.\n",
    "- Gained an understanding of **n-gram language models**, their structure, and their role in text generation and probability-based modeling.  \n",
    "- Explored the theoretical and practical aspects of **n-grams**, delving into their use in NLP tasks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
