{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.di.uniroma1.it/sites/all/themes/sapienza_bootstrap/logo.png' width=\"200\"/> \n",
    "\n",
    "# Part_1_5_Words_Corpora_Text_Normalization\n",
    "In Natural Language Processing (NLP), key techniques such as tokenization, stemming, and sentence segmentation are fundamental for transforming raw text into a structured format that can be effectively analyzed by language models. Byte-Pair Encoding (BPE) tokenization helps handle rare words and out-of-vocabulary terms by breaking text into subword units. Stemming, using algorithms like the Porter Stemmer, reduces words to their root forms, ensuring consistency across the text for better analysis. Sentence segmentation is crucial for dividing text into meaningful sentences, allowing for more accurate processing in downstream tasks. These techniques play a vital role in preparing text for various NLP tasks, ensuring that language data is in a normalized and analyzable state.\n",
    "\n",
    "### **Objectives:**\n",
    "By the end of this notebook, Parham will have a thorough understanding of tokenization and its importance in NLP, specifically learning how to implement **Byte-Pair Encoding (BPE)** to handle words and subwords. He will explore the **Porter Stemmer**, gaining insight into how it reduces words to their base forms and why this is essential for text normalization. Additionally, Parham will learn to apply **Sentence Segmentation** to split text into meaningful sentences for deeper analysis. Through hands-on coding exercises, he will gain practical experience using these techniques, utilizing Python libraries like `NLTK` and `SpaCy` to prepare text for NLP tasks.\n",
    "\n",
    "### **References**:\n",
    "- [https://github.com/DolbyUUU/byte_pair_encoding_BPE_subword_tokenization_implementation_python](https://github.com/DolbyUUU/byte_pair_encoding_BPE_subword_tokenization_implementation_python)\n",
    "- [https://spacy.io/usage/spacy-101](https://spacy.io/usage/spacy-101)\n",
    "- [https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/)\n",
    "- [https://medium.com/@hsinhungw/understanding-byte-pair-encoding-fd196ebfe93f](https://medium.com/@hsinhungw/understanding-byte-pair-encoding-fd196ebfe93f)\n",
    "\n",
    "### **Tutors**:\n",
    "- Professor Stefano Farali\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: Stefano.faralli@uniroma1.it\n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/stefano-faralli-b1183920/) \n",
    "- Professor Iacopo Masi\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: masi@di.uniroma1.it  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/iacopomasi/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/iacopomasi)  \n",
    "    \n",
    "### **Contributors**:\n",
    "- Parham Membari\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: p.membari96@gmail.com\n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/p-mem/)\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**:  [GitHub](https://github.com/parham075)\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ec/Medium_logo_Monogram.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Medium**: [Medium](https://medium.com/@p.membari96)\n",
    "\n",
    "**Table of Contents:** \n",
    "1. Import Libraries\n",
    "2. Tokenization Techniques\n",
    "3. Text Normalization: Porter Stemmer\n",
    "4. Sentence Segmentation\n",
    "5. Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from loguru import logger\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import subprocess\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Techniques\n",
    "Byte-pair encoding was first introduced in 1994 as a simple data compression technique by iteratively replacing the most frequent pair of bytes in a sequence with a single, unused byte.\n",
    "Imagine Parham is reading a really big book, but some of the words are really long or tricky, and he might not know all of them. To make things easier, Parham can break the long words into smaller, simpler parts or pieces. This way, he can still understand the book without needing to know every single big word.\n",
    "\n",
    "Byte-Pair Encoding (BPE) is a popular technique used in natural language processing for tokenizing text into subword units. The idea is to break down words into smaller, more frequent pieces, allowing models to efficiently handle rare or unknown words. BPE is especially useful in scenarios where the vocabulary is limited but the text contains a large variety of words, including compound or out-of-vocabulary words.\n",
    "\n",
    "The algorithm works by:\n",
    "- Starting with a sequence of individual characters.\n",
    "- Finding the most frequent pair of consecutive characters (or subwords).\n",
    "- Merging this pair into a single token.\n",
    "- Repeating the process until a predefined number of merges or a desired vocabulary size is reached.\n",
    "\n",
    "This process allows models to represent both common words and subwords effectively, making it easier to process any text.\n",
    "\n",
    "In the cell below Parham will an example of BEP tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After merge 1:\n",
      "Best Pair: ('s', '</w>')\n",
      "Updated Vocabulary: {'T o k e n i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k i n g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u e n c e </w>': 1, 't e x t </w>': 2, 'i n t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k e n s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v e n </w>': 1, 'i n d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t e n </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'i n </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'e n t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's e n t i m e n t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t i n g </w>': 1, 't o k e n s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'i n p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s e n t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h i n e </w>': 1, 'l e a r n i n g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 2:\n",
      "Best Pair: ('e', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k i n g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e </w>': 1, 't e x t </w>': 2, 'i n t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'i n d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'i n </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t i n g </w>': 1, 't o k en s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'i n p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h i n e </w>': 1, 'l e a r n i n g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 3:\n",
      "Best Pair: ('i', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e </w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e </w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e </w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e </w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e </w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e </w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e </w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 4:\n",
      "Best Pair: ('e', '</w>')\n",
      "Updated Vocabulary: {'T o k en i z a t i o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n , </w>': 1, 'n a m e d </w>': 1, 'en t i t y </w>': 1, 'r e c o g n i t i o n , </w>': 1, 'a n d </w>': 1, 's en t i m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a t i o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 5:\n",
      "Best Pair: ('t', 'i')\n",
      "Updated Vocabulary: {'T o k en i z a ti o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in t o </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 't o k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti o n , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti o n , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 't o </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c t o r i z a ti o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 6:\n",
      "Best Pair: ('t', 'o')\n",
      "Updated Vocabulary: {'T o k en i z a ti o n </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti o n , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti o n , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti o n , </w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti o n s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 7:\n",
      "Best Pair: ('o', 'n')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m a l l e r </w>': 1, 'u n i t s</w>': 1, 'c a l l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u a l </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r a l </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n a l y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti on , </w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c a l </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 8:\n",
      "Best Pair: ('a', 'l')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s , </w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s , </w>': 1, 'p h r a s e s , </w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on , </w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on , </w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s , </w>': 1, 'v e c to r i z a ti on , </w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 9:\n",
      "Best Pair: (',', '</w>')\n",
      "Updated Vocabulary: {'T o k en i z a ti on </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s ,</w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a ti on ,</w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i ti on ,</w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s ,</w>': 1, 'v e c to r i z a ti on ,</w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a ti on s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n",
      "\n",
      "After merge 10:\n",
      "Best Pair: ('ti', 'on')\n",
      "Updated Vocabulary: {'T o k en i z a tion </w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f </w>': 2, 'b r e a k in g </w>': 1, 'd o w n </w>': 1, 'a </w>': 1, 's e q u en c e</w>': 1, 't e x t </w>': 2, 'in to </w>': 2, 's m al l e r </w>': 1, 'u n i t s</w>': 1, 'c al l e d </w>': 1, 'to k en s ,</w>': 1, 'w h i c h </w>': 1, 'c a n </w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r </w>': 1, 'e v en </w>': 1, 'in d i v i d u al </w>': 1, 'c h a r a c t e r s . </w>': 1, 'o f t en </w>': 1, 'f i r s t </w>': 1, 's t e p </w>': 1, 'in </w>': 1, 'n a t u r al </w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s in g </w>': 2, 't a s k s</w>': 1, 's u c h </w>': 2, 'a s</w>': 3, 'c l a s s i f i c a tion ,</w>': 1, 'n a m e d </w>': 1, 'en ti t y </w>': 1, 'r e c o g n i tion ,</w>': 1, 'a n d </w>': 1, 's en ti m en t </w>': 1, 'a n al y s i s . </w>': 1, 'T h e</w>': 1, 'r e s u l t in g </w>': 1, 'to k en s</w>': 2, 'a r e</w>': 2, 't y p i c al l y </w>': 1, 'u s e d </w>': 1, 'in p u t </w>': 1, 'to </w>': 2, 'f u r t h e r </w>': 1, 's t e p s ,</w>': 1, 'v e c to r i z a tion ,</w>': 1, 'w h e r e</w>': 1, 'c on v e r t e d </w>': 1, 'n u m e r i c al </w>': 1, 'r e p r e s en t a tion s</w>': 1, 'f o r </w>': 1, 'm a c h in e</w>': 1, 'l e a r n in g </w>': 1, 'm o d e l s</w>': 1, 'u s e . </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a\n",
    "    dictionary of tuples representing the frequency count of pairs of characters\n",
    "    in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    Given a pair of characters and a vocabulary, returns a new vocabulary with the\n",
    "    pair of characters merged together wherever they appear.\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(\"\".join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "\n",
    "def get_vocab(data):\n",
    "    \"\"\"\n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency\n",
    "    count in the data.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def byte_pair_encoding(data, n):\n",
    "    \"\"\"\n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
    "    of characters found in the vocabulary of the input data.\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(n):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(f\"\\nAfter merge {i + 1}:\")\n",
    "        print(f\"Best Pair: {best}\")\n",
    "        print(f\"Updated Vocabulary: {vocab}\")\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "corpus = \"\"\"Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.\"\"\"\n",
    "data = corpus.split(\" \")\n",
    "\n",
    "n = 10\n",
    "bpe_pairs = byte_pair_encoding(data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    " The algorithm for Byte-Pair Encoding (BPE) in the previous example is a simplified version for educational purposes. While it illustrates the core concept of BPE, it might not be efficient for processing large datasets or in production environments. Here are some reasons why the naive implementation could be problematic with big data.\n",
    " 1. **Computational Complexity Counting Pairs:** The algorithm counts pairs of characters for every iteration, which can become computationally expensive as the number of tokens increases. For large datasets, this might lead to significant slowdowns.\n",
    "Repeated Iteration: The naive implementation involves multiple passes over the data to find the most frequent pairs, which can be inefficient when working with massive corpora.\n",
    "2. **Memory Usage\n",
    "Storage of Vocabulary:** The implementation keeps an entire vocabulary in memory, which can become unmanageable with large datasets. Each unique token needs to be stored, and the size of this vocabulary can grow quickly.\n",
    "3. **Inefficiency in Merging\n",
    "String Manipulation:** Frequent string replacements can be slow in Python. The current method creates new strings for each merge operation, leading to increased overhead.\n",
    "4. **Scalability Issues\n",
    "Lack of Parallelism:** The algorithm does not leverage parallel processing. For big data, parallelizing tasks can significantly reduce processing time.\n",
    "More Efficient Approaches\n",
    "For working with large datasets, here are a few alternative approaches:\n",
    "\n",
    "**Optimized Libraries:**\n",
    "\n",
    "  Use optimized libraries like `sentencepiece` or `subword-nmt`, which are designed for BPE and can handle larger datasets more efficiently. These libraries implement BPE in a way that’s optimized for speed and memory usage.\n",
    "\n",
    "- **Stream Processing**:\n",
    "\n",
    "  Instead of loading the entire dataset into memory, consider processing it in chunks or streams. This way, you can update your vocabulary and statistics without needing to load everything at once.\n",
    "- **Use Hash Maps for Counting**:\n",
    "\n",
    "  Instead of using lists or arrays, using hash maps (dictionaries) can improve the efficiency of counting pairs and merging them.\n",
    "\n",
    "As an excersise, please implement BPE using `sentencepiece` using the corpus provided in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/p/miniconda3/envs/nlp/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# @title 🧑🏿‍💻 Your code here\n",
    "\n",
    "! pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "with open(\"input.txt\", \"w+\") as f:\n",
    "    f.write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: ['▁T', 'oken', 'ization', '▁', 'is', '▁', 'the', '▁process', '▁of', '▁b', 're', 'a', 'k', 'i', 'ng', '▁', 'd', 'o', 'w', 'n', '▁a', '▁se', 'q', 'u', 'en', 'ce', '▁of', '▁', 'te', 'x', 't', '▁in', 'to', '▁s', 'm', 'alle', 'r', '▁', 'u', 'nit', 's', '▁', 'call', 'ed', '▁tokens', ',', '▁wh', 'i', 'ch', '▁ca', 'n', '▁b', 'e', '▁w', 'or', 'd', 's', ',', '▁p', 'h', 'r', 'as', 'es', ',', '▁', 'or', '▁', 'e', 've', 'n', '▁in', 'd', 'i', 'v', 'i', 'd', 'u', 'al', '▁', 'ch', 'ar', 'ac', 'te', 'rs', '.', '▁T', 'oken', 'ization', '▁', 'is', '▁of', 't', 'en', '▁', 'the', '▁', 'fi', 'rs', 't', '▁step', '▁in', '▁na', 't', 'ur', 'al', '▁', 'la', 'ng', 'ua', 'g', 'es', '▁process', 'i', 'ng', '▁t', 'as', 'k', 's', '▁', 'su', 'ch', '▁', 'as', '▁', 'te', 'x', 't', '▁c', 'la', 'ssi', 'fi', 'c', 'ation', ',', '▁na', 'me', 'd', '▁', 'enti', 'ty', '▁re', 'co', 'g', 'ni', 'tion', ',', '▁an', 'd', '▁s', 'enti', 'me', 'n', 't', '▁an', 'a', 'ly', 's', 'is', '.', '▁T', 'he', '▁re', 'su', 'l', 'ti', 'ng', '▁tokens', '▁a', 're', '▁', 'ty', 'p', 'ical', 'ly', '▁use', 'd', '▁', 'as', '▁in', 'p', 'u', 't', '▁to', '▁f', 'ur', 'the', 'r', '▁process', 'i', 'ng', '▁step', 's', ',', '▁', 'su', 'ch', '▁', 'as', '▁', 've', 'ct', 'or', 'ization', ',', '▁wh', 'e', 're', '▁', 'the', '▁tokens', '▁a', 're', '▁c', 'on', 've', 'r', 'te', 'd', '▁in', 'to', '▁', 'n', 'u', 'me', 'r', 'ical', '▁re', 'p', 're', 'sent', 'ation', 's', '▁f', 'or', '▁', 'ma', 'ch', 'in', 'e', '▁', 'l', 'e', 'ar', 'ni', 'ng', '▁m', 'o', 'd', 'e', 'l', 's', '▁to', '▁use', '.']\n",
      "Decoded: Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters. Tokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis. The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: input.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: input.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 8 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=511\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=250\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 153 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 61\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 61 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109 obj=15.6515 num_tokens=218 num_tokens/piece=2\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=99 obj=14.9491 num_tokens=219 num_tokens/piece=2.21212\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "# @title 👀 Solution\n",
    "\n",
    "# Train a SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=\"input.txt\", model_prefix=\"m\", vocab_size=100)\n",
    "\n",
    "# Load the model\n",
    "sp = spm.SentencePieceProcessor(model_file=\"m.model\")\n",
    "\n",
    "encoded = sp.encode(corpus, out_type=str)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decode back to text\n",
    "decoded = sp.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "os.remove(\"input.txt\")\n",
    "os.remove(\"m.model\")\n",
    "os.remove(\"m.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Normalization: Porter Stemmer\n",
    "\n",
    "Text normalization is a critical preprocessing step in Natural Language Processing (NLP) that involves transforming text into a standard format. One common method of normalization is stemming, which reduces words to their root forms. In linguistics, a **stem** is a part of a word that is common to all of its inflected variants. \n",
    "\n",
    "For example:\n",
    "- Integrate\n",
    "- Integrated\n",
    "- Integration\n",
    "- Integrating\n",
    "\n",
    "The above words are inflected variants of **Integrat**. Hence, **Integrat** is a stem. To this stem, we can add different suffixes to form various words. The process of reducing such inflected (or sometimes derived) words to their stem is known as stemming. For instance, \"Integrate,\" \"Integrated,\" \"Integration,\" and \"Integrating\" can be reduced to the stem **Integrat**.\n",
    "\n",
    "### The Porter Stemmer Algorithm\n",
    "\n",
    "The Porter Stemmer operates by applying a series of rules to strip suffixes from words. Here’s a brief overview of its functionality:\n",
    "\n",
    "**Step-wise Process**: The algorithm consists of multiple steps, each aimed at removing specific types of suffixes. The process can be categorized into five main phases, each focusing on a distinct set of morphological rules.\n",
    "\n",
    "**Suffix Removal**: It employs a set of predefined rules to eliminate common suffixes such as \"ing,\" \"ed,\" \"ly,\" etc. For example:\n",
    "\n",
    "- \"running\" → \"run\"\n",
    "- \"happily\" → \"happi\"\n",
    "- \"better\" → \"better\" (the word is left unchanged as it's already a stem)\n",
    "\n",
    "### Consonant-Vowel Rules\n",
    "\n",
    "A consonant is defined as any letter that is not a vowel or the letter **Y** when preceded by a consonant. For instance, in the word **TOY**, the consonants are **T** and **Y**, while in **SYZYGY**, they are **S**, **Z**, and **G**. \n",
    "\n",
    "If a letter is not a consonant, it is classified as a vowel.\n",
    "\n",
    "**Notation**:\n",
    "- A consonant is denoted by **c** and a vowel by **v**.\n",
    "- A sequence of one or more consecutive consonants is denoted by **C**, and a sequence of one or more consecutive vowels is denoted by **V**. \n",
    "\n",
    "Thus, any word or part of a word can be represented in one of the following forms:\n",
    "\n",
    "- CVCV … C → e.g., collection, management\n",
    "- CVCV … V → e.g., conclude, revise\n",
    "- VCVC … C → e.g., entertainment, illumination\n",
    "- VCVC … V → e.g., illustrate, abundance\n",
    "\n",
    "All of these forms can be succinctly represented as:\n",
    "\n",
    "{C\\}VCVC ... \\{V\\}\n",
    "\n",
    "Here, the brackets (`{}`) indicate the arbitrary presence of consonants or vowels.\n",
    "\n",
    "**Measure of the Word (m)**: \n",
    "\n",
    "The value **m** found in the above expression is referred to as the measure of any word or word part when represented in the form \\([C](VC)^m[V]\\). Examples for different values of **m** include:\n",
    "\n",
    "- **m=0**   →   TREE, TR, EE, Y, BY\n",
    "- **m=1**   →   TROUBLE, OATS, TREES, IVY\n",
    "- **m=2**   →   TROUBLES, PRIVATE, OATEN, ROBBERY\n",
    "\n",
    "<p align=\"center\"><img src=\"https://vijinimallawaarachchi.com/wp-content/uploads/2017/05/stemmer1.png\" alt=\"Stemmer\" width=\"50%\" height=\"10%\" style=\"display: block; margin: 20px auto;\"/></p>\n",
    "\n",
    "### Rules for Suffix Replacement\n",
    "\n",
    "The rules for replacing (or removing) a suffix are articulated in the following formal structure:\n",
    "\n",
    "`(condition) S1 → S2`\n",
    "\n",
    "\n",
    "Where:\n",
    "- **S1** represents the suffix subject to replacement or removal.\n",
    "- **S2** denotes the new suffix (which may also be null, indicating complete removal).\n",
    "\n",
    "#### Example Rules\n",
    "1. **(ends with \"ed\" or \"ing\" and has a root word)**: S1 → S2  \n",
    "2. **(ends with \"y\" preceded by a consonant)**: S1 → S2  \n",
    "3. **(ends with \"ational\", \"tional\", \"enci\", or \"anci\")**: S1 → S2  \n",
    "4. **(ends with \"icate\", \"ative\", or \"alize\")**: S1 → S2  \n",
    "5. **(ends with \"ment\", \"ness\", or \"ity\")**: S1 → S2  \n",
    "6. **(specific irregular forms)**: S1 → S2\n",
    "\n",
    "This means that if a word ends with the suffix **S1**, and the stem before **S1** satisfies the given condition, **S1** is replaced by **S2**. The condition is generally expressed in terms of **m** concerning the stem preceding **S1**.\n",
    "\n",
    "**Example**:\n",
    "\\[\n",
    "(m > 1) \\, EMENT \\rightarrow \\text{(S2 is null)}\n",
    "\\]\n",
    "In this instance, **S1** is ‘EMENT’ and **S2** is null. This would transform **REPLACEMENT** to **REPLAC**, since **REPLAC** is a word part for which **m = 2**.\n",
    "\n",
    "### Conditions\n",
    "\n",
    "The conditions may incorporate the following components:\n",
    "\n",
    "- **S**: the stem ends with S (and similarly for other letters)\n",
    "- **v**: the stem contains a vowel\n",
    "- **d**: the stem ends with a double consonant (e.g., -TT, -SS)\n",
    "- **o**: the stem ends in a CVC pattern, where the second consonant is not W, X, or Y (e.g., -WIL, -HOP)\n",
    "\n",
    "The condition part may also feature expressions using **and**, **or**, and **not**.\n",
    "\n",
    "Examples of Conditions:\n",
    "- **(m > 1 and (*S or *T))**: tests for a stem with **m > 1** ending in **S** or **T**.\n",
    "- **(*d and not (*L or *S or *Z))**: tests for a stem ending with a double consonant, excluding endings with letters **L**, **S**, or **Z**.\n",
    "\n",
    "### How Rules Are Applied\n",
    "\n",
    "In a set of rules written sequentially, only one rule is obeyed, specifically the one with the longest matching **S1** for the given word. For instance, consider the following rules:\n",
    "\n",
    "`SSES → SS IES → I SS → SS S →`\n",
    "\n",
    "\n",
    "Here, all conditions are null. The word **CARESSES** maps to **CARESS**, as **SSES** is the longest match for **S1**. Similarly, **CARESS** maps to **CARESS** (since **S1 = \"SS\"**) and **CARES** maps to **CARE** (since **S1 = \"S\"**).\n",
    "\n",
    "### Advantages of Using Porter Stemmer\n",
    "\n",
    "- **Simplicity**: The algorithm is straightforward to implement and does not require extensive computational resources.\n",
    "- **Efficiency**: It significantly reduces the size of the vocabulary, allowing models to run faster and consume less memory.\n",
    "- **Performance**: In many cases, stemming can lead to improved performance in tasks such as search and classification by grouping related words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Stemmed Words: ['mypi', 'mypyc', 'licens', 'term', 'mit', 'licens', 'reproduc', 'mit', 'licens', 'copyright', 'c', 'jukka', 'lehtosalo', 'contributor', 'copyright', 'c', 'dropbox', 'inc', 'permiss', 'herebi', 'grant', 'free', 'charg', 'person', 'obtain', 'copi', 'softwar', 'associ', 'document', 'file', 'softwar', 'deal', 'softwar', 'without', 'restrict', 'includ', 'without', 'limit', 'right', 'use', 'copi', 'modifi', 'merg', 'publish', 'distribut', 'sublicens', 'sell', 'copi', 'softwar', 'permit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/p/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/p/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK resources are available\n",
    "nltk.download(info_or_id=\"punkt\")\n",
    "nltk.download(info_or_id=\"stopwords\")\n",
    "\n",
    "\n",
    "# Function to process the corpus in chunks\n",
    "def process_corpus(file_path, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Processes a large text file in chunks, tokenizing, stemming,\n",
    "    and removing stop words from each chunk.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file.\n",
    "        chunk_size (int): The number of lines to process at once.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of stemmed words from the entire corpus,\n",
    "              with stop words removed.\n",
    "    \"\"\"\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))  # Set of English stop words\n",
    "    all_stemmed_words = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        while True:\n",
    "            # Read a chunk of lines from the file\n",
    "            lines = [file.readline() for _ in range(chunk_size)]\n",
    "            if not lines or all(line == \"\" for line in lines):\n",
    "                break\n",
    "\n",
    "            # Combine the lines into a single string\n",
    "            corpus_chunk = \" \".join(lines)\n",
    "\n",
    "            # Tokenize the text into words\n",
    "            tokens = word_tokenize(corpus_chunk)\n",
    "\n",
    "            # Remove stop words, punctuation, and perform stemming on each token\n",
    "            stemmed_words = [\n",
    "                porter_stemmer.stem(token)\n",
    "                for token in tokens\n",
    "                if token.lower() not in stop_words\n",
    "                and token.isalpha()  # Check if token is alphabetic\n",
    "            ]\n",
    "\n",
    "            # Extend the list of all stemmed words\n",
    "            all_stemmed_words.extend(stemmed_words)\n",
    "\n",
    "    return all_stemmed_words\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"../LICENSE\"  # Replace with your large corpus file path\n",
    "stemmed_words = process_corpus(file_path)\n",
    "\n",
    "# Display a sample of the stemmed words\n",
    "print(\"Sample of Stemmed Words:\", stemmed_words[:50])  # Show first 50 stemmed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of Using Porter Stemmer:\n",
    "\n",
    "**Over-Stemming:** The Porter Stemmer may lead to excessive reduction, where different words that should not be grouped together are erroneously stemmed to the same form. For example, \"university\" and \"universal\" may both be reduced to \"univer.\"\n",
    "\n",
    "**Loss of Meaning:** Since stemming focuses on morphological similarity rather than semantic meaning, it can sometimes result in a loss of contextual information.\n",
    "\n",
    "**Language Dependence:** The rules are tailored specifically for the English language and may not work well for other languages with different morphological structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is the process of dividing a text into its constituent sentences. It is a crucial step in Natural Language Processing (NLP) and is often one of the first stages in text processing. Accurate sentence segmentation is essential for downstream tasks such as tokenization, parsing, and semantic analysis.\n",
    "The advantages of sentence segmentation are, it helps in comprehending the structure and meaning of text, and it ensures that model trained on segmented data perform effectively by maintaining contextual relationships withing sentences.\n",
    "\n",
    "### Approaches to Sentence Segmentation\n",
    "\n",
    "Sentence segmentation can be approached using either **rule-based** methods or **machine learning** techniques. **Rule-based** methods identify sentence boundaries primarily by detecting punctuation marks and established patterns; however, they often struggle with exceptions, such as abbreviations and other nuanced cases. On the other hand, machine learning models, including advanced deep learning architectures like `Recurrent Neural Networks (RNNs)` and `BERT`, are capable of recognizing complex patterns by learning from extensive datasets. Modern Natural Language Processing (NLP) libraries, such as `spaCy` and `NLTK`, effectively integrate both methodologies to enhance the accuracy of sentence segmentation. When using a model from `spaCy`, for instance, the following processing pipeline is typically employed:\n",
    "\n",
    "<p align=\"center\"><img src=\"https://spacy.io/images/pipeline.svg\" alt=\"Stemmer\" width=\"50%\" height=\"10%\" style=\"display: block; margin: 20px auto;\"/></p>\n",
    "\n",
    "- `spaCy` first tokenizes the text to produce a Doc object. \n",
    "- The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
    "- The pipeline used by the trained pipelines typically include a **tagger**, a **lemmatizer**, a **parser** and an entity recognizer. \n",
    "- Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "> Notice: For more information please check spaCy [documentations](https://spacy.io/usage/spacy-101).\n",
    "\n",
    " In the cell below, Parham uses `spaCy` to segment sentences from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['mypy', '(', 'and', 'mypyc', ')', 'are', 'licensed', 'under', 'the', 'terms', 'of', 'the', 'mit', 'license', ',', 'reproduced', 'below', '.', '\\n\\n', '=', '=', '=', '=', '=', '\\n\\n', 'the', 'mit', 'license', '\\n\\n', 'copyright', '(', 'c', ')', '2012', '-', '2023', 'jukka', 'lehtosalo', 'and', 'contributors', '\\n', 'copyright', '(', 'c', ')', '2015', '-', '2023', 'dropbox', ',', 'inc', '.', '\\n\\n', 'permission', 'is', 'hereby', 'granted', ',', 'free', 'of', 'charge', ',', 'to', 'any', 'person', 'obtaining', 'a', '\\n', 'copy', 'of', 'this', 'software', 'and', 'associated', 'documentation', 'files', '(', 'the', '\"', 'software', '\"', ')', ',', '\\n', 'to', 'deal', 'in', 'the', 'software', 'without', 'restriction', ',', 'including', 'without', 'limitation', '\\n', 'the', 'rights', 'to', 'use', ',', 'copy', ',', 'modify', ',', 'merge', ',', 'publish', ',', 'distribute', ',', 'sublicense', ',', '\\n', 'and/or', 'sell', 'copies', 'of', 'the', 'software', ',', 'and', 'to', 'permit', 'persons', 'to', 'whom', 'the', '\\n', 'software', 'is', 'furnished', 'to', 'do', 'so', ',', 'subject', 'to', 'the', 'following', 'conditions', ':', '\\n\\n', 'the', 'above', 'copyright', 'notice', 'and', 'this', 'permission', 'notice', 'shall', 'be', 'included', 'in', '\\n', 'all', 'copies', 'or', 'substantial', 'portions', 'of', 'the', 'software', '.', '\\n\\n', 'the', 'software', 'is', 'provided', '\"', 'as', 'is', '\"', ',', 'without', 'warranty', 'of', 'any', 'kind', ',', 'express', 'or', '\\n', 'implied', ',', 'including', 'but', 'not', 'limited', 'to', 'the', 'warranties', 'of', 'merchantability', ',', '\\n', 'fitness', 'for', 'a', 'particular', 'purpose', 'and', 'noninfringement', '.', 'in', 'no', 'event', 'shall', 'the', '\\n', 'authors', 'or', 'copyright', 'holders', 'be', 'liable', 'for', 'any', 'claim', ',', 'damages', 'or', 'other', '\\n', 'liability', ',', 'whether', 'in', 'an', 'action', 'of', 'contract', ',', 'tort', 'or', 'otherwise', ',', 'arising', '\\n', 'from', ',', 'out', 'of', 'or', 'in', 'connection', 'with', 'the', 'software', 'or', 'the', 'use', 'or', 'other', '\\n', 'dealings', 'in', 'the', 'software', '.', '\\n\\n', '=', '=', '=', '=', '=', '\\n\\n', 'portions', 'of', 'mypy', 'and', 'mypyc', 'are', 'licensed', 'under', 'different', 'licenses', '.', '\\n', 'the', 'files', '\\n', 'mypyc', '/', 'lib', '-', 'rt', '/', 'pythonsupport.h', ',', 'mypyc', '/', 'lib', '-', 'rt', '/', 'getargs.c', 'and', '\\n', 'mypyc', '/', 'lib', '-', 'rt', '/', 'getargsfast.c', 'are', 'licensed', 'under', 'the', 'psf', '2', 'license', ',', 'reproduced', '\\n', 'below', '.', '\\n\\n', '=', '=', '=', '=', '=', '\\n\\n', 'python', 'software', 'foundation', 'license', 'version', '2', '\\n', '--------------------------------------------', '\\n\\n', '1', '.', 'this', 'license', 'agreement', 'is', 'between', 'the', 'python', 'software', 'foundation', '\\n', '(', '\"', 'psf', '\"', ')', ',', 'and', 'the', 'individual', 'or', 'organization', '(', '\"', 'licensee', '\"', ')', 'accessing', 'and', '\\n', 'otherwise', 'using', 'this', 'software', '(', '\"', 'python', '\"', ')', 'in', 'source', 'or', 'binary', 'form', 'and', '\\n', 'its', 'associated', 'documentation', '.', '\\n\\n', '2', '.', 'subject', 'to', 'the', 'terms', 'and', 'conditions', 'of', 'this', 'license', 'agreement', ',', 'psf', 'hereby', '\\n', 'grants', 'licensee', 'a', 'nonexclusive', ',', 'royalty', '-', 'free', ',', 'world', '-', 'wide', 'license', 'to', 'reproduce', ',', '\\n', 'analyze', ',', 'test', ',', 'perform', 'and/or', 'display', 'publicly', ',', 'prepare', 'derivative', 'works', ',', '\\n', 'distribute', ',', 'and', 'otherwise', 'use', 'python', 'alone', 'or', 'in', 'any', 'derivative', 'version', ',', '\\n', 'provided', ',', 'however', ',', 'that', 'psf', \"'s\", 'license', 'agreement', 'and', 'psf', \"'s\", 'notice', 'of', 'copyright', ',', '\\n', 'i.e.', ',', '\"', 'copyright', '(', 'c', ')', '2001', ',', '2002', ',', '2003', ',', '2004', ',', '2005', ',', '2006', ',', '2007', ',', '2008', ',', '2009', ',', '2010', ',', '\\n', '2011', ',', '2012', 'python', 'software', 'foundation', ';', 'all', 'rights', 'reserved', '\"', 'are', 'retained', 'in', 'python', '\\n', 'alone', 'or', 'in', 'any', 'derivative', 'version', 'prepared', 'by', 'licensee', '.', '\\n\\n', '3', '.', 'in', 'the', 'event', 'licensee', 'prepares', 'a', 'derivative', 'work', 'that', 'is', 'based', 'on', '\\n', 'or', 'incorporates', 'python', 'or', 'any', 'part', 'thereof', ',', 'and', 'wants', 'to', 'make', '\\n', 'the', 'derivative', 'work', 'available', 'to', 'others', 'as', 'provided', 'herein', ',', 'then', '\\n', 'licensee', 'hereby', 'agrees', 'to', 'include', 'in', 'any', 'such', 'work', 'a', 'brief', 'summary', 'of', '\\n', 'the', 'changes', 'made', 'to', 'python', '.', '\\n\\n', '4', '.', 'psf', 'is', 'making', 'python', 'available', 'to', 'licensee', 'on', 'an', '\"', 'as', 'is', '\"', '\\n', 'basis', '.', ' ', 'psf', 'makes', 'no', 'representations', 'or', 'warranties', ',', 'express', 'or', '\\n', 'implied', '.', ' ', 'by', 'way', 'of', 'example', ',', 'but', 'not', 'limitation', ',', 'psf', 'makes', 'no', 'and', '\\n', 'disclaims', 'any', 'representation', 'or', 'warranty', 'of', 'merchantability', 'or', 'fitness', '\\n', 'for', 'any', 'particular', 'purpose', 'or', 'that', 'the', 'use', 'of', 'python', 'will', 'not', '\\n', 'infringe', 'any', 'third', 'party', 'rights', '.', '\\n\\n', '5', '.', 'psf', 'shall', 'not', 'be', 'liable', 'to', 'licensee', 'or', 'any', 'other', 'users', 'of', 'python', '\\n', 'for', 'any', 'incidental', ',', 'special', ',', 'or', 'consequential', 'damages', 'or', 'loss', 'as', '\\n', 'a', 'result', 'of', 'modifying', ',', 'distributing', ',', 'or', 'otherwise', 'using', 'python', ',', '\\n', 'or', 'any', 'derivative', 'thereof', ',', 'even', 'if', 'advised', 'of', 'the', 'possibility', 'thereof', '.', '\\n\\n', '6', '.', 'this', 'license', 'agreement', 'will', 'automatically', 'terminate', 'upon', 'a', 'material', '\\n', 'breach', 'of', 'its', 'terms', 'and', 'conditions', '.', '\\n\\n', '7', '.', 'nothing', 'in', 'this', 'license', 'agreement', 'shall', 'be', 'deemed', 'to', 'create', 'any', '\\n', 'relationship', 'of', 'agency', ',', 'partnership', ',', 'or', 'joint', 'venture', 'between', 'psf', 'and', '\\n', 'licensee', '.', ' ', 'this', 'license', 'agreement', 'does', 'not', 'grant', 'permission', 'to', 'use', 'psf', '\\n', 'trademarks', 'or', 'trade', 'name', 'in', 'a', 'trademark', 'sense', 'to', 'endorse', 'or', 'promote', '\\n', 'products', 'or', 'services', 'of', 'licensee', ',', 'or', 'any', 'third', 'party', '.', '\\n\\n', '8', '.', 'by', 'copying', ',', 'installing', 'or', 'otherwise', 'using', 'python', ',', 'licensee', '\\n', 'agrees', 'to', 'be', 'bound', 'by', 'the', 'terms', 'and', 'conditions', 'of', 'this', 'license', '\\n', 'agreement', '.', '\\n\\n\\n', 'beopen.com', 'license', 'agreement', 'for', 'python', '2.0', '\\n', '-------------------------------------------', '\\n\\n', 'beopen', 'python', 'open', 'source', 'license', 'agreement', 'version', '1', '\\n\\n', '1', '.', 'this', 'license', 'agreement', 'is', 'between', 'beopen.com', '(', '\"', 'beopen', '\"', ')', ',', 'having', 'an', '\\n', 'office', 'at', '160', 'saratoga', 'avenue', ',', 'santa', 'clara', ',', 'ca', '95051', ',', 'and', 'the', '\\n', 'individual', 'or', 'organization', '(', '\"', 'licensee', '\"', ')', 'accessing', 'and', 'otherwise', 'using', '\\n', 'this', 'software', 'in', 'source', 'or', 'binary', 'form', 'and', 'its', 'associated', '\\n', 'documentation', '(', '\"', 'the', 'software', '\"', ')', '.', '\\n\\n', '2', '.', 'subject', 'to', 'the', 'terms', 'and', 'conditions', 'of', 'this', 'beopen', 'python', 'license', '\\n', 'agreement', ',', 'beopen', 'hereby', 'grants', 'licensee', 'a', 'non', '-', 'exclusive', ',', '\\n', 'royalty', '-', 'free', ',', 'world', '-', 'wide', 'license', 'to', 'reproduce', ',', 'analyze', ',', 'test', ',', 'perform', '\\n', 'and/or', 'display', 'publicly', ',', 'prepare', 'derivative', 'works', ',', 'distribute', ',', 'and', '\\n', 'otherwise', 'use', 'the', 'software', 'alone', 'or', 'in', 'any', 'derivative', 'version', ',', '\\n', 'provided', ',', 'however', ',', 'that', 'the', 'beopen', 'python', 'license', 'is', 'retained', 'in', 'the', '\\n', 'software', ',', 'alone', 'or', 'in', 'any', 'derivative', 'version', 'prepared', 'by', 'licensee', '.', '\\n\\n', '3', '.', 'beopen', 'is', 'making', 'the', 'software', 'available', 'to', 'licensee', 'on', 'an', '\"', 'as', 'is', '\"', '\\n', 'basis', '.', ' ', 'beopen', 'makes', 'no', 'representations', 'or', 'warranties', ',', 'express', 'or', '\\n', 'implied', '.', ' ', 'by', 'way', 'of', 'example', ',', 'but', 'not', 'limitation', ',', 'beopen', 'makes', 'no', 'and', '\\n', 'disclaims', 'any', 'representation', 'or', 'warranty', 'of', 'merchantability', 'or', 'fitness', '\\n', 'for', 'any', 'particular', 'purpose', 'or', 'that', 'the', 'use', 'of', 'the', 'software', 'will', 'not', '\\n', 'infringe', 'any', 'third', 'party', 'rights', '.', '\\n\\n', '4', '.', 'beopen', 'shall', 'not', 'be', 'liable', 'to', 'licensee', 'or', 'any', 'other', 'users', 'of', 'the', '\\n', 'software', 'for', 'any', 'incidental', ',', 'special', ',', 'or', 'consequential', 'damages', 'or', 'loss', '\\n', 'as', 'a', 'result', 'of', 'using', ',', 'modifying', 'or', 'distributing', 'the', 'software', ',', 'or', 'any', '\\n', 'derivative', 'thereof', ',', 'even', 'if', 'advised', 'of', 'the', 'possibility', 'thereof', '.', '\\n\\n', '5', '.', 'this', 'license', 'agreement', 'will', 'automatically', 'terminate', 'upon', 'a', 'material', '\\n', 'breach', 'of', 'its', 'terms', 'and', 'conditions', '.', '\\n\\n', '6', '.', 'this', 'license', 'agreement', 'shall', 'be', 'governed', 'by', 'and', 'interpreted', 'in', 'all', '\\n', 'respects', 'by', 'the', 'law', 'of', 'the', 'state', 'of', 'california', ',', 'excluding', 'conflict', 'of', '\\n', 'law', 'provisions', '.', ' ', 'nothing', 'in', 'this', 'license', 'agreement', 'shall', 'be', 'deemed', 'to', '\\n', 'create', 'any', 'relationship', 'of', 'agency', ',', 'partnership', ',', 'or', 'joint', 'venture', '\\n', 'between', 'beopen', 'and', 'licensee', '.', ' ', 'this', 'license', 'agreement', 'does', 'not', 'grant', '\\n', 'permission', 'to', 'use', 'beopen', 'trademarks', 'or', 'trade', 'names', 'in', 'a', 'trademark', '\\n', 'sense', 'to', 'endorse', 'or', 'promote', 'products', 'or', 'services', 'of', 'licensee', ',', 'or', 'any', '\\n', 'third', 'party', '.', ' ', 'as', 'an', 'exception', ',', 'the', '\"', 'beopen', 'python', '\"', 'logos', 'available', 'at', '\\n', 'http://www.pythonlabs.com/logos.html', 'may', 'be', 'used', 'according', 'to', 'the', '\\n', 'permissions', 'granted', 'on', 'that', 'web', 'page', '.', '\\n\\n', '7', '.', 'by', 'copying', ',', 'installing', 'or', 'otherwise', 'using', 'the', 'software', ',', 'licensee', '\\n', 'agrees', 'to', 'be', 'bound', 'by', 'the', 'terms', 'and', 'conditions', 'of', 'this', 'license', '\\n', 'agreement', '.', '\\n\\n\\n', 'cnri', 'license', 'agreement', 'for', 'python', '1.6.1', '\\n', '---------------------------------------', '\\n\\n', '1', '.', 'this', 'license', 'agreement', 'is', 'between', 'the', 'corporation', 'for', 'national', '\\n', 'research', 'initiatives', ',', 'having', 'an', 'office', 'at', '1895', 'preston', 'white', 'drive', ',', '\\n', 'reston', ',', 'va', '20191', '(', '\"', 'cnri', '\"', ')', ',', 'and', 'the', 'individual', 'or', 'organization', '\\n', '(', '\"', 'licensee', '\"', ')', 'accessing', 'and', 'otherwise', 'using', 'python', '1.6.1', 'software', 'in', '\\n', 'source', 'or', 'binary', 'form', 'and', 'its', 'associated', 'documentation', '.', '\\n\\n', '2', '.', 'subject', 'to', 'the', 'terms', 'and', 'conditions', 'of', 'this', 'license', 'agreement', ',', 'cnri', '\\n', 'hereby', 'grants', 'licensee', 'a', 'nonexclusive', ',', 'royalty', '-', 'free', ',', 'world', '-', 'wide', '\\n', 'license', 'to', 'reproduce', ',', 'analyze', ',', 'test', ',', 'perform', 'and/or', 'display', 'publicly', ',', '\\n', 'prepare', 'derivative', 'works', ',', 'distribute', ',', 'and', 'otherwise', 'use', 'python', '1.6.1', '\\n', 'alone', 'or', 'in', 'any', 'derivative', 'version', ',', 'provided', ',', 'however', ',', 'that', 'cnri', \"'s\", '\\n', 'license', 'agreement', 'and', 'cnri', \"'s\", 'notice', 'of', 'copyright', ',', 'i.e.', ',', '\"', 'copyright', '(', 'c', ')', '\\n', '1995', '-', '2001', 'corporation', 'for', 'national', 'research', 'initiatives', ';', 'all', 'rights', '\\n', 'reserved', '\"', 'are', 'retained', 'in', 'python', '1.6.1', 'alone', 'or', 'in', 'any', 'derivative', '\\n', 'version', 'prepared', 'by', 'licensee', '.', ' ', 'alternately', ',', 'in', 'lieu', 'of', 'cnri', \"'s\", 'license', '\\n', 'agreement', ',', 'licensee', 'may', 'substitute', 'the', 'following', 'text', '(', 'omitting', 'the', '\\n', 'quotes', '):', '\"', 'python', '1.6.1', 'is', 'made', 'available', 'subject', 'to', 'the', 'terms', 'and', '\\n', 'conditions', 'in', 'cnri', \"'s\", 'license', 'agreement', '.', ' ', 'this', 'agreement', 'together', 'with', '\\n', 'python', '1.6.1', 'may', 'be', 'located', 'on', 'the', 'internet', 'using', 'the', 'following', '\\n', 'unique', ',', 'persistent', 'identifier', '(', 'known', 'as', 'a', 'handle', '):', '1895.22/1013', '.', ' ', 'this', '\\n', 'agreement', 'may', 'also', 'be', 'obtained', 'from', 'a', 'proxy', 'server', 'on', 'the', 'internet', '\\n', 'using', 'the', 'following', 'url', ':', 'http://hdl.handle.net/1895.22/1013', '\"', '.', '\\n\\n', '3', '.', 'in', 'the', 'event', 'licensee', 'prepares', 'a', 'derivative', 'work', 'that', 'is', 'based', 'on', '\\n', 'or', 'incorporates', 'python', '1.6.1', 'or', 'any', 'part', 'thereof', ',', 'and', 'wants', 'to', 'make', '\\n', 'the', 'derivative', 'work', 'available', 'to', 'others', 'as', 'provided', 'herein', ',', 'then', '\\n', 'licensee', 'hereby', 'agrees', 'to', 'include', 'in', 'any', 'such', 'work', 'a', 'brief', 'summary', 'of', '\\n', 'the', 'changes', 'made', 'to', 'python', '1.6.1', '.', '\\n\\n', '4', '.', 'cnri', 'is', 'making', 'python', '1.6.1', 'available', 'to', 'licensee', 'on', 'an', '\"', 'as', 'is', '\"', '\\n', 'basis', '.', ' ', 'cnri', 'makes', 'no', 'representations', 'or', 'warranties', ',', 'express', 'or', '\\n', 'implied', '.', ' ', 'by', 'way', 'of', 'example', ',', 'but', 'not', 'limitation', ',', 'cnri', 'makes', 'no', 'and', '\\n', 'disclaims', 'any', 'representation', 'or', 'warranty', 'of', 'merchantability', 'or', 'fitness', '\\n', 'for', 'any', 'particular', 'purpose', 'or', 'that', 'the', 'use', 'of', 'python', '1.6.1', 'will', 'not', '\\n', 'infringe', 'any', 'third', 'party', 'rights', '.', '\\n\\n', '5', '.', 'cnri', 'shall', 'not', 'be', 'liable', 'to', 'licensee', 'or', 'any', 'other', 'users', 'of', 'python', '\\n', '1.6.1', 'for', 'any', 'incidental', ',', 'special', ',', 'or', 'consequential', 'damages', 'or', 'loss', 'as', '\\n', 'a', 'result', 'of', 'modifying', ',', 'distributing', ',', 'or', 'otherwise', 'using', 'python', '1.6.1', ',', '\\n', 'or', 'any', 'derivative', 'thereof', ',', 'even', 'if', 'advised', 'of', 'the', 'possibility', 'thereof', '.', '\\n\\n', '6', '.', 'this', 'license', 'agreement', 'will', 'automatically', 'terminate', 'upon', 'a', 'material', '\\n', 'breach', 'of', 'its', 'terms', 'and', 'conditions', '.', '\\n\\n', '7', '.', 'this', 'license', 'agreement', 'shall', 'be', 'governed', 'by', 'the', 'federal', '\\n', 'intellectual', 'property', 'law', 'of', 'the', 'united', 'states', ',', 'including', 'without', '\\n', 'limitation', 'the', 'federal', 'copyright', 'law', ',', 'and', ',', 'to', 'the', 'extent', 'such', '\\n', 'u.s', '.', 'federal', 'law', 'does', 'not', 'apply', ',', 'by', 'the', 'law', 'of', 'the', 'commonwealth', 'of', '\\n', 'virginia', ',', 'excluding', 'virginia', \"'s\", 'conflict', 'of', 'law', 'provisions', '.', '\\n', 'notwithstanding', 'the', 'foregoing', ',', 'with', 'regard', 'to', 'derivative', 'works', 'based', '\\n', 'on', 'python', '1.6.1', 'that', 'incorporate', 'non', '-', 'separable', 'material', 'that', 'was', '\\n', 'previously', 'distributed', 'under', 'the', 'gnu', 'general', 'public', 'license', '(', 'gpl', ')', ',', 'the', '\\n', 'law', 'of', 'the', 'commonwealth', 'of', 'virginia', 'shall', 'govern', 'this', 'license', '\\n', 'agreement', 'only', 'as', 'to', 'issues', 'arising', 'under', 'or', 'with', 'respect', 'to', '\\n', 'paragraphs', '4', ',', '5', ',', 'and', '7', 'of', 'this', 'license', 'agreement', '.', ' ', 'nothing', 'in', 'this', '\\n', 'license', 'agreement', 'shall', 'be', 'deemed', 'to', 'create', 'any', 'relationship', 'of', '\\n', 'agency', ',', 'partnership', ',', 'or', 'joint', 'venture', 'between', 'cnri', 'and', 'licensee', '.', ' ', 'this', '\\n', 'license', 'agreement', 'does', 'not', 'grant', 'permission', 'to', 'use', 'cnri', 'trademarks', 'or', '\\n', 'trade', 'name', 'in', 'a', 'trademark', 'sense', 'to', 'endorse', 'or', 'promote', 'products', 'or', '\\n', 'services', 'of', 'licensee', ',', 'or', 'any', 'third', 'party', '.', '\\n\\n', '8', '.', 'by', 'clicking', 'on', 'the', '\"', 'accept', '\"', 'button', 'where', 'indicated', ',', 'or', 'by', 'copying', ',', '\\n', 'installing', 'or', 'otherwise', 'using', 'python', '1.6.1', ',', 'licensee', 'agrees', 'to', 'be', '\\n', 'bound', 'by', 'the', 'terms', 'and', 'conditions', 'of', 'this', 'license', 'agreement', '.', '\\n\\n        ', 'accept', '\\n\\n\\n', 'cwi', 'license', 'agreement', 'for', 'python', '0.9.0', 'through', '1.2', '\\n', '--------------------------------------------------', '\\n\\n', 'copyright', '(', 'c', ')', '1991', '-', '1995', ',', 'stichting', 'mathematisch', 'centrum', 'amsterdam', ',', '\\n', 'the', 'netherlands', '.', ' ', 'all', 'rights', 'reserved', '.', '\\n\\n', 'permission', 'to', 'use', ',', 'copy', ',', 'modify', ',', 'and', 'distribute', 'this', 'software', 'and', 'its', '\\n', 'documentation', 'for', 'any', 'purpose', 'and', 'without', 'fee', 'is', 'hereby', 'granted', ',', '\\n', 'provided', 'that', 'the', 'above', 'copyright', 'notice', 'appear', 'in', 'all', 'copies', 'and', 'that', '\\n', 'both', 'that', 'copyright', 'notice', 'and', 'this', 'permission', 'notice', 'appear', 'in', '\\n', 'supporting', 'documentation', ',', 'and', 'that', 'the', 'name', 'of', 'stichting', 'mathematisch', '\\n', 'centrum', 'or', 'cwi', 'not', 'be', 'used', 'in', 'advertising', 'or', 'publicity', 'pertaining', 'to', '\\n', 'distribution', 'of', 'the', 'software', 'without', 'specific', ',', 'written', 'prior', '\\n', 'permission', '.', '\\n\\n', 'stichting', 'mathematisch', 'centrum', 'disclaims', 'all', 'warranties', 'with', 'regard', 'to', '\\n', 'this', 'software', ',', 'including', 'all', 'implied', 'warranties', 'of', 'merchantability', 'and', '\\n', 'fitness', ',', 'in', 'no', 'event', 'shall', 'stichting', 'mathematisch', 'centrum', 'be', 'liable', '\\n', 'for', 'any', 'special', ',', 'indirect', 'or', 'consequential', 'damages', 'or', 'any', 'damages', '\\n', 'whatsoever', 'resulting', 'from', 'loss', 'of', 'use', ',', 'data', 'or', 'profits', ',', 'whether', 'in', 'an', '\\n', 'action', 'of', 'contract', ',', 'negligence', 'or', 'other', 'tortious', 'action', ',', 'arising', 'out', '\\n', 'of', 'or', 'in', 'connection', 'with', 'the', 'use', 'or', 'performance', 'of', 'this', 'software', '.', '\\n']\n",
      "\n",
      "Filtered Tokens (without stopwords): ['mypy', 'mypyc', 'licensed', 'terms', 'mit', 'license', 'reproduced', 'mit', 'license', 'copyright', 'c', 'jukka', 'lehtosalo', 'contributors', 'copyright', 'c', 'dropbox', 'inc', 'permission', 'granted', 'free', 'charge', 'person', 'obtaining', 'copy', 'software', 'associated', 'documentation', 'files', 'software', 'deal', 'software', 'restriction', 'including', 'limitation', 'rights', 'use', 'copy', 'modify', 'merge', 'publish', 'distribute', 'sublicense', 'sell', 'copies', 'software', 'permit', 'persons', 'software', 'furnished', 'subject', 'following', 'conditions', 'copyright', 'notice', 'permission', 'notice', 'shall', 'included', 'copies', 'substantial', 'portions', 'software', 'software', 'provided', 'warranty', 'kind', 'express', 'implied', 'including', 'limited', 'warranties', 'merchantability', 'fitness', 'particular', 'purpose', 'noninfringement', 'event', 'shall', 'authors', 'copyright', 'holders', 'liable', 'claim', 'damages', 'liability', 'action', 'contract', 'tort', 'arising', 'connection', 'software', 'use', 'dealings', 'software', 'portions', 'mypy', 'mypyc', 'licensed', 'different', 'licenses', 'files', 'mypyc', 'lib', 'rt', 'mypyc', 'lib', 'rt', 'mypyc', 'lib', 'rt', 'licensed', 'psf', 'license', 'reproduced', 'python', 'software', 'foundation', 'license', 'version', 'license', 'agreement', 'python', 'software', 'foundation', 'psf', 'individual', 'organization', 'licensee', 'accessing', 'software', 'python', 'source', 'binary', 'form', 'associated', 'documentation', 'subject', 'terms', 'conditions', 'license', 'agreement', 'psf', 'grants', 'licensee', 'nonexclusive', 'royalty', 'free', 'world', 'wide', 'license', 'reproduce', 'analyze', 'test', 'perform', 'display', 'publicly', 'prepare', 'derivative', 'works', 'distribute', 'use', 'python', 'derivative', 'version', 'provided', 'psf', 'license', 'agreement', 'psf', 'notice', 'copyright', 'copyright', 'c', 'python', 'software', 'foundation', 'rights', 'reserved', 'retained', 'python', 'derivative', 'version', 'prepared', 'licensee', 'event', 'licensee', 'prepares', 'derivative', 'work', 'based', 'incorporates', 'python', 'thereof', 'wants', 'derivative', 'work', 'available', 'provided', 'licensee', 'agrees', 'include', 'work', 'brief', 'summary', 'changes', 'python', 'psf', 'making', 'python', 'available', 'licensee', 'basis', 'psf', 'makes', 'representations', 'warranties', 'express', 'implied', 'way', 'example', 'limitation', 'psf', 'makes', 'disclaims', 'representation', 'warranty', 'merchantability', 'fitness', 'particular', 'purpose', 'use', 'python', 'infringe', 'party', 'rights', 'psf', 'shall', 'liable', 'licensee', 'users', 'python', 'incidental', 'special', 'consequential', 'damages', 'loss', 'result', 'modifying', 'distributing', 'python', 'derivative', 'thereof', 'advised', 'possibility', 'thereof', 'license', 'agreement', 'automatically', 'terminate', 'material', 'breach', 'terms', 'conditions', 'license', 'agreement', 'shall', 'deemed', 'create', 'relationship', 'agency', 'partnership', 'joint', 'venture', 'psf', 'licensee', 'license', 'agreement', 'grant', 'permission', 'use', 'psf', 'trademarks', 'trade', 'trademark', 'sense', 'endorse', 'promote', 'products', 'services', 'licensee', 'party', 'copying', 'installing', 'python', 'licensee', 'agrees', 'bound', 'terms', 'conditions', 'license', 'agreement', 'license', 'agreement', 'python', 'beopen', 'python', 'open', 'source', 'license', 'agreement', 'version', 'license', 'agreement', 'beopen', 'office', 'saratoga', 'avenue', 'santa', 'clara', 'individual', 'organization', 'licensee', 'accessing', 'software', 'source', 'binary', 'form', 'associated', 'documentation', 'software', 'subject', 'terms', 'conditions', 'beopen', 'python', 'license', 'agreement', 'beopen', 'grants', 'licensee', 'non', 'exclusive', 'royalty', 'free', 'world', 'wide', 'license', 'reproduce', 'analyze', 'test', 'perform', 'display', 'publicly', 'prepare', 'derivative', 'works', 'distribute', 'use', 'software', 'derivative', 'version', 'provided', 'beopen', 'python', 'license', 'retained', 'software', 'derivative', 'version', 'prepared', 'licensee', 'beopen', 'making', 'software', 'available', 'licensee', 'basis', 'beopen', 'makes', 'representations', 'warranties', 'express', 'implied', 'way', 'example', 'limitation', 'beopen', 'makes', 'disclaims', 'representation', 'warranty', 'merchantability', 'fitness', 'particular', 'purpose', 'use', 'software', 'infringe', 'party', 'rights', 'beopen', 'shall', 'liable', 'licensee', 'users', 'software', 'incidental', 'special', 'consequential', 'damages', 'loss', 'result', 'modifying', 'distributing', 'software', 'derivative', 'thereof', 'advised', 'possibility', 'thereof', 'license', 'agreement', 'automatically', 'terminate', 'material', 'breach', 'terms', 'conditions', 'license', 'agreement', 'shall', 'governed', 'interpreted', 'respects', 'law', 'state', 'california', 'excluding', 'conflict', 'law', 'provisions', 'license', 'agreement', 'shall', 'deemed', 'create', 'relationship', 'agency', 'partnership', 'joint', 'venture', 'beopen', 'licensee', 'license', 'agreement', 'grant', 'permission', 'use', 'beopen', 'trademarks', 'trade', 'names', 'trademark', 'sense', 'endorse', 'promote', 'products', 'services', 'licensee', 'party', 'exception', 'beopen', 'python', 'logos', 'available', 'according', 'permissions', 'granted', 'web', 'page', 'copying', 'installing', 'software', 'licensee', 'agrees', 'bound', 'terms', 'conditions', 'license', 'agreement', 'cnri', 'license', 'agreement', 'python', 'license', 'agreement', 'corporation', 'national', 'research', 'initiatives', 'office', 'preston', 'white', 'drive', 'reston', 'va', 'cnri', 'individual', 'organization', 'licensee', 'accessing', 'python', 'software', 'source', 'binary', 'form', 'associated', 'documentation', 'subject', 'terms', 'conditions', 'license', 'agreement', 'cnri', 'grants', 'licensee', 'nonexclusive', 'royalty', 'free', 'world', 'wide', 'license', 'reproduce', 'analyze', 'test', 'perform', 'display', 'publicly', 'prepare', 'derivative', 'works', 'distribute', 'use', 'python', 'derivative', 'version', 'provided', 'cnri', 'license', 'agreement', 'cnri', 'notice', 'copyright', 'copyright', 'c', 'corporation', 'national', 'research', 'initiatives', 'rights', 'reserved', 'retained', 'python', 'derivative', 'version', 'prepared', 'licensee', 'alternately', 'lieu', 'cnri', 'license', 'agreement', 'licensee', 'substitute', 'following', 'text', 'omitting', 'quotes', 'python', 'available', 'subject', 'terms', 'conditions', 'cnri', 'license', 'agreement', 'agreement', 'python', 'located', 'internet', 'following', 'unique', 'persistent', 'identifier', 'known', 'handle', 'agreement', 'obtained', 'proxy', 'server', 'internet', 'following', 'url', 'event', 'licensee', 'prepares', 'derivative', 'work', 'based', 'incorporates', 'python', 'thereof', 'wants', 'derivative', 'work', 'available', 'provided', 'licensee', 'agrees', 'include', 'work', 'brief', 'summary', 'changes', 'python', 'cnri', 'making', 'python', 'available', 'licensee', 'basis', 'cnri', 'makes', 'representations', 'warranties', 'express', 'implied', 'way', 'example', 'limitation', 'cnri', 'makes', 'disclaims', 'representation', 'warranty', 'merchantability', 'fitness', 'particular', 'purpose', 'use', 'python', 'infringe', 'party', 'rights', 'cnri', 'shall', 'liable', 'licensee', 'users', 'python', 'incidental', 'special', 'consequential', 'damages', 'loss', 'result', 'modifying', 'distributing', 'python', 'derivative', 'thereof', 'advised', 'possibility', 'thereof', 'license', 'agreement', 'automatically', 'terminate', 'material', 'breach', 'terms', 'conditions', 'license', 'agreement', 'shall', 'governed', 'federal', 'intellectual', 'property', 'law', 'united', 'states', 'including', 'limitation', 'federal', 'copyright', 'law', 'extent', 'federal', 'law', 'apply', 'law', 'commonwealth', 'virginia', 'excluding', 'virginia', 'conflict', 'law', 'provisions', 'notwithstanding', 'foregoing', 'regard', 'derivative', 'works', 'based', 'python', 'incorporate', 'non', 'separable', 'material', 'previously', 'distributed', 'gnu', 'general', 'public', 'license', 'gpl', 'law', 'commonwealth', 'virginia', 'shall', 'govern', 'license', 'agreement', 'issues', 'arising', 'respect', 'paragraphs', 'license', 'agreement', 'license', 'agreement', 'shall', 'deemed', 'create', 'relationship', 'agency', 'partnership', 'joint', 'venture', 'cnri', 'licensee', 'license', 'agreement', 'grant', 'permission', 'use', 'cnri', 'trademarks', 'trade', 'trademark', 'sense', 'endorse', 'promote', 'products', 'services', 'licensee', 'party', 'clicking', 'accept', 'button', 'indicated', 'copying', 'installing', 'python', 'licensee', 'agrees', 'bound', 'terms', 'conditions', 'license', 'agreement', 'accept', 'cwi', 'license', 'agreement', 'python', 'copyright', 'c', 'stichting', 'mathematisch', 'centrum', 'amsterdam', 'netherlands', 'rights', 'reserved', 'permission', 'use', 'copy', 'modify', 'distribute', 'software', 'documentation', 'purpose', 'fee', 'granted', 'provided', 'copyright', 'notice', 'appear', 'copies', 'copyright', 'notice', 'permission', 'notice', 'appear', 'supporting', 'documentation', 'stichting', 'mathematisch', 'centrum', 'cwi', 'advertising', 'publicity', 'pertaining', 'distribution', 'software', 'specific', 'written', 'prior', 'permission', 'stichting', 'mathematisch', 'centrum', 'disclaims', 'warranties', 'regard', 'software', 'including', 'implied', 'warranties', 'merchantability', 'fitness', 'event', 'shall', 'stichting', 'mathematisch', 'centrum', 'liable', 'special', 'indirect', 'consequential', 'damages', 'damages', 'whatsoever', 'resulting', 'loss', 'use', 'data', 'profits', 'action', 'contract', 'negligence', 'tortious', 'action', 'arising', 'connection', 'use', 'performance', 'software'] \n",
      "\n",
      "------------------\n",
      "------------------\n",
      "\n",
      "Sentence 1: mypy (and mypyc) are licensed under the terms of the mit license, reproduced below.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 2: = = = = =\n",
      "\n",
      "the mit license\n",
      "\n",
      "copyright (c) 2012-2023 jukka lehtosalo and contributors\n",
      "copyright (c) 2015-2023 dropbox, inc.\n",
      "\n",
      "permission is hereby granted, free of charge, to any person obtaining a\n",
      "copy of this software and associated documentation files (the \"software\"),\n",
      "to deal in the software without restriction, including without limitation\n",
      "the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
      "and/or sell copies of the software, and to permit persons to whom the\n",
      "software is furnished to do so, subject to the following conditions:\n",
      "\n",
      "the above copyright notice and this permission notice shall be included in\n",
      "all copies or substantial portions of the software.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 3: the software is provided \"as is\", without warranty of any kind, express or\n",
      "implied, including but not limited to the warranties of merchantability,\n",
      "fitness for a particular purpose and noninfringement.\n",
      "------------------\n",
      "\n",
      "Sentence 4: in no event shall the\n",
      "authors or copyright holders be liable for any claim, damages or other\n",
      "liability, whether in an action of contract, tort or otherwise, arising\n",
      "from, out of or in connection with the software or the use or other\n",
      "dealings in the software.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 5: = = = = =\n",
      "\n",
      "portions of mypy and mypyc are licensed under different licenses.\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 6: the files\n",
      "mypyc/lib-rt/pythonsupport.h, mypyc/lib-rt/getargs.c and\n",
      "mypyc/lib-rt/getargsfast.c are licensed under the psf 2 license, reproduced\n",
      "below.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 7: = = = = =\n",
      "\n",
      "python software foundation license version 2\n",
      "--------------------------------------------\n",
      "\n",
      "1.\n",
      "------------------\n",
      "\n",
      "Sentence 8: this license agreement is between the python software foundation\n",
      "(\"psf\"), and the individual or organization (\"licensee\") accessing and\n",
      "otherwise using this software (\"python\") in source or binary form and\n",
      "its associated documentation.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 9: 2. subject to the terms and conditions of this license agreement, psf hereby\n",
      "grants licensee a nonexclusive, royalty-free, world-wide license to reproduce,\n",
      "analyze, test, perform and/or display publicly, prepare derivative works,\n",
      "distribute, and otherwise use python alone or in any derivative version,\n",
      "provided, however, that psf's license agreement and psf's notice of copyright,\n",
      "i.e., \"copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n",
      "2011, 2012 python software foundation; all rights reserved\" are retained in python\n",
      "alone or in any derivative version prepared by licensee.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 10: 3. in the event licensee prepares a derivative work that is based on\n",
      "or incorporates python or any part thereof, and wants to make\n",
      "the derivative work available to others as provided herein, then\n",
      "licensee hereby agrees to include in any such work a brief summary of\n",
      "the changes made to python.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 11: 4. psf is making python available to licensee on an \"as is\"\n",
      "basis.  \n",
      "------------------\n",
      "\n",
      "Sentence 12: psf makes no representations or warranties, express or\n",
      "implied.  \n",
      "------------------\n",
      "\n",
      "Sentence 13: by way of example, but not limitation, psf makes no and\n",
      "disclaims any representation or warranty of merchantability or fitness\n",
      "for any particular purpose or that the use of python will not\n",
      "infringe any third party rights.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 14: 5. psf shall not be liable to licensee or any other users of python\n",
      "for any incidental, special, or consequential damages or loss as\n",
      "a result of modifying, distributing, or otherwise using python,\n",
      "or any derivative thereof, even if advised of the possibility thereof.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 15: 6.\n",
      "------------------\n",
      "\n",
      "Sentence 16: this license agreement will automatically terminate upon a material\n",
      "breach of its terms and conditions.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 17: 7. nothing in this license agreement shall be deemed to create any\n",
      "relationship of agency, partnership, or joint venture between psf and\n",
      "licensee.  \n",
      "------------------\n",
      "\n",
      "Sentence 18: this license agreement does not grant permission to use psf\n",
      "trademarks or trade name in a trademark sense to endorse or promote\n",
      "products or services of licensee, or any third party.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 19: 8.\n",
      "------------------\n",
      "\n",
      "Sentence 20: by copying, installing or otherwise using python, licensee\n",
      "agrees to be bound by the terms and conditions of this license\n",
      "agreement.\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 21: beopen.com license agreement for python 2.0\n",
      "-------------------------------------------\n",
      "\n",
      "beopen python open source license agreement version 1\n",
      "\n",
      "1.\n",
      "------------------\n",
      "\n",
      "Sentence 22: this license agreement is between beopen.com (\"beopen\"), having an\n",
      "office at 160 saratoga avenue, santa clara, ca 95051, and the\n",
      "individual or organization (\"licensee\") accessing and otherwise using\n",
      "this software in source or binary form and its associated\n",
      "documentation (\"the software\").\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 23: 2. subject to the terms and conditions of this beopen python license\n",
      "agreement, beopen hereby grants licensee a non-exclusive,\n",
      "royalty-free, world-wide license to reproduce, analyze, test, perform\n",
      "and/or display publicly, prepare derivative works, distribute, and\n",
      "otherwise use the software alone or in any derivative version,\n",
      "provided, however, that the beopen python license is retained in the\n",
      "software, alone or in any derivative version prepared by licensee.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 24: 3. beopen is making the software available to licensee on an \"as is\"\n",
      "basis.  \n",
      "------------------\n",
      "\n",
      "Sentence 25: beopen makes no representations or warranties, express or\n",
      "implied.  \n",
      "------------------\n",
      "\n",
      "Sentence 26: by way of example, but not limitation, beopen makes no and\n",
      "disclaims any representation or warranty of merchantability or fitness\n",
      "for any particular purpose or that the use of the software will not\n",
      "infringe any third party rights.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 27: 4. beopen shall not be liable to licensee or any other users of the\n",
      "software for any incidental, special, or consequential damages or loss\n",
      "as a result of using, modifying or distributing the software, or any\n",
      "derivative thereof, even if advised of the possibility thereof.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 28: 5.\n",
      "------------------\n",
      "\n",
      "Sentence 29: this license agreement will automatically terminate upon a material\n",
      "breach of its terms and conditions.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 30: 6.\n",
      "------------------\n",
      "\n",
      "Sentence 31: this license agreement shall be governed by and interpreted in all\n",
      "respects by the law of the state of california, excluding conflict of\n",
      "law provisions.  \n",
      "------------------\n",
      "\n",
      "Sentence 32: nothing in this license agreement shall be deemed to\n",
      "create any relationship of agency, partnership, or joint venture\n",
      "between beopen and licensee.  \n",
      "------------------\n",
      "\n",
      "Sentence 33: this license agreement does not grant\n",
      "permission to use beopen trademarks or trade names in a trademark\n",
      "sense to endorse or promote products or services of licensee, or any\n",
      "third party.  \n",
      "------------------\n",
      "\n",
      "Sentence 34: as an exception, the \"beopen python\" logos available at\n",
      "http://www.pythonlabs.com/logos.html may be used according to the\n",
      "permissions granted on that web page.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 35: 7. by copying, installing or otherwise using the software, licensee\n",
      "agrees to be bound by the terms and conditions of this license\n",
      "agreement.\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 36: cnri license agreement for python 1.6.1\n",
      "---------------------------------------\n",
      "\n",
      "1.\n",
      "------------------\n",
      "\n",
      "Sentence 37: this license agreement is between the corporation for national\n",
      "research initiatives, having an office at 1895 preston white drive,\n",
      "reston, va 20191 (\"cnri\"), and the individual or organization\n",
      "(\"licensee\") accessing and otherwise using python 1.6.1 software in\n",
      "source or binary form and its associated documentation.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 38: 2. subject to the terms and conditions of this license agreement, cnri\n",
      "hereby grants licensee a nonexclusive, royalty-free, world-wide\n",
      "license to reproduce, analyze, test, perform and/or display publicly,\n",
      "prepare derivative works, distribute, and otherwise use python 1.6.1\n",
      "alone or in any derivative version, provided, however, that cnri's\n",
      "license agreement and cnri's notice of copyright, i.e., \"copyright (c)\n",
      "1995-2001 corporation for national research initiatives; all rights\n",
      "reserved\" are retained in python 1.6.1 alone or in any derivative\n",
      "version prepared by licensee.  \n",
      "------------------\n",
      "\n",
      "Sentence 39: alternately, in lieu of cnri's license\n",
      "agreement, licensee may substitute the following text (omitting the\n",
      "quotes): \"python 1.6.1 is made available subject to the terms and\n",
      "conditions in cnri's license agreement.  \n",
      "------------------\n",
      "\n",
      "Sentence 40: this agreement together with\n",
      "python 1.6.1 may be located on the internet using the following\n",
      "unique, persistent identifier (known as a handle): 1895.22/1013.  \n",
      "------------------\n",
      "\n",
      "Sentence 41: this\n",
      "agreement may also be obtained from a proxy server on the internet\n",
      "using the following url: http://hdl.handle.net/1895.22/1013\".\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 42: 3. in the event licensee prepares a derivative work that is based on\n",
      "or incorporates python 1.6.1 or any part thereof, and wants to make\n",
      "the derivative work available to others as provided herein, then\n",
      "licensee hereby agrees to include in any such work a brief summary of\n",
      "the changes made to python 1.6.1.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 43: 4. cnri is making python 1.6.1 available to licensee on an \"as is\"\n",
      "basis.  \n",
      "------------------\n",
      "\n",
      "Sentence 44: cnri makes no representations or warranties, express or\n",
      "implied.  \n",
      "------------------\n",
      "\n",
      "Sentence 45: by way of example, but not limitation, cnri makes no and\n",
      "disclaims any representation or warranty of merchantability or fitness\n",
      "for any particular purpose or that the use of python 1.6.1 will not\n",
      "infringe any third party rights.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 46: 5. cnri shall not be liable to licensee or any other users of python\n",
      "1.6.1 for any incidental, special, or consequential damages or loss as\n",
      "a result of modifying, distributing, or otherwise using python 1.6.1,\n",
      "or any derivative thereof, even if advised of the possibility thereof.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 47: 6.\n",
      "------------------\n",
      "\n",
      "Sentence 48: this license agreement will automatically terminate upon a material\n",
      "breach of its terms and conditions.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 49: 7.\n",
      "------------------\n",
      "\n",
      "Sentence 50: this license agreement shall be governed by the federal\n",
      "intellectual property law of the united states, including without\n",
      "limitation the federal copyright law, and, to the extent such\n",
      "u.s. federal law does not apply, by the law of the commonwealth of\n",
      "virginia, excluding virginia's conflict of law provisions.\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 51: notwithstanding the foregoing, with regard to derivative works based\n",
      "on python 1.6.1 that incorporate non-separable material that was\n",
      "previously distributed under the gnu general public license (gpl), the\n",
      "law of the commonwealth of virginia shall govern this license\n",
      "agreement only as to issues arising under or with respect to\n",
      "paragraphs 4, 5, and 7 of this license agreement.  \n",
      "------------------\n",
      "\n",
      "Sentence 52: nothing in this\n",
      "license agreement shall be deemed to create any relationship of\n",
      "agency, partnership, or joint venture between cnri and licensee.  \n",
      "------------------\n",
      "\n",
      "Sentence 53: this\n",
      "license agreement does not grant permission to use cnri trademarks or\n",
      "trade name in a trademark sense to endorse or promote products or\n",
      "services of licensee, or any third party.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 54: 8.\n",
      "------------------\n",
      "\n",
      "Sentence 55: by clicking on the \"accept\" button where indicated, or by copying,\n",
      "installing or otherwise using python 1.6.1, licensee agrees to be\n",
      "bound by the terms and conditions of this license agreement.\n",
      "\n",
      "        \n",
      "------------------\n",
      "\n",
      "Sentence 56: accept\n",
      "\n",
      "\n",
      "cwi license agreement for python 0.9.0 through 1.2\n",
      "--------------------------------------------------\n",
      "\n",
      "copyright (c) 1991 - 1995, stichting mathematisch centrum amsterdam,\n",
      "the netherlands.  \n",
      "------------------\n",
      "\n",
      "Sentence 57: all rights reserved.\n",
      "\n",
      "permission to use, copy, modify, and distribute this software and its\n",
      "documentation for any purpose and without fee is hereby granted,\n",
      "provided that the above copyright notice appear in all copies and that\n",
      "both that copyright notice and this permission notice appear in\n",
      "supporting documentation, and that the name of stichting mathematisch\n",
      "centrum or cwi not be used in advertising or publicity pertaining to\n",
      "distribution of the software without specific, written prior\n",
      "permission.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Sentence 58: stichting mathematisch centrum disclaims all warranties with regard to\n",
      "this software, including all implied warranties of merchantability and\n",
      "fitness, in no event shall stichting mathematisch centrum be liable\n",
      "for any special, indirect or consequential damages or any damages\n",
      "whatsoever resulting from loss of use, data or profits, whether in an\n",
      "action of contract, negligence or other tortious action, arising out\n",
      "of or in connection with the use or performance of this software.\n",
      "\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the MIT License or any document as a string\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "  new_corpus = file.read()\n",
    "  new_corpus = new_corpus.lower()\n",
    "\n",
    "# Use spaCy to process the document\n",
    "doc = nlp(new_corpus)\n",
    "\n",
    "# Tokenize the document and print each token\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Tokenize the document, filter out punctuation and stopwords\n",
    "# Keep only alphabetic tokens (words), remove punctuation and stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token.text for token in doc if token.is_alpha and not token.is_stop and token.text.lower() not in stop_words]\n",
    "print(f\"\\nFiltered Tokens (without stopwords): {filtered_tokens} \\n\\n------------------\\n------------------\\n\" )\n",
    "\n",
    "# Optionally, if you want to see sentences segmented by spaCy:\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for idx, sentence in enumerate(sentences):\n",
    "  print(f\"Sentence {idx+1}: {sentence}\\n------------------\\n\" )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Closing Thoughts\n",
    "Through this Notebooks Parham performed the following activities:\n",
    "- Acquired knowlege of how tokenize a text into subword using **BPE**\n",
    "- Normalized text into standard format using the **Porter Stemmer**\n",
    "- Segment sentences using `spaCy` from a given text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
