{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAwm4RYHosH5",
    "outputId": "ce4bed62-16b9-4499-e819-80d8f9e967d5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Example corpus\n",
    "tokens = brown.words()  # get the words from the brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU4CxWgtosH8"
   },
   "source": [
    "## 1. Unigram\n",
    "\n",
    "A unigram model in NLP is a type of probabilistic language model used for predicting the next item in a sequence as a single unit (word) independent of its preceding or following words. This model assumes that the probability of each word occurring in a text is independent of the words around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Epyj4wkNqFgE"
   },
   "source": [
    "We can find such probabilities and use them to generate sentences, as you'll find out in the next exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TvUmL5sTosH-"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "probabilities = (\n",
    "    {}\n",
    ")  # dictionary to store the probabilities in the format word:probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "gYaZ_xNoosH_"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Calculating word frequencies\n",
    "frequency = Counter(tokens)\n",
    "total_words = sum(frequency.values())\n",
    "\n",
    "# Converting frequencies to probabilities\n",
    "probabilities = {word: freq / total_words for word, freq in frequency.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1X9lcAxhosH_",
    "outputId": "ae78f9fb-64c5-4025-df76-c19b032ab536"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "1.0853868324836725e-37\n"
     ]
    }
   ],
   "source": [
    "# let's get the probability of a sentence\n",
    "sentence = tokens[\n",
    "    0:10\n",
    "]  # \"The Fulton County Grand Jury said Friday an investigation of\"\n",
    "\n",
    "sentence_prob = 1\n",
    "\n",
    "for word in sentence:\n",
    "    sentence_prob *= probabilities[word]\n",
    "\n",
    "print(sentence)\n",
    "print(sentence_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wLPWShwposH_"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "\n",
    "def generate_sentence(length, probabilities):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "xu2wrX5AosIA"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def generate_sentence(length, probabilities):\n",
    "    sentence = []\n",
    "    words = list(probabilities.keys())\n",
    "    word_probabilities = list(probabilities.values())\n",
    "    for _ in range(length):\n",
    "        word = random.choices(words, weights=word_probabilities)[0]\n",
    "        sentence.append(word)\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Di1j3JREosIA",
    "outputId": "f728ba25-b04d-45bb-c7ae-63e9c05851aa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "used of was , Thomas economic editors . with of\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(10, probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDcqnIphosIA"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "- What are the limitations of generating text using a unigram model?\n",
    "- How might the sentences differ if a bigram or trigram model were used instead?\n",
    "- What improvements might be considered for a more realistic text generation given the constraint of using a unigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AIM-BGQosIB"
   },
   "source": [
    "## 2. Bigram LM\n",
    "\n",
    "A bigram model is used for predicting the next word in a sequence based on the previous word. It is a simple form of n-gram model where n is equal to 2. The concept of a bigram model is rooted in the Markov assumption that the probability of a word depends only on a finite history of previous words. In the case of a bigram, just the immediately preceding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NR6abCRvosIB"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "bigram_probabilities = (\n",
    "    {}\n",
    ")  # dictionary to store the probabilities in the format word_i:word_{i+1}:probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "BANZTJ-OosIB"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Calculating bigram frequencies\n",
    "bigram_freq = Counter(bigrams(tokens))\n",
    "total_bigrams = sum(bigram_freq.values())\n",
    "\n",
    "# Building bigram probabilities\n",
    "bigram_probabilities = defaultdict(dict)\n",
    "for (w1, w2), freq in bigram_freq.items():\n",
    "    bigram_probabilities[w1][w2] = freq / total_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jAS3QaOosIC",
    "outputId": "3b0c3ee1-cc72-4c6d-ecc1-bf1e770a42af"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Fulton 8.611847663304314e-07\n",
      "The jury 7.750662896973882e-06\n",
      "The September-October 8.611847663304314e-07\n",
      "The grand 1.7223695326608628e-06\n",
      "The City 4.305923831652157e-06\n",
      "The jurors 8.611847663304314e-07\n",
      "The couple 1.7223695326608628e-06\n",
      "The petition 1.7223695326608628e-06\n",
      "The Hartsfield 8.611847663304314e-07\n",
      "The mayor's 8.611847663304314e-07\n",
      "Fulton County 5.1671085979825885e-06\n",
      "Fulton Superior 1.7223695326608628e-06\n",
      "Fulton legislators 1.7223695326608628e-06\n",
      "Fulton taxpayers 8.611847663304314e-07\n",
      "Fulton ordinary's 8.611847663304314e-07\n",
      "Fulton Tax 8.611847663304314e-07\n",
      "Fulton Health 8.611847663304314e-07\n",
      "Fulton to 8.611847663304314e-07\n",
      "Fulton was 8.611847663304314e-07\n",
      "Fulton , 8.611847663304314e-07\n"
     ]
    }
   ],
   "source": [
    "for k1 in list(bigram_probabilities.keys())[:2]:\n",
    "    for k2 in list(bigram_probabilities[k1].keys())[:10]:\n",
    "        print(k1, k2, bigram_probabilities[k1][k2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QWxDvr7ZosIC"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "\n",
    "def generate_bigram_sentence(start_word, length, bigram_probabilities):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L_Ph1_z6osIC",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "\n",
    "def generate_bigram_sentence(start_word, length, bigram_probabilities):\n",
    "    if start_word not in bigram_probabilities:\n",
    "        raise ValueError(\"Start word not in bigram probabilities\")\n",
    "    sentence = [start_word]\n",
    "    current_word = start_word\n",
    "    for _ in range(length - 1):\n",
    "        next_word = random.choices(\n",
    "            list(bigram_probabilities[current_word].keys()),\n",
    "            weights=bigram_probabilities[current_word].values(),\n",
    "        )[0]\n",
    "        sentence.append(next_word)\n",
    "        current_word = next_word\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PBlnn_tosID",
    "outputId": "67666e84-7708-4ae5-919a-529bd9dee6a5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "billing . That Prokofieff's own complex of what the expressions in the Sherman Act , get a family affair .\n"
     ]
    }
   ],
   "source": [
    "# Generate a 10-word sentence starting with a given word\n",
    "start_word = random.choice(list(bigram_probabilities.keys()))\n",
    "print(generate_bigram_sentence(start_word, 20, bigram_probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzmZEOffosID"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "- Compare the sentences generated by the unigram and bigram models. Which model produces more coherent sentences?\n",
    "- What are the limitations of a bigram model in text generation?\n",
    "- How do the results change if starting words are varied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8H2RcGPGosID"
   },
   "source": [
    "## 3. Word2Vec\n",
    "\n",
    "We have already seen Word2Vec. Unlike the previously discussed unigram and bigram models, which focus on word frequencies or word sequences, Word2Vec captures the semantic relationships between words in a way that preserves context and similarity. Can we make use of these relationships in order to generate sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GVvLJhURosID"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "WINDOW_SIZE = 3\n",
    "\n",
    "tokenized_sentences = brown.sents()\n",
    "model = Word2Vec(\n",
    "    tokenized_sentences, vector_size=100, window=WINDOW_SIZE, min_count=1, sg=1\n",
    ")  # sg=1 -> skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_1pCE-GosID"
   },
   "source": [
    "Word2Vec itself is not a language model in the traditional sense, as it doesn't model the probability distribution of a sequence of words in a language. As we already discussed, it is primarily used to learn vector representations of words that capture semantic similarities and relationships between them. Still, we can use this model to predict context words, which may appear in the same window as target words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Mhcuhii2osIE"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "\n",
    "def generate_sentence(start_word, max_length=10):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "QHykc2-wosIE"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "\n",
    "def generate_sentence(start_word, max_length=10):\n",
    "    if start_word not in model.wv:\n",
    "        return \"Word not in vocabulary!\"\n",
    "\n",
    "    current_words = [start_word]\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length - 1):\n",
    "        # Find most similar words\n",
    "        similar_words = model.wv.most_similar(current_words, topn=10)\n",
    "        # print(similar_words)\n",
    "        next_word = similar_words[0][0]  # pick the most similar word\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        # Update current words\n",
    "        if len(current_words) == WINDOW_SIZE:\n",
    "            current_words.pop(0)\n",
    "        current_words.append(next_word)\n",
    "\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2gcHGm_osIE",
    "outputId": "d535b3db-256f-44da-ba81-3b7da7af7b97"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated sentence from 'Rio': Rio Stamford Peck Palm Turner Stamford Peck Palm Turner Stamford\n"
     ]
    }
   ],
   "source": [
    "# Generate a sentence from a random starting word\n",
    "random_start_word = random.choice(list(model.wv.index_to_key))\n",
    "generated_sentence = generate_sentence(random_start_word, 10)\n",
    "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8s9iOruoHO1"
   },
   "source": [
    "### 3.1 Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb0JtVUaoHO1"
   },
   "source": [
    "#### Uniform Top-K\n",
    "\n",
    "As you can see in the solution of the previous exercise, one way of picking the next word is taking the most similar. Such a method is called Argmax Sampling. For the next exercise, code and see what happens when we sample uniformly from the 10 most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uasw163voHO1"
   },
   "outputs": [],
   "source": [
    "def generate_sentence_with_sampling(start_word, sample_func, max_length=10, topk=10):\n",
    "    if start_word not in model.wv:\n",
    "        return \"Word not in vocabulary!\"\n",
    "\n",
    "    current_words = [start_word]\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length - 1):\n",
    "        # Find most similar words\n",
    "        similar_words = model.wv.most_similar(current_words, topn=topk)\n",
    "        next_word = sample_func(similar_words)\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        # Update current words\n",
    "        if len(current_words) == WINDOW_SIZE:\n",
    "            current_words.pop(0)\n",
    "        current_words.append(next_word)\n",
    "\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "P-Xu81ApoHO1"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "\n",
    "def sample_unif_topk(start_word, max_length=10):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "4RFF-uDgoHO2"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "\n",
    "def sample_unif_topk(similar_words):\n",
    "    return random.choices([word for word, _ in similar_words], k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKgPLAodoHO2",
    "outputId": "4052cdc0-522e-4c93-fe99-993a1285f511"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated sentence from 'Rondo': Rondo restoring Georgia's well-established Dusseldorf Missouri's Sherlock Panza dip piety\n"
     ]
    }
   ],
   "source": [
    "# Generate a sentence from a random starting word\n",
    "random_start_word = random.choice(list(model.wv.index_to_key))\n",
    "generated_sentence = generate_sentence_with_sampling(\n",
    "    random_start_word, sample_unif_topk, 10\n",
    ")\n",
    "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9A0PbX0oHO2"
   },
   "source": [
    "#### Top-K Sampling\n",
    "\n",
    "Sampling uniformly from the top-k similar words introduces significant variety into the text. However, expanding the selection to a larger pool, such as 100 or 1000 similar words, could introduce excessive noise if we continue to sample uniformly. To refine this approach, you can enhance the sampling method by weighting the probability of each word according to its similarity score. To do so, we may rely on python's `random.choice()` or implement inverse transform sampling directly.\n",
    "\n",
    "The next exercise involves coding this weighted sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TuZd4WehoHO3"
   },
   "outputs": [],
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "\n",
    "def sample_topk(similar_words):\n",
    "    pass\n",
    "\n",
    "\n",
    "def sample_topk_inverse_transform(similar_words):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "id": "dmE7oI0UoHO3"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "\n",
    "\n",
    "# relying on python's random.choices to sample based on the weights\n",
    "def sample_topk(similar_words):\n",
    "    words = []\n",
    "    weights = []\n",
    "    for word, similarity in similar_words:\n",
    "        words.append(word)\n",
    "        weights.append(similarity)\n",
    "    return random.choices(words, weights=weights, k=1)[0]\n",
    "\n",
    "\n",
    "# implement inverse transform sampling\n",
    "def sample_topk_inverse_transform(similar_words):\n",
    "    words = []\n",
    "    weights = []\n",
    "    for word, similarity in similar_words:\n",
    "        words.append(word)\n",
    "        weights.append(similarity)\n",
    "\n",
    "    # Normalize the weights to form a probability distribution\n",
    "    total_weight = sum(weights)\n",
    "    probabilities = [weight / total_weight for weight in weights]\n",
    "\n",
    "    # Create the cumulative distribution function (CDF)\n",
    "    cumulative_probabilities = []\n",
    "    cumulative_sum = 0\n",
    "    for p in probabilities:\n",
    "        cumulative_sum += p\n",
    "        cumulative_probabilities.append(cumulative_sum)\n",
    "\n",
    "    # Generate a random float in the range [0, 1]\n",
    "    u = random.random()\n",
    "\n",
    "    # Find the first index where the cumulative probability exceeds the random number\n",
    "    for index, cumulative_probability in enumerate(cumulative_probabilities):\n",
    "        if cumulative_probability > u:\n",
    "            return words[index]\n",
    "\n",
    "    # As a fallback, return the first word if something goes wrong\n",
    "    return words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46kyw607oHO4",
    "outputId": "68235104-b460-4ae6-c7df-9fe2af7e6760"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated sentence from 'subjectively': subjectively assented disarmed minded Koehler reassemble housekeeping Nehru proffered Palmer's\n",
      "Generated sentence from 'subjectively': subjectively Sidney Self Travel Late leering Contest Paula's Lamar Horne\n"
     ]
    }
   ],
   "source": [
    "# Generate a sentence from a random starting word\n",
    "random_start_word = random.choice(list(model.wv.index_to_key))\n",
    "generated_sentence = generate_sentence_with_sampling(\n",
    "    random_start_word, sample_topk, 10, topk=1000\n",
    ")\n",
    "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")\n",
    "\n",
    "generated_sentence = generate_sentence_with_sampling(\n",
    "    random_start_word, sample_topk_inverse_transform, 10, topk=1000\n",
    ")\n",
    "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4qkGf01osIE"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "- Why might sentences generated using the most similar word method lack coherence over longer sequences? How could the model be adjusted to produce more grammatically coherent outputs? With this perspective in mind, how could we modify the model, in order to also predict the position of the generated context word?\n",
    "- What are some methods to prevent the generated sentences from looping or becoming repetitive? How might introducing randomness or diversity in word selection impact the quality of the generated sentences?\n",
    "- How do different parameters of the Word2Vec model (like vector size, window size, and training algorithm) affect the outcome of the generated sentences? What happens when you alter these parameters?\n",
    "- Have you noticed anything by generating sentences starting with an arbitrary word (i.e. \"gensim\")? Describe the issue and potential solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAB86BnLoHO5"
   },
   "source": [
    "### 3.2 Beam Search (Extra)\n",
    "\n",
    "In this exercise, you will implement the beam search algorithm using Word2Vec embeddings to generate text. Beam search, a heuristic search algorithm, balances exploration and efficiency by keeping only the top-scoring word sequences at each step. You'll integrate this with Word2Vec, which provides word embeddings that capture semantic similarities, to guide the generation process.\n",
    "\n",
    "Through this task, explore how different settings for the beam width affect the coherence and diversity of the generated sentences, enhancing your understanding of both beam search mechanics and the practical use of word embeddings in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "\n",
    "\n",
    "def beam_search(start_word, beam_width=3, max_length=10):\n",
    "    pass"
   ],
   "metadata": {
    "id": "pjDQUlGOoRWC"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "-p4aZXwkoHO5"
   },
   "outputs": [],
   "source": [
    "# @title üëÄ Solution\n",
    "def beam_search(start_word, beam_width=3, max_length=10):\n",
    "    if start_word not in model.wv:\n",
    "        return \"Word not in vocabulary!\"\n",
    "\n",
    "    # Initialize the beam with the start word\n",
    "    beam = [(start_word, 0)]  # List of tuples (sentence, cumulative_score)\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "        candidates = []\n",
    "        # Explore each word in the beam\n",
    "        for sentence, cum_score in beam:\n",
    "            current_word = sentence.split()[-1]\n",
    "            # Find potential next words\n",
    "            try:\n",
    "                next_words = model.wv.most_similar(current_word, topn=beam_width)\n",
    "            except KeyError:\n",
    "                continue  # Skip if current_word has no similar words\n",
    "\n",
    "            # Add new sentences with updated scores\n",
    "            for next_word, sim in next_words:\n",
    "                new_sentence = sentence + \" \" + next_word\n",
    "                new_score = cum_score + sim  # Update cumulative score\n",
    "                candidates.append((new_sentence, new_score))\n",
    "\n",
    "        # Prune to keep only the top beam_width entries\n",
    "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "    # Choose the best sequence\n",
    "    best_sequence, _ = sorted(beam, key=lambda x: x[1], reverse=True)[0]\n",
    "    return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate a sentence from a random starting word\n",
    "random_start_word = random.choice(list(model.wv.index_to_key))\n",
    "generated_sentence = beam_search(random_start_word, beam_width=3, max_length=10)\n",
    "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9o9njzTYofdQ",
    "outputId": "3bea5fe7-16c2-4576-a7b0-08b3e2a6ed52"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated sentence from 'Body': Body supersonic geese smelling geese smelling geese smelling geese smelling\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "To enhance the beam search function and address potential issues such as repetition or lack of diversity in the generated sentences, one effective strategy is to introduce a penalty for sampled words. This approach helps to discourage the selection of the same word or similar words too frequently within a short segment of the generated text."
   ],
   "metadata": {
    "id": "Q15ZjpemqMED"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üßëüèø‚Äçüíª Your code here\n",
    "def beam_search_with_penalty(start_word, beam_width=3, max_length=10, penalty=0.1):\n",
    "    pass"
   ],
   "metadata": {
    "id": "yJnjlpwPqicF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üëÄ Solution\n",
    "def beam_search_with_penalty(start_word, beam_width=3, max_length=10, penalty=0.1):\n",
    "    if start_word not in model.wv:\n",
    "        return \"Word not in vocabulary!\"\n",
    "\n",
    "    # Initialize the beam with the start word\n",
    "    beam = [\n",
    "        (start_word, 0, [start_word])\n",
    "    ]  # Each entry is (sentence, cumulative_score, recent_words)\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "        candidates = []\n",
    "        # Explore each word in the beam\n",
    "        for sentence, cum_score, recent_words in beam:\n",
    "            current_word = sentence.split()[-1]\n",
    "            # Find potential next words\n",
    "            try:\n",
    "                next_words = model.wv.most_similar(\n",
    "                    current_word, topn=beam_width + len(recent_words)\n",
    "                )\n",
    "            except KeyError:\n",
    "                continue  # Skip if current_word has no similar words\n",
    "\n",
    "            # Apply penalty to similar words based on recent usage\n",
    "            filtered_words = []\n",
    "            for next_word, sim in next_words:\n",
    "                if next_word not in recent_words:\n",
    "                    filtered_words.append((next_word, sim))\n",
    "                else:\n",
    "                    # Apply penalty\n",
    "                    filtered_words.append((next_word, sim * (1 - penalty)))\n",
    "\n",
    "            # Keep only the top beam_width entries after applying penalty\n",
    "            filtered_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)[\n",
    "                :beam_width\n",
    "            ]\n",
    "\n",
    "            # Add new sentences with updated scores and update recent words\n",
    "            for next_word, sim in filtered_words:\n",
    "                new_sentence = sentence + \" \" + next_word\n",
    "                new_score = cum_score + sim  # Update cumulative score\n",
    "                new_recent_words = recent_words[-(beam_width - 1) :] + [\n",
    "                    next_word\n",
    "                ]  # Update recent words\n",
    "                candidates.append((new_sentence, new_score, new_recent_words))\n",
    "\n",
    "        # Prune to keep only the top beam_width entries\n",
    "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "    # Choose the best sequence\n",
    "    best_sequence, _, _ = sorted(beam, key=lambda x: x[1], reverse=True)[0]\n",
    "    return best_sequence"
   ],
   "metadata": {
    "cellView": "form",
    "id": "2yIm19u0pTd-"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate a sentence from a random starting word\n",
    "random_start_word = random.choice(list(model.wv.index_to_key))\n",
    "generated_sentence = beam_search_with_penalty(\n",
    "    random_start_word, beam_width=100, max_length=100, penalty=1\n",
    ")\n",
    "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyOy0WWipWW8",
    "outputId": "9e0eb839-b693-474c-dc65-cc22515f7589"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated sentence from 'unfastened': unfastened triangular boarding supersonic geese Cemetery Palm Turner Snow's Janssen Campbell Clifford Moody Blumberg Spurdle celery Von Worthy Faber Freight Minnett '48 Gene-Princess Pembina Staten Bench Compson Peck Motor Dog Mail Musical France's dictators excitatory fermentation Erich chisel sediments pickoff electrode 5000 tomato biscuits oxalate centering olive idioms Projects bulbs engraved fermented jejunum butts hikes Cr filtering nitrogen tumble Turk summertime aft excesses Carnival canvases backwoods Leopoldville suppression tribunals Chesapeake Tudor negotiated rubbish Investment Savings Marin Simpson Trophy Vocational scours ketosis Rall Pride-Starlette '58 Mon-Columbia Heel-Miracle Plate Area teaspoon adjectives walnuts maple shrinkage diarrhea Loan elution Des Electrical expandable vapor-pressure\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Designing an Autoregressive Word2Vec-Inspired Model for Position-Aware Language Generation (Extra)\n",
    "\n",
    "Word2Vec is a powerful tool for generating vector representations of words, capturing their semantic relationships based on their contexts. However, traditional Word2Vec models do not account for the order or position of words in context; they merely predict context words regardless of their specific positions relative to the target word. This limitation can be addressed by integrating positional awareness into a model inspired by Word2Vec but designed for language generation.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Propose a detailed architecture for an autoregressive model that not only predicts context words but also their positions relative to the current word, similar to how Word2Vec captures contextual word relationships. Your proposed model should be capable of generating coherent language sequences by considering both the semantic similarity of words and their positional dynamics in sentences.\n",
    "\n",
    "Points to consider in your proposal:\n",
    "\n",
    "1. **Model Foundation**: Describe how you would modify the foundational architecture of Word2Vec (either CBOW or Skip-gram) to include positional information. What changes would be necessary to incorporate the sequential nature of language?\n",
    "\n",
    "2. **Position Encoding**: How would you encode positional information within the model? Consider techniques used in other models like transformers (e.g., positional encodings) and describe how they could be adapted or improved for this use case.\n",
    "\n",
    "3. **Autoregressive Mechanism**: Explain how the model would generate text in an autoregressive manner, particularly how it would use the positionally-aware embeddings to predict the next word in a sequence. What kind of neural network architecture would be suitable for this?\n",
    "\n",
    "4. **Training Strategy**: Discuss potential strategies for training this model. What kind of corpus and preprocessing would be needed? How would you define the loss function to include accuracy not just for predicting the right word, but also placing it correctly?\n",
    "\n",
    "5. **Potential Applications and Benefits**: Highlight how this position-aware model could outperform traditional Word2Vec models in specific NLP tasks. Consider applications in machine translation, summarization, or interactive dialog systems.\n",
    "\n",
    "Your response should outline the theoretical framework, practical considerations for implementation, and possible challenges you might face with this architecture. This exercise aims to deepen your understanding of how word embeddings can be extended beyond traditional models to enhance language generation capabilities in NLP systems."
   ],
   "metadata": {
    "id": "lj5PHhggujp6"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}