{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "### 2.2 Language Modeling (LM) and Sampling from a LM\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "\n",
        "- Language Modeling (LM) with bigram\n",
        "- How to sample from a LM\n",
        "- Different sampling strategy\n",
        "\n",
        "Prerequisites:\n",
        "\n",
        "- Python\n",
        "- numpy\n",
        "\n",
        "#### Authors\n",
        "\n",
        "<br>Prof. Iacopo Masi and Prof. Stefano Faralli\n",
        "\n",
        "TA: Robert Adrian Minut"
      ],
      "metadata": {
        "id": "trYFWn16a-lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Requirements\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0bAQGt0jAzlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Toolkit (NLTK)\n",
        "\n",
        "![NLTK](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dNH8WI8Oy3etClaRvRCgw.png)\n",
        "\n",
        "https://www.nltk.org/howto.html\n",
        "\n",
        "NLTK, or the Natural Language Toolkit, is a powerful open-source library in Python that provides tools for working with human language data. It's widely used in Natural Language Processing (NLP) for tasks such as tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Text Processing:** NLTK allows for tokenizing, part-of-speech tagging, and named entity recognition.\n",
        "- **Corpora Support:** The library comes with a wide range of linguistic data, including corpora like the Brown Corpus and the WordNet lexical database.\n",
        "- **Machine Learning:** It supports classification, clustering, and other machine learning techniques specifically designed for language processing.\n",
        "\n",
        "## Installation\n",
        "\n",
        "You can install NLTK using `pip`:\n",
        "\n",
        "```bash\n",
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "yembgTwjDmFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us download the brown and punkt corpus"
      ],
      "metadata": {
        "id": "5Wv__LE6A4jX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAwm4RYHosH5",
        "outputId": "2939cbd8-7743-4e97-8dd5-4eb78e51663d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example corpus\n",
        "tokens = brown.words() # get the words from the brown corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU4CxWgtosH8"
      },
      "source": [
        "## 1. Unigram\n",
        "\n",
        "A unigram model in NLP is a type of probabilistic language model used for predicting the next item in a sequence as a single unit (word) independent of its preceding or following words. This model assumes that the probability of each word occurring in a text is independent of the words around it.\n",
        "\n",
        "\n",
        "Modeling Fully the probabilities of a sentence\n",
        "\n",
        "![probs](https://miro.medium.com/v2/resize:fit:968/format:webp/1*vkvHegfkMxfjZ8hrqa1aAQ.png)\n",
        "\n",
        "### Unigram assumes all words independent\n",
        "\n",
        "![unigram](https://miro.medium.com/v2/resize:fit:1252/format:webp/1*qF3ZRwKDgmAG4cUBw3XeGA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epyj4wkNqFgE"
      },
      "source": [
        "### **EXERCISE 1** üíª\n",
        "\n",
        "Find such probabilities and use them to generate sentences, as you'll find out in the next exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvUmL5sTosH-"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "probabilities = {} # dictionary to store the probabilities in the format word:probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gYaZ_xNoosH_"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Calculating word frequencies\n",
        "frequency = Counter(tokens)\n",
        "total_words = sum(frequency.values())\n",
        "\n",
        "# Converting frequencies to probabilities\n",
        "probabilities = {word: freq / total_words for word, freq in frequency.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 2** üíª\n",
        "\n",
        "Compute the probability of the sentence\n",
        "\n",
        "\n",
        "> \"The Fulton County Grand Jury said Friday an investigation of\""
      ],
      "metadata": {
        "id": "qe1GDYLWG0TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "sentence = tokens[0:10] #\"The Fulton County Grand Jury said Friday an investigation of\""
      ],
      "metadata": {
        "id": "D9ZtxLwOG8LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X9lcAxhosH_",
        "outputId": "534c56fc-4f7c-46ef-c690-30e8d104fd36",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
            "1.0853868324836725e-37\n"
          ]
        }
      ],
      "source": [
        "# @title üëÄ Solution\n",
        "# let's get the probability of a sentence\n",
        "sentence = tokens[0:10] #\"The Fulton County Grand Jury said Friday an investigation of\"\n",
        "\n",
        "sentence_prob = 1\n",
        "\n",
        "for word in sentence:\n",
        "    sentence_prob *= probabilities[word]\n",
        "\n",
        "print(sentence)\n",
        "print(sentence_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 3** üíª\n",
        "\n",
        "Given the unigram model that we developed so far, write a function to sample from the unigram model possibile sentences of length `10`.\n",
        "\n",
        "Then run this function 10 times and check the result."
      ],
      "metadata": {
        "id": "ljtCTocDIMSX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLPWShwposH_"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def generate_sentence(length, probabilities):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xu2wrX5AosIA"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def generate_sentence(length, probabilities):\n",
        "    sentence = []\n",
        "    words = list(probabilities.keys())\n",
        "    word_probabilities = list(probabilities.values())\n",
        "    for _ in range(length):\n",
        "        word = random.choices(words, weights=word_probabilities)[0]\n",
        "        sentence.append(word)\n",
        "    return ' '.join(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di1j3JREosIA",
        "outputId": "bfcc6394-01e6-4249-fa6a-ec11d6d4928e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "used of was , Thomas economic editors . with of\n",
            ", -- of , afford Washington , estimated probate said\n",
            "London know it the bucking it . . B decided\n",
            "streets presentation House Chadwick is long plot case drunk their\n",
            "small `` , to . , . was say on\n",
            "two , for cared absolutely could the whose the is\n",
            "Committeemen room I ; remain clubs , of a for\n",
            ", goodbye backgrounds a ignored so manage ) and and\n",
            "violence and only lock at , Blackwell before . ``\n",
            ". ? surely with that is SX-21 A pallid character\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(generate_sentence(10, probabilities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDcqnIphosIA"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "- What are the limitations of generating text using a unigram model?\n",
        "- How might the sentences differ if a bigram or trigram model were used instead?\n",
        "- What improvements might be considered for a more realistic text generation given the constraint of using a unigram model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AIM-BGQosIB"
      },
      "source": [
        "## 2. Bigram LM and beyond\n",
        "\n",
        "A bigram model is used for predicting the next word in a sequence based on the previous word. It is a simple form of n-gram model where n is equal to 2. The concept of a bigram model is rooted in the **Markov assumption** that the probability of a word depends only on a finite history of previous words. In the case of a bigram, just the immediately preceding word.\n",
        "\n",
        "$$\n",
        "P(w_1,\\ldots,w_m) = \\prod^m_{i=1} P(w_i\\mid w_1,\\ldots,w_{i-1})\\approx \\prod^m_{i=2} P(w_i\\mid w_{i-(n-1)},\\ldots,w_{i-1})\n",
        "$$\n",
        "\n",
        "### We estimate Bigram via MLE (frequency-based estimator)\n",
        "\n",
        "$$\n",
        "P(w_i\\mid w_{i-(n-1)},\\ldots,w_{i-1}) = \\frac{\\mathrm{count}(w_{i-(n-1)},\\ldots,w_{i-1},w_i)}{\\mathrm{count}(w_{i-(n-1)},\\ldots,w_{i-1})}\n",
        "$$\n",
        "\n",
        "## Let us make an example\n",
        "\n",
        "> I saw the red house\n",
        "\n",
        "with bigram it becomes:\n",
        "\n",
        "$$\n",
        "P(\\text{I, saw, the, red, house}) \\approx P(\\text{I}\\mid\\langle s\\rangle) P(\\text{saw}\\mid \\text{I}) P(\\text{the}\\mid\\text{saw}) P(\\text{red}\\mid\\text{the}) P(\\text{house}\\mid\\text{red}) P(\\langle /s\\rangle\\mid \\text{house})\n",
        "$$\n",
        "\n",
        "`<s> and </s>` indicates start and end of sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 4** üíª\n",
        "\n",
        "Compute a bigram model over the tokens of the previous corpus.\n",
        "\n",
        "Use\n",
        "```python\n",
        "from nltk import bigrams\n",
        "```\n",
        "\n",
        "where `bigram_probabilities` is dictionary of dictionary that contains the probability."
      ],
      "metadata": {
        "id": "TnZrEN_cK-po"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR6abCRvosIB"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "bigram_probabilities = {} # dictionary to store the probabilities in the\n",
        "# format word_i:word_{i+1} -> probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BANZTJ-OosIB"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "from nltk import bigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Calculating bigram frequencies\n",
        "bigram_freq = Counter(bigrams(tokens))\n",
        "total_bigrams = sum(bigram_freq.values())\n",
        "\n",
        "# Building bigram probabilities\n",
        "bigram_probabilities = defaultdict(dict)\n",
        "for (w1, w2), freq in bigram_freq.items():\n",
        "    bigram_probabilities[w1][w2] = freq / frequency[w1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let us plot a few of them..."
      ],
      "metadata": {
        "id": "xXbvG23gLwQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jAS3QaOosIC",
        "outputId": "b114e816-82d9-4b03-94f3-601249215cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fulton 0.00013777900248002206\n",
            "The jury 0.0012400110223201985\n",
            "The September-October 0.00013777900248002206\n",
            "The grand 0.0002755580049600441\n",
            "The City 0.0006888950124001103\n",
            "The jurors 0.00013777900248002206\n",
            "The couple 0.0002755580049600441\n",
            "The petition 0.0002755580049600441\n",
            "The Hartsfield 0.00013777900248002206\n",
            "The mayor's 0.00013777900248002206\n",
            "Fulton County 0.35294117647058826\n",
            "Fulton Superior 0.11764705882352941\n",
            "Fulton legislators 0.11764705882352941\n",
            "Fulton taxpayers 0.058823529411764705\n",
            "Fulton ordinary's 0.058823529411764705\n",
            "Fulton Tax 0.058823529411764705\n",
            "Fulton Health 0.058823529411764705\n",
            "Fulton to 0.058823529411764705\n",
            "Fulton was 0.058823529411764705\n",
            "Fulton , 0.058823529411764705\n"
          ]
        }
      ],
      "source": [
        "for k1 in list(bigram_probabilities.keys())[:2]:\n",
        "  for k2 in list(bigram_probabilities[k1].keys())[:10]:\n",
        "    print(k1, k2, bigram_probabilities[k1][k2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 5** üíª\n",
        "\n",
        "Given the bigram model that we developed so far, write a function to sample from the bigram model possibile sentences of length `10` and with a starting word.\n",
        "\n",
        "Then run this function 10 times and check the result."
      ],
      "metadata": {
        "id": "5TrQ8TIFL93j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWxDvr7ZosIC"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "def generate_bigram_sentence(start_word, length, bigram_probabilities):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Ph1_z6osIC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "def generate_bigram_sentence(start_word, length, bigram_probabilities):\n",
        "    if start_word not in bigram_probabilities:\n",
        "        raise ValueError(\"Start word not in bigram probabilities\")\n",
        "    sentence = [start_word]\n",
        "    current_word = start_word\n",
        "    for _ in range(length - 1):\n",
        "        next_word = random.choices(list(bigram_probabilities[current_word].keys()),\n",
        "                                   weights=bigram_probabilities[current_word].values())[0]\n",
        "        sentence.append(next_word)\n",
        "        current_word = next_word\n",
        "    return ' '.join(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PBlnn_tosID",
        "outputId": "cba743b0-3bc0-4301-8ace-d8d1038b4bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colquitt Policeman Tom Horn had much for a man and certain tissues she couldn't lift the reader will give the\n",
            "Colquitt -- or lover . Complementing the nature of it across enough to be confused over these differences which begins\n",
            "Colquitt -- although Brown University , following Monday morning the 2 . But at least an ideal . The other\n",
            "Colquitt Policeman Tom Williams wrote the right of a demurrer to an equal ) 3 ) . One thing as\n",
            "Colquitt Policeman Tom attended Midwood High blood '' , into three factors as it has been confined almost a class\n",
            "Colquitt Policeman Tom '' . The president made a young conductor also failed to have it seldom a team ,\n",
            "Colquitt Policeman Tom '' . `` I was meant to a public ventures he ( 2 . America . The\n",
            "Colquitt Policeman Tom Swift . ) and in what I had a Russian tests . `` Thou art relates human\n",
            "Colquitt -- long way to share one of one of the children , far as nice of many universities ,\n",
            "Colquitt Policeman Tom Horn , technological factors that it , however , rhythm method of pit-run gravel which people .\n"
          ]
        }
      ],
      "source": [
        "# Generate a 10-word sentence starting with a given word\n",
        "start_word = random.choice(list(bigram_probabilities.keys()))\n",
        "for i in range(10):\n",
        "  print(generate_bigram_sentence(start_word, 20, bigram_probabilities))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzmZEOffosID"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "- Compare the sentences generated by the unigram and bigram models. Which model produces more coherent sentences?\n",
        "- What are the limitations of a bigram model in text generation?\n",
        "- How do the results change if starting words are varied?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H2RcGPGosID"
      },
      "source": [
        "## 3. Word2Vec\n",
        "\n",
        "We have already seen Word2Vec. Unlike the previously discussed unigram and bigram models, which focus on word frequencies or word sequences, Word2Vec captures the semantic relationships between words in a way that preserves context and similarity. Can we make use of these relationships in order to generate sentences?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBErLVD9mmQQ",
        "outputId": "5fb0226b-767d-4ed7-d498-7570f99fcd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us change and now use `glove-wiki-gigaword-100`"
      ],
      "metadata": {
        "id": "5XCoH_eb_2uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oMPjfSkmu6_",
        "outputId": "d1f26dd3-2e51-469d-d2f1-7873d4a71a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just a small hack to call it `model` instead of `wv`"
      ],
      "metadata": {
        "id": "tdJaRnLz_7OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel:\n",
        "  def __init__(self, model):\n",
        "    self.wv = model\n",
        "model = MyModel(glove_vectors)"
      ],
      "metadata": {
        "id": "OYANoYrymybh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVvLJhURosID"
      },
      "outputs": [],
      "source": [
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# WINDOW_SIZE = 3\n",
        "\n",
        "#tokenized_sentences = brown.sents()\n",
        "#model = Word2Vec(tokenized_sentences, vector_size=100, window=WINDOW_SIZE, min_count=1, sg=1) # sg=1 -> skip-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_1pCE-GosID"
      },
      "source": [
        "Word2Vec itself is not a language model in the traditional sense, as it doesn't model the probability distribution of a sequence of words in a language. As we already discussed, it is primarily used to learn vector representations of words that capture semantic similarities and relationships between them. Still, we can use this model to predict context words, which may appear in the same window as target words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 6** üíª\n",
        "\n",
        "Generate a sentence from a word2vec model treating it as an autoregressive model.\n",
        "\n",
        "The pseudo-algorithm can be:\n",
        "\n",
        "1. start from a inital word called `start_word`\n",
        "2. As you know word2vec was trained considering a buffer of words, thus we can consider a `WINDOW_SIZE` of the most recent generated words and use the most recent generated words of size `WINDOW_SIZE` to \"sample\" a new word. To do this use the function `most_similar(current_words, topn=10)` where current_words is of size `WINDOW_SIZE`.\n",
        "3. Now that you have \"sampled\" a new word we stored it in:\n",
        "  - The sentence that will be generated\n",
        "  - the list of current words of length `WINDOW_SIZE`. Note that current_words at most has to be of size  `WINDOW_SIZE`"
      ],
      "metadata": {
        "id": "_3ifYIJ-NrrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhcuhii2osIE"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "def generate_sentence(start_word, WINDOW_SIZE, max_length=100):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHykc2-wosIE"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "def generate_sentence(start_word, WINDOW_SIZE, max_length=100):\n",
        "    if start_word not in model.wv:\n",
        "        return \"Word not in vocabulary!\"\n",
        "\n",
        "    current_words = [start_word]\n",
        "    sentence = [start_word]\n",
        "    for _ in range(max_length - 1):\n",
        "        # Find most similar words\n",
        "        similar_words = model.wv.most_similar(current_words, topn=10)\n",
        "        #print(similar_words)\n",
        "        next_word = similar_words[0][0]  # pick the most similar word\n",
        "        sentence.append(next_word)\n",
        "\n",
        "        # Update current words\n",
        "        if len(current_words) == WINDOW_SIZE:\n",
        "            current_words.pop(0)\n",
        "        current_words.append(next_word)\n",
        "\n",
        "    return ' '.join(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2gcHGm_osIE",
        "outputId": "ebfe97f7-871f-464f-e501-814203ed8f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from 'the':\n",
            " the this same one only it but so though even because not this that if what it but so though even because not this that if what it but so though even because not this that if what it but so though even because not this that if what it but so though even because not this that if what it but so though even because not this that if what it but so though even because not this that if what it but so though even because not this that if what it but so though even because not\n"
          ]
        }
      ],
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random.seed(42)\n",
        "random_start_word = \"the\"  # or use random.choice(list(model.wv.index_to_key))\n",
        "generated_sentence = generate_sentence(random_start_word, 10, 100)\n",
        "print(f\"Generated sentence from '{random_start_word}':\\n {generated_sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8s9iOruoHO1"
      },
      "source": [
        "### 3.1 Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb0JtVUaoHO1"
      },
      "source": [
        "#### Uniform Top-K\n",
        "\n",
        "As you can see in the solution of the previous exercise, one way of picking the next word is taking the most similar.\n",
        "\n",
        "**Such a method is called Argmax Sampling.**\n",
        "\n",
        "### **EXERCISE 7** üíª\n",
        "\n",
        " For the next exercise,  **code and see what happens when we sample uniformly from the 10 most similar words.**\n",
        "\n",
        "\n",
        "`generate_sentence_with_sampling` is already done you need to complete `sample_unif_topk`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uasw163voHO1"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_with_sampling(start_word, sample_func, WINDOW_SIZE,\n",
        "                                    max_length=100, topk=10):\n",
        "    if start_word not in model.wv:\n",
        "        return \"Word not in vocabulary!\"\n",
        "\n",
        "    current_words = [start_word]\n",
        "    sentence = [start_word]\n",
        "    for _ in range(max_length - 1):\n",
        "        # Find most similar words\n",
        "        similar_words = model.wv.most_similar(current_words, topn=topk)\n",
        "        next_word = sample_func(similar_words)\n",
        "        sentence.append(next_word)\n",
        "\n",
        "        # Update current words\n",
        "        if len(current_words) == WINDOW_SIZE:\n",
        "            current_words.pop(0)\n",
        "        current_words.append(next_word)\n",
        "\n",
        "    return ' '.join(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Xu81ApoHO1"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "def sample_unif_topk(start_word, max_length=100):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4RFF-uDgoHO2"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "def sample_unif_topk(similar_words):\n",
        "    return random.choices([word for word, _ in similar_words], k=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKgPLAodoHO2",
        "outputId": "5ca96826-42c5-48ed-a484-26e5a470e2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from 'the':\n",
            " the on this it same part but so one though only because well this even yet there not they could but n't would that because should did might not say we what be even do would if that should did but might not they even could because n't if should be did might but it we that could yet would even what be n't might come did so not if they we that what but yet it way could even because n't if we did they but yet that not even because what be n't so they this would might\n"
          ]
        }
      ],
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random.seed(42)\n",
        "random_start_word = \"the\"  # or use random.choice(list(model.wv.index_to_key))\n",
        "generated_sentence = generate_sentence_with_sampling(random_start_word,\n",
        "                                                     sample_unif_topk,\n",
        "                                                     10, #WINDOW SIZE\n",
        "                                                     100,# How much to generate\n",
        "                                                     10) # topk\n",
        "print(f\"Generated sentence from '{random_start_word}':\\n {generated_sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We start now from \"The cat is\"\n",
        "Now let us change the function so that we start always with\n",
        "\n",
        "> ['the','cat', 'is']"
      ],
      "metadata": {
        "id": "PCX5OHLBFN7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence_with_sampling(current_words, sample_func, WINDOW_SIZE,\n",
        "                                    max_length=100, topk=10):\n",
        "\n",
        "  sentence = current_words[:]\n",
        "  for _ in range(max_length - 1):\n",
        "      # Find most similar words\n",
        "      similar_words = model.wv.most_similar(current_words, topn=topk)\n",
        "      next_word = sample_func(similar_words)\n",
        "      sentence.append(next_word)\n",
        "\n",
        "      # Update current words\n",
        "      if len(current_words) == WINDOW_SIZE:\n",
        "          current_words.pop(0)\n",
        "      current_words.append(next_word)\n",
        "\n",
        "  return ' '.join(sentence)"
      ],
      "metadata": {
        "id": "ATQZaH6qEZGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random.seed(42)\n",
        "random_start_word = \"the\"  # or use random.choice(list(model.wv.index_to_key))\n",
        "current_words = ['the','cat', 'is']\n",
        "generated_sentence = generate_sentence_with_sampling(current_words[:],\n",
        "                                                     sample_unif_topk,\n",
        "                                                     10, #WINDOW SIZE\n",
        "                                                     100,# How much to generate\n",
        "                                                     10,# topk\n",
        "                                                     )\n",
        "print(f\"Generated sentence from '{current_words}':\\n {generated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUg5qx9YEnyn",
        "outputId": "0f431d2a-9325-4c6d-f22f-a969c49ad333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from '['the', 'cat', 'is']':\n",
            " the cat is now one only as though . although but because it this well even however as same . that fact though there it this although what so but because much . however well only they not that yet this there because . but so it though although however that well only not this even they be . though there so that would yet this could now they even not it be that might still because n't would now they but even be so that did it because still n't not but they yet what if even could it did this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9A0PbX0oHO2"
      },
      "source": [
        "#### Top-K Sampling\n",
        "\n",
        "Sampling uniformly from the top-k similar words introduces significant variety into the text. However, expanding the selection to a larger pool, such as 100 or 1000 similar words, could introduce excessive noise if we continue to sample uniformly. To refine this approach, you can enhance the sampling method by weighting the probability of each word according to its similarity score. To do so, we may rely on python's `random.choice()` or implement inverse transform sampling directly.\n",
        "\n",
        "### **EXERCISE 7** üíª\n",
        "\n",
        "The next exercise involves coding this weighted sampling technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuZd4WehoHO3"
      },
      "outputs": [],
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "def sample_topk(similar_words):\n",
        "    # use random.choice()\n",
        "    pass\n",
        "\n",
        "\n",
        "## This is OPTIONAL since it can take longer time to implement\n",
        "def sample_topk_inverse_transform(similar_words):\n",
        "    # do not use random.choice() but reimplement inverse transform sampling by hand\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dmE7oI0UoHO3"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "\n",
        "# relying on python's random.choices to sample based on the weights\n",
        "def sample_topk(similar_words):\n",
        "    words = []\n",
        "    weights = []\n",
        "    for word, similarity in similar_words:\n",
        "        words.append(word)\n",
        "        weights.append(similarity)\n",
        "    return random.choices(words, weights=weights, k=1)[0]\n",
        "\n",
        "# implement inverse transform sampling\n",
        "def sample_topk_inverse_transform(similar_words):\n",
        "    words = []\n",
        "    weights = []\n",
        "    for word, similarity in similar_words:\n",
        "        words.append(word)\n",
        "        weights.append(similarity)\n",
        "\n",
        "    # Normalize the weights to form a probability distribution\n",
        "    total_weight = sum(weights)\n",
        "    probabilities = [weight / total_weight for weight in weights]\n",
        "\n",
        "    # Create the cumulative distribution function (CDF)\n",
        "    cumulative_probabilities = []\n",
        "    cumulative_sum = 0\n",
        "    for p in probabilities:\n",
        "        cumulative_sum += p\n",
        "        cumulative_probabilities.append(cumulative_sum)\n",
        "\n",
        "    # Generate a random float in the range [0, 1]\n",
        "    u = random.random()\n",
        "\n",
        "    # Find the first index where the cumulative probability exceeds the random number\n",
        "    for index, cumulative_probability in enumerate(cumulative_probabilities):\n",
        "        if cumulative_probability > u:\n",
        "            return words[index]\n",
        "\n",
        "    # As a fallback, return the first word if something goes wrong\n",
        "    return words[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random.seed(42)\n",
        "current_words = ['the','cat', 'is']\n",
        "generated_sentence = generate_sentence_with_sampling(current_words[:],\n",
        "                                                     sample_topk,\n",
        "                                                     10, #WINDOW SIZE\n",
        "                                                     100,# How much to generate\n",
        "                                                     10, # topk\n",
        "                                                     )\n",
        "print(f\"Generated sentence from '{current_words}':\\n {generated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2n8TJA0F0S8",
        "outputId": "8d5b3ae7-37ee-4eaa-9542-ce36644d2701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from '['the', 'cat', 'is']':\n",
            " the cat is now one only as though . although but because it this not even however well they that yet if but would only because though still it not even although could now however they . so this well that there because only but it not even . so if although however that this because there what fact but only even not one although yet however if they so but only because not did n't it we though be could so they that even yet only would because have might not could they did though n't even that if should say\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not so many difference as before but...\n",
        "now we can try to increase top-k because we inject as randomness in the sampling (noise) but we are sure that the noise is proportional to the similarity with the current words."
      ],
      "metadata": {
        "id": "m4alL4xjGwvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random.seed(42)\n",
        "current_words = ['the','cat', 'is']\n",
        "generated_sentence = generate_sentence_with_sampling(current_words[:],\n",
        "                                                     sample_topk,\n",
        "                                                     10, #WINDOW SIZE\n",
        "                                                     100,# How much to generate\n",
        "                                                     25, # topk\n",
        "                                                     )\n",
        "print(f\"Generated sentence from '{current_words}':\\n {generated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfZeKpiDG_vf",
        "outputId": "8ca76970-0714-4fb4-ad9c-3b1b565ebbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from '['the', 'cat', 'is']':\n",
            " the cat is so it even only what way time one same this well as so because although yet . they when but way could only not as would because that time however same no if should could n't does might now only do would we so even did could come way because going might make this that take n't does could they you want but way can it do n't make get if 'll what want did going really even so think nothing something certainly thought you why ? i really n't sure want everyone what think something everybody 've thought\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4qkGf01osIE"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "- Why might sentences generated using the most similar word method lack coherence over longer sequences? How could the model be adjusted to produce more grammatically coherent outputs? With this perspective in mind, how could we modify the model, in order to also predict the position of the generated context word?\n",
        "- What are some methods to prevent the generated sentences from looping or becoming repetitive? How might introducing randomness or diversity in word selection impact the quality of the generated sentences?\n",
        "- How do different parameters of the Word2Vec model (like vector size, window size, and training algorithm) affect the outcome of the generated sentences? What happens when you alter these parameters?\n",
        "- Have you noticed anything by generating sentences starting with an arbitrary word (i.e. \"gensim\")? Describe the issue and potential solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAB86BnLoHO5"
      },
      "source": [
        "### 3.2 Beam Search (Extra)\n",
        "\n",
        "### **EXERCISE 7** üíª\n",
        "\n",
        "\n",
        "In this exercise, you will implement the beam search algorithm using Word2Vec embeddings to generate text. Beam search, a heuristic search algorithm, balances exploration and efficiency by keeping only the top-scoring word sequences at each step. You'll integrate this with Word2Vec, which provides word embeddings that capture semantic similarities, to guide the generation process.\n",
        "\n",
        "Through this task, explore how different settings for the beam width affect the coherence and diversity of the generated sentences, enhancing your understanding of both beam search mechanics and the practical use of word embeddings in NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "\n",
        "def beam_search(start_word, beam_width=3, max_length=10):\n",
        "  pass"
      ],
      "metadata": {
        "id": "pjDQUlGOoRWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-p4aZXwkoHO5"
      },
      "outputs": [],
      "source": [
        "# @title üëÄ Solution\n",
        "def beam_search(start_word, beam_width=3, max_length=10):\n",
        "    if start_word not in model.wv:\n",
        "        return \"Word not in vocabulary!\"\n",
        "\n",
        "    # Initialize the beam with the start word\n",
        "    beam = [(start_word, 0)]  # List of tuples (sentence, cumulative_score)\n",
        "\n",
        "    for _ in range(max_length - 1):\n",
        "        candidates = []\n",
        "        # Explore each word in the beam\n",
        "        for sentence, cum_score in beam:\n",
        "            current_word = sentence.split()[-1]\n",
        "            # Find potential next words\n",
        "            try:\n",
        "                next_words = model.wv.most_similar(current_word, topn=beam_width)\n",
        "            except KeyError:\n",
        "                continue  # Skip if current_word has no similar words\n",
        "\n",
        "            # Add new sentences with updated scores\n",
        "            for next_word, sim in next_words:\n",
        "                new_sentence = sentence + ' ' + next_word\n",
        "                new_score = cum_score + sim  # Update cumulative score\n",
        "                candidates.append((new_sentence, new_score))\n",
        "\n",
        "        # Prune to keep only the top beam_width entries\n",
        "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    # Choose the best sequence\n",
        "    best_sequence, _ = sorted(beam, key=lambda x: x[1], reverse=True)[0]\n",
        "    return best_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random_start_word = 'the' #random.choice(list(model.wv.index_to_key))\n",
        "generated_sentence = beam_search(random_start_word, beam_width=3, max_length=10)\n",
        "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o9njzTYofdQ",
        "outputId": "38ed5a35-ab45-4610-b97d-3caa16543415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from 'the': the one another a another a another a another a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To enhance the beam search function and address potential issues such as repetition or lack of diversity in the generated sentences, one effective strategy is to introduce a penalty for sampled words. This approach helps to discourage the selection of the same word or similar words too frequently within a short segment of the generated text."
      ],
      "metadata": {
        "id": "Q15ZjpemqMED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üßëüèø‚Äçüíª Your code here\n",
        "def beam_search_with_penalty(start_word, beam_width=3, max_length=10, penalty=0.1):\n",
        "  pass"
      ],
      "metadata": {
        "id": "yJnjlpwPqicF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üëÄ Solution\n",
        "def beam_search_with_penalty(start_word, beam_width=3, max_length=10, penalty=0.1):\n",
        "    if start_word not in model.wv:\n",
        "        return \"Word not in vocabulary!\"\n",
        "\n",
        "    # Initialize the beam with the start word\n",
        "    beam = [(start_word, 0, [start_word])]  # Each entry is (sentence, cumulative_score, recent_words)\n",
        "\n",
        "    for _ in range(max_length - 1):\n",
        "        candidates = []\n",
        "        # Explore each word in the beam\n",
        "        for sentence, cum_score, recent_words in beam:\n",
        "            current_word = sentence.split()[-1]\n",
        "            # Find potential next words\n",
        "            try:\n",
        "                next_words = model.wv.most_similar(current_word, topn=beam_width + len(recent_words))\n",
        "            except KeyError:\n",
        "                continue  # Skip if current_word has no similar words\n",
        "\n",
        "            # Apply penalty to similar words based on recent usage\n",
        "            filtered_words = []\n",
        "            for next_word, sim in next_words:\n",
        "                if next_word not in recent_words:\n",
        "                    filtered_words.append((next_word, sim))\n",
        "                else:\n",
        "                    # Apply penalty\n",
        "                    filtered_words.append((next_word, sim * (1 - penalty)))\n",
        "\n",
        "            # Keep only the top beam_width entries after applying penalty\n",
        "            filtered_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "            # Add new sentences with updated scores and update recent words\n",
        "            for next_word, sim in filtered_words:\n",
        "                new_sentence = sentence + ' ' + next_word\n",
        "                new_score = cum_score + sim  # Update cumulative score\n",
        "                new_recent_words = recent_words[-(beam_width-1):] + [next_word]  # Update recent words\n",
        "                candidates.append((new_sentence, new_score, new_recent_words))\n",
        "\n",
        "        # Prune to keep only the top beam_width entries\n",
        "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    # Choose the best sequence\n",
        "    best_sequence, _, _ = sorted(beam, key=lambda x: x[1], reverse=True)[0]\n",
        "    return best_sequence"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2yIm19u0pTd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sentence from a random starting word\n",
        "random_start_word = 'the' #random.choice(list(model.wv.index_to_key))\n",
        "generated_sentence = beam_search_with_penalty(random_start_word, beam_width=100, max_length=100, penalty=1)\n",
        "print(f\"Generated sentence from '{random_start_word}': {generated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyOy0WWipWW8",
        "outputId": "8dd1b294-de6a-403b-8187-67890ba3f735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence from 'the': None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Designing an Autoregressive Word2Vec-Inspired Model for Position-Aware Language Generation (Extra)\n",
        "\n",
        "Word2Vec is a powerful tool for generating vector representations of words, capturing their semantic relationships based on their contexts. However, traditional Word2Vec models do not account for the order or position of words in context; they merely predict context words regardless of their specific positions relative to the target word. This limitation can be addressed by integrating positional awareness into a model inspired by Word2Vec but designed for language generation (i.e. Language Modeling).\n",
        "\n",
        "**Task:**\n",
        "\n",
        "Propose a detailed architecture for an autoregressive model that not only predicts context words but also their positions relative to the current word, similar to how Word2Vec captures contextual word relationships. Your proposed model should be capable of generating coherent language sequences by considering both the semantic similarity of words and their positional dynamics in sentences.\n",
        "\n",
        "Points to consider in your proposal:\n",
        "\n",
        "1. **Model Foundation**: Describe how you would modify the foundational architecture of Word2Vec (either CBOW or Skip-gram) to include positional information. What changes would be necessary to incorporate the sequential nature of language?\n",
        "\n",
        "2. **Position Encoding**: How would you encode positional information within the model? Consider techniques used in other models like transformers (e.g., positional encodings) and describe how they could be adapted or improved for this use case.\n",
        "\n",
        "3. **Autoregressive Mechanism**: Explain how the model would generate text in an autoregressive manner, particularly how it would use the positionally-aware embeddings to predict the next word in a sequence. What kind of neural network architecture would be suitable for this?\n",
        "\n",
        "4. **Training Strategy**: Discuss potential strategies for training this model. What kind of corpus and preprocessing would be needed? How would you define the loss function to include accuracy not just for predicting the right word, but also placing it correctly?\n",
        "\n",
        "5. **Potential Applications and Benefits**: Highlight how this position-aware model could outperform traditional Word2Vec models in specific NLP tasks. Consider applications in machine translation, summarization, or interactive dialog systems.\n",
        "\n",
        "Your response should outline the theoretical framework, practical considerations for implementation, and possible challenges you might face with this architecture. This exercise aims to deepen your understanding of how word embeddings can be extended beyond traditional models to enhance language generation capabilities in NLP systems."
      ],
      "metadata": {
        "id": "lj5PHhggujp6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}