{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1qlvegjW_MxAAS6wMAkkyAnqEj0PSXR0k","timestamp":1744715689476}],"toc_visible":true,"gpuType":"T4","collapsed_sections":["pFh4hrwsAh-F"],"mount_file_id":"1qlvegjW_MxAAS6wMAkkyAnqEj0PSXR0k","authorship_tag":"ABX9TyMhNfnh/nfEulnuaz+eX/t7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Deep Learning & Applied AI\n","\n","We recommend going through the notebook using Google Colaboratory.\n","\n","# Tutorial 7b: Batchnorm and dropout\n","\n","In this tutorial, we will show two implementations of Batchnorm and Dropout from scratch.\n","\n","You are encouraged to try and implement them by yourselves before looking at the solution. At the very least, once you have read and understood the code, try to re-implement it on your own.\n","\n","Author:\n","\n","Prof. Emanuele RodolÃ \n","\n","Course:\n","\n","- Website and notebooks will be available at https://github.com/erodola/DLAI-s2-2025/"]},{"cell_type":"markdown","source":["# Batchnorm from scratch\n","\n","This is the 2d version of batchnorm, operating on batches with shape `(b,c,h,w)`. The 1d version is way simpler as it simply digests batches with shape `(b,c,n)` or `(b,n)`.\n","\n","Key points to keep in mind:\n","\n","- *Training:*\n","  - Mean/stdev are accumulated across all pixels of all images in the batch, yielding **one scalar per channel**.\n","  - Use `unbiased=False` in the stdev.\n","  - Compute running stats across batches.\n","  - Introduce trainable $\\gamma$ and $\\beta$ (again, **one per channel**).\n","\n","- *Inference:*\n","  - Apply running stats computed during training.\n","  - Use batchnorm *before* the nonlinearity."],"metadata":{"id":"l94VgEvPP7HE"}},{"cell_type":"code","source":["# Just the raw calculations for double check\n","\n","import torch\n","import torch.nn as nn\n","\n","b, c, h, w = 2, 3, 2, 2\n","\n","x = torch.arange(b*c*h*w, dtype=torch.float32).reshape((b, c, h, w))\n","\n","BN = nn.BatchNorm2d(c)\n","\n","mu = x.mean(dim=(0, 2, 3), keepdim=True)\n","var = x.var(dim=(0, 2, 3), keepdim=True, unbiased=False)  # divide by N instead of N-1\n","\n","torch_bn = BN(x)\n","my_bn = (x - mu) / torch.sqrt(var + 1e-5)\n","\n","torch.allclose(torch_bn, my_bn, atol=1e-6)"],"metadata":{"id":"cXviK3SYP8Ns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom batch norm module\n","\n","import torch\n","import torch.nn as nn\n","\n","class MyBatchNorm2d(nn.Module):\n","\n","  def __init__(self, num_features: int, momentum: float, affine: bool, eps: float = 1e-5):\n","    \"\"\"\n","    Implements BatchNorm2d from scratch.\n","\n","    Args:\n","      num_features (int): The number of input features (channels) per data point.\n","      momentum (float): Momentum parameter; the larger, the more emphasis is given to later batches.\n","      affine (bool): Whether or not to include trainable scale and shift parameters.\n","      eps (float): Epsilon to prevent instabilities in the calculation of the batch variance.\n","    \"\"\"\n","    super().__init__()\n","\n","    self.momentum = momentum\n","    self.affine = affine\n","    self.eps = eps\n","\n","    # trainable gamma (scale) and beta (shift) to allow learning the identity map\n","    if self.affine:\n","      self.gamma = nn.Parameter(torch.ones(num_features))  # includes them in model.parameters(),\n","      self.beta = nn.Parameter(torch.zeros(num_features))  # thus making them trainable\n","\n","    # NOTE: the two commented lines below are *wrong*:\n","    # - requires_grad=False implies these will not be saved/loaded!\n","    # - will not be moved to the device if .to(device) is called\n","    # - unclear, because might be misinterpreted as non-trainable model parameters\n","    # self.running_mu = torch.zeros(num_features, requires_grad=False)\n","    # self.running_var = torch.ones(num_features, requires_grad=False)\n","\n","    self.register_buffer(\"running_mu\", torch.zeros(num_features))\n","    self.register_buffer(\"running_var\", torch.ones(num_features))\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","\n","    if self.training:\n","\n","      print(\"training\")\n","\n","      mu = x.mean(dim=(0, 2, 3))\n","      var = x.var(dim=(0, 2, 3), unbiased=False)\n","\n","      x = (x - mu[None, :, None, None]) / torch.sqrt(var[None, :, None, None] + self.eps)\n","\n","      self.running_mu = self.momentum * self.running_mu + (1 - self.momentum) * mu\n","      self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n","\n","    else:\n","\n","      print(\"evaluation\")\n","      x = (x - self.running_mu[None, :, None, None]) / torch.sqrt(self.running_var[None, :, None, None] + self.eps)\n","\n","    if self.affine:\n","      x = x * self.gamma[None, :, None, None] + self.beta[None, :, None, None]\n","\n","    return x\n","\n","  def __repr__(self) -> str:\n","    return f\"MyBatchNorm2d({self.momentum=}, {self.affine=})\"\n","\n","# Usage example\n","\n","b, c, h, w = 32, 5, 28, 28\n","xb = torch.randn((b, c, h, w))\n","\n","model = nn.Sequential(\n","    nn.Conv2d(in_channels=c, out_channels=7, kernel_size=3),\n","    MyBatchNorm2d(num_features=7, momentum=0.9, affine=True)\n",")\n","\n","# model.train()\n","model.eval()\n","model(xb).shape"],"metadata":{"id":"cX1EmMadh8t7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dropout from scratch\n","\n","Key points:\n","\n","- **The output is rescaled by $\\frac{1}{1-p}$** to compensate for the dropped elements. This way, the activations have similar scale at training and inference.\n","\n","- Zero-out the **features** instead of the layer neurons, e.g.\n","\n","```\n","x = self.fc1(x)\n","x = self.relu(x)\n","x = self.dropout(x)  # after the nonlinearity\n","x = self.fc2(x)\n","...\n","```\n","\n","- For convolutional layers, there are two variants:\n","  1. randomly drop pixel-wise features\n","  2. randomly drop entire feature maps (*spatial dropout*, in pytorch `dropout2d`)"],"metadata":{"id":"Ig0P5jDXP8w6"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","torch.manual_seed(1337)\n","\n","x = torch.randn((3, 4))\n","p = 0.8  # percent of elements to drop on average\n","\n","dropout = nn.Dropout(p=p)\n","dropout(x)"],"metadata":{"id":"TqsN9SviP90j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.bernoulli(x) draws 0 or 1 for each element in x,\n","# according to the probability specified at that element\n","\n","mask = torch.bernoulli(torch.ones_like(x) * (1 - p)).bool()\n","mask * x / (1 - p)  # apply mask and rescale"],"metadata":{"id":"4v8fXqAGyjNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class MyDropout(nn.Module):\n","\n","  def __init__(self, p: float):\n","    super().__init__()\n","    self.p = p\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","\n","    if self.training:\n","      mask = torch.bernoulli(torch.ones_like(x) * (1 - self.p)).bool()\n","      return x * mask / (1 - self.p)\n","    else:\n","      return x\n","\n","  def __repr__(self) -> str:\n","    pass  # TODO\n","\n","# Usage example\n","\n","x = torch.rand(10)\n","\n","model = nn.Sequential(\n","    nn.Linear(10, 20),\n","    nn.ReLU(),\n","    MyDropout(p=0.5)\n",")\n","\n","model.eval()\n","model(x)\n"],"metadata":{"id":"VEy2iIPN4sx3"},"execution_count":null,"outputs":[]}]}