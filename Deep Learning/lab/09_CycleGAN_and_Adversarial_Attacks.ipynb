{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/erodola/DLAI-s2-2023/blob/main/labs/09/CycleGAN_and_Adversarial_Attacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Deep Learning & Applied AI\n","\n","We recommend going through the notebook using Google Colaboratory.\n","\n","# Tutorial 9: CycleGAN and Adversarial Attacks\n","\n","\n","In this tutorial, we will cover:\n","\n","- GAN, cGAN and CycleGAN\n","- Adversarial Attacks\n","\n","Based on original material by Dr. Luca Moschella, Dr. Antonio Norelli and Dr. Marco Fumero.\n","\n","Course:\n","\n","- Website and notebooks will be available at https://github.com/erodola/DLAI-s2-2025/"]},{"cell_type":"code","source":["!pip install wandb\n","!pip install pytorch-lightning==1.8.0\n","!pip install pyyaml==5.4.1"],"metadata":{"id":"mgzNAg27Fy__"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"5CApnc6X4bFq"},"source":["# @title import dependencies\n","\n","from typing import Sequence, List, Dict, Tuple, Optional, Any, Set, Union, Callable, Mapping\n","import itertools\n","\n","import dataclasses\n","from dataclasses import dataclass\n","from dataclasses import asdict\n","from pathlib import Path\n","from pprint import pprint\n","from urllib.request import urlopen\n","import random\n","\n","from PIL import Image\n","import PIL\n","\n","import torchvision.utils\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import plotly.express as px\n","\n","import numpy as np\n","import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","import wandb\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers.wandb import WandbLogger\n","from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n","\n","import torchvision\n","from torchvision import transforms\n","from tqdm.notebook import tqdm\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"2tGN_bJOcfd3"},"source":["# @title reproducibility stuff\n","\n","import random\n","np.random.seed(0)\n","random.seed(0)\n","\n","torch.cuda.manual_seed(0)\n","torch.manual_seed(0)\n","torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n","torch.backends.cudnn.benchmark = False\n","\n","# This prevoius seeds should be redundant.\n","_ = pl.seed_everything(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> 🚜🚜🚜 Since the download we will perform later on is quite slow, it may be convenient to start the download by executing the \"Download datasets\" cell as soon as possible"],"metadata":{"id":"1i6ebUJBJjm4"}},{"cell_type":"markdown","metadata":{"id":"5mDMlAu8Iq-f"},"source":["# Generative Adversarial Networks"]},{"cell_type":"markdown","metadata":{"id":"POxCVU0gFFVS"},"source":["The Generative Adversarial Networks (GANs) are based on a game theoretic scenario.\n","\n","- The **discriminator** tries its best to discriminate *real* samples drawn from the training data and *fake* samples drawn from the generator.\n","\n","- The **generator** does its best to trick the discriminator and produce a fake sample, that is wrongly recognized as a real one.\n","\n","The generator network directly produces samples $\\mathbf{x}= g(\\mathbf{z}; \\mathbf{\\theta}^{(g)})$. The discriminator emits a probability value given by $d(\\mathbf{x};\\mathbf{\\theta}^{(d)})$, indicating the probability that $\\mathbf{x}$ is a real training example rather than a fake sample drawn from the generator network.\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/gan.png)\n","\n","\n","## Learning formulation\n","\n","The simplest way to formulate learning in generative adversarial networks is a zero-sum game.\n","\n","We choose a function $v(\\mathbf{\\theta}^{(g)}, \\mathbf{\\theta}^{(d)})$ as the reward for the the discriminator and $-v(\\mathbf{\\theta}^{(g)}, \\mathbf{\\theta}^{(d)})$ as the generator reward. They both want to maximise their reward!\n","\n","This can be phrased mathematically:\n","\n","$$\n","g^* = \\min_g \\max_d v(g, d)\n","$$\n","\n","The default choice for $v$ is:\n","\n","$$\n","v(\\mathbf{\\theta}^{(g)}, \\mathbf{\\theta}^{(d)})\n","=\n","\\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\log d(\\mathbf{x})\n","+\n","\\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{model}}} \\log (1 - d(\\mathbf{x}))\n","$$\n","\n","This drives the discriminator to attempt learning to correctly classify samples as real or fake.\n","Simultaneously, the generator attempts to fool the classifier into believing its samples are real! Since the discriminator tries to maximise and the generator to minimise.\n","\n","At convergence, the generator's samples are ingistinguishable from real data and the discriminator outputs $d(\\mathbf{x}) = \\frac{1}{2}$ everywhere.\n","This means that the generator is able to produce data that lies in the same distribution of the training data.\n","\n","\n","In the slides you can find the closed form derivation for the previous formulation.\n","\n","\n","## The Generator\n","\n","Let's look with more attention at what the generator is doing.\n","\n","It takes in input *noise* and produces an image from the desired distribution.\n","The noise is usually drawn from a gaussian distribution:\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/generator.png)\n","\n","\n","Intuitively, it is learning how to translate samples drawn from a normal distribution to samples drawn from the desired distribution (e.g. the distribution of faces).\n","It translates one distribution to another.\n","\n","Why should we limit the generator to translate only the plain normal distribution to other distributions? Can't we condition the starting distribution somehow? Can't we translate arbitrary distributions from one into another one?\n","Welcome to the Conditional Adversarial Networks (cGAN).\n","\n","\n","\n","---\n","\n","References:\n","\n","- Deep Learning, Goodfellow et al, 2016. Section 20.10.4\n","\n","Image credits:\n","\n","- [Thalles Silva](https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/)\n","\n","- [Pankaj Kishore](https://towardsdatascience.com/art-of-generative-adversarial-networks-gan-62e96a21bc35)"]},{"cell_type":"markdown","metadata":{"id":"v93vF1rbo_1F"},"source":["## Conditional Adversarial Network\n","\n","The normal GANs learn a mapping from a random noise vector $z$ to output image $y$, $G: z \\to y$. The conditional GANs proposed in (Isola et al.) learns a mapping from an observed image $x$ *and* a random noise vector $z$ to $y$, $G: \\{x,  z\\} \\to y$.\n","\n","> Interestingly, in (Isola et al.) the noise vector $z$ is provided only in the form of dropout (!), applied on several layers of the generator both at training and test time. Despite the dropout at test time, they observe only minor stochasticity in the output of the net.\n","\n","For example, this is the high level architecture of a cGAN that maps edges $\\to$ photos:\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/cGAN.png)\n","\n","The discriminator $D$ learns to classify between fake and real $\\{\\text{edge},\\text{photo}\\}$ tuples. The generator $G$ learns to fool the discriminator. Unlike an uncoditional GAN, **both the generator and discriminator observe the input edge map**!\n","\n","\n","## Learning formulation\n","\n","The objective of a conditional GAN can be expressed as:\n","\n","$$\n","\\mathcal{L}_{cGAN}(G, D)\n","=\n","\\mathbb{E}_{x, y} \\log D(x, y)\n","+\n","\\mathbb{E}_{x, z} \\log (1 - D(x, G(x, z)))\n","$$\n","\n","where $G$ tries to minimize this objective against an adversarial $D$ that tries to maximise it:\n","\n","$$\n","G^* = \\arg \\min_G \\max_D \\mathcal{L}_{cGAN}(G, D)\n","$$\n","\n","\n","This technique is very general and is able to tackle many different tasks!\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/pix2pix.png)\n","\n","Take a look at the paper to see more detailed performance.\n","\n","## The need of paired data\n","\n","Let's think a bit about the data required to implement this architecture.\n","Both the generator and discriminator observe the conditioning image. The discriminator observes tuples of conditioning image and real image, otherwise it has no way to understand the correlation between the two images and make a sensible decision.\n","\n","What does it mean? **To implement this architecture we need paired data**!\n","i.e. we need tuples of $(\\text{conditioning image}, \\text{real image})$.\n","\n","It may be possible to get this data for some tasks. For example, if we want to translate a photo taken during the day to a overnight photo the needed data is possible to acquire. Maybe it is expensive, but it is enough to take the same photo at different hours of the day.\n","\n","But what if we want to translate our photo, that lies on the distribution of realistic images, to the distribution where the Van Gogh paintings lie? i.e. we want to transform our photo to a Van Gogh painting.\n","How can we acquire enough paired data? Is it even possible?\n","\n","It turns out that it is possible to solve this styling task, the previous tasks you just saw and many others... **without paired data**. In the next sections we are going to go deep and implement this technique that exploits unpaired data: **CycleGAN**.\n","\n","---\n","\n","References:\n","\n","- Isola et al. [“Image-to-Image Translation with Conditional Adversarial Networks.”](http://arxiv.org/abs/1611.07004)"]},{"cell_type":"markdown","metadata":{"id":"ZBKlw9ElEETu"},"source":["## CycleGAN: Unpaired Image-to-Image Translation\n","\n","CycleGAN [(Zhu et al)](https://arxiv.org/abs/1703.10593) is able to **automatically translate** an image from one set of images into another and viceversa.\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/cycleGAN.png)\n","\n","Moreover, it does so **without the need of paired data**!\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/unpaired.png)\n","\n","As already said, paired training data consists of samples\n","$\\{x_i, y_i\\}^N$\n","where the corrispondence between $x_i$ and $y_i$ exists.\n","The *unpaired* data consist of a source set $\\{x_i\\}^N (x_i \\in X)$ and a target set $\\{y_j\\}^N (y_j \\in Y)$... there is no information as to which $x_i$ matches which $y_i$!\n","\n","\n","## Learning formulation\n","\n","The goal is to learn mapping functions between two domains $X$ and $Y$ given training examples $\\{x_i\\}^N$ where $x_i \\in X$ and $\\{y_j\\}^N$ where $y_j \\in Y$. Let's denote the data distribution as $x \\sim p_{data}(x)$  and $y \\sim p_{data}(y)$.\n","\n","\n","CycleGAN includes two mappings $G: X \\to Y$ and $F: Y \\to X$, moreover, both $G$ and $F$ have an adversarial discriminator $D_X$ and $D_Y$.\n","\n","The role of these adversarial discriminators is to distinguish generated *fake* images from *real* images: $D_X$ aims to distinguish between images $\\{x\\}$ and translated images $\\{F(y)\\}$; $D_Y$ aims to discriminate between $\\{y\\}$ and $\\{G(x)\\}$\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/cycle.png)\n","\n","The objective contains two different terms:\n","\n","- A standard *adversarial loss*, two in total, one for each GAN\n","- A *cycle consistency loss* to prevent incosistencies between the mappings $G$ and $F$.\n","\n","### Adversarial losses\n","The adversarial losses are very similar to the conditional GANs, but:\n","\n","- The data is unpaired, thus the discriminator doesn't have any image pairs to look at.\n","- There is no noise vector $z$, not even in the form of dropout.\n","\n","For the mapping $G: X \\to Y$ and associated discriminator $D_Y$ we have the adversarial objective:\n","\n","\n","$$\n","\\mathcal{L}_{GAN}(G, D_Y, X, Y)\n","=\n","\\mathbb{E}_{y \\sim p_{data}(y)} \\log D_Y(y)\n","+\n","\\mathbb{E}_{x \\sim p_{data}(x)} \\log (1 - D_Y(G(x))\n","$$\n","\n","where again the generator tries to minimize the objective against and adversary $D$ that tries to maximize it: $\\min_G \\max_{D_Y} \\mathcal{L}_{GAN}(G, D_Y, X, Y)$\n","\n","Equivalently for the mapping $F: Y \\to X$ and associated discriminator $D_X$ we have the adversarial objective:\n","\n","$$\n","\\mathcal{L}_{GAN}(F, D_X, Y, X)\n","=\n","\\mathbb{E}_{x \\sim p_{data}(x)} \\log D_X(x)\n","+\n","\\mathbb{E}_{y \\sim p_{data}(y)} \\log (1 - D_X(F(y))\n","$$\n","\n","with: $\\min_F \\max_{D_X} \\mathcal{L}_{GAN}(F, D_X, Y, X)$\n","\n","\n","### Cycle consistency\n","\n","Adversarial losses alone cannot guarantee that the learned function maps a given $x_i$ to a desired output $y_i$. To force the semantically \"correct\" mapping, it is enforced the cycle consistency.\n","\n","Intuitively we want that $x \\approx F(G(x))$ and $y \\approx G(F(y))$.\n","\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/cycle2.png)\n","\n","In practice this can be enforced using this objective:\n","\n","$$\n","\\mathcal{L}_{cyc}(G, F) =\n","\\mathbb{E}_{x \\sim p_{data}(x)}|| F(G(x)) - x ||_1\n","+\n","\\mathbb{E}_{y \\sim p_{data}(y)} || G(F(y)) - y ||_1\n","$$\n","\n","\n","### Identity consistency\n","\n","In some applications (Zhu et al) found useful to enforce a form of *identity consistency*. Meaning that the mapping $G: X \\to Y$ if given $y\\in Y$ in input should produce the same $y$. Similarly for the mapping $F$.\n","\n","This can be enforce with the objective:\n","\n","$$\n","\\mathcal{L}_{\\text{identity}}(G, F) =\n","\\mathbb{E}_{y \\sim p_{data}(y)}|| G(y) - y ||_1\n","+\n","\\mathbb{E}_{x \\sim p_{data}(x)} || F(x) - x ||_1\n","$$\n","\n","\n","### Full Objective\n","\n","The full objective is given by:\n","\n","$$\n","\\mathcal{L}(G, F, D_X, D_Y) =\n","\\mathcal{L}_{GAN}(G, D_Y, X, Y)\n","+\n","\\mathcal{L}_{GAN}(F, D_X, Y, X)\n","+\n","\\lambda\\mathcal{L}_{cyc}(G, F)\n","+\n","\\beta\\mathcal{L}_{\\text{identity}}(G, F)\n","$$\n","\n","The goal is to solve:\n","\n","$$\n","G^*, F^* =\n","\\arg\n","\\min_{G, F}\n","\\max_{D_X, D_Y}\n","\\mathcal{L}(G, F, D_X, D_Y)\n","$$\n","\n","> **QUESTION:** What could happen if we minimize only the $\\mathcal{L}_{cyc}$ and $\\mathcal{L}_{\\text{identity}}$ terms, so removing the $\\mathcal{L}_{GAN}$ ones?\n","\n","### Implementation\n","\n","This is a brief overview of all the implementation and training tricks in the paper that we are going to implement.\n","\n","#### 1. Architecture\n","\n","Regarding the architecture, keep in mind that it is not required to undersand every detail of the generators and discriminators to get the main idea of CycleGAN!\n","\n","The technique proposed is general and can be applied to a variety of types of generators and discriminators.\n","\n","\n","#### 2. Loss stability\n","\n","(Zhu et al.) **replaces all the negative log likelihood objectives with least-squares losses** to improve stability.\n","\n","In particular for a GAN loss $\\mathcal{L}_{GAN}(G, D, X, Y)$ they train:\n","\n","- $G$ to minimize\n","  $$\\mathbb{E}_{x \\sim p_{data}(x)} (D(G(x)) - 1)^2$$\n","\n","- $D$ to minimize\n","  $$\n","  \\mathbb{E}_{y \\sim p_{data}(y)} (D(y) - 1)^2\n","  +\n","  \\mathbb{E}_{x \\sim p_{data}(x)} D(G(x))^2\n","  $$\n","\n","\n","#### 3. Model oscillations\n","\n","They don't update the discriminator using the images generated by the latest generator. Instead, they keep a history of generated images and randomly swap some images generated by the latest generator with the old ones.\n","This buffer has length 50.\n","\n","\n","#### 4. Weights initialization\n","\n","All the weights are initialized from a Gaussian distribution $\\mathcal{N}(0, 0.02)$\n","\n","\n","#### 5. Convergence speed\n","\n","They divide the objective by two while optimizing the discriminator, in order to slow down the rate at which $D$ learns relative to the rate of $G$.\n","\n","#### 6. Learning rate decay\n","\n","They use a learning rate scheduler. It keeps the same learning rate for the first 100 epochs, then, linearly decay the rate to zero over the next 100 epochs.\n","\n","\n","---\n","\n","References:\n","\n","- Zhu et al. [\"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.\"](https://arxiv.org/abs/1703.10593)\n","\n","\n","Implementation inspired by:\n","\n","- [PyTorch GAN](https://github.com/eriklindernoren/PyTorch-GAN/). Several components of the architecture comes from here.\n","\n","- Official [paper repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n"]},{"cell_type":"markdown","metadata":{"id":"lhzMe7KyH8_9"},"source":["## PyTorch Lightning\n","\n","\n","Since the code for this example will be fairly long, we are going to use [PyTorch Lightning](https://www.pytorchlightning.ai/) to keep the code clean and organized.\n","\n","Lightning is a way to organize your PyTorch code to **decouple the science code from the engineering**. It's more of a PyTorch style-guide than a framework.\n","\n","In Lightning, you organize your code into 3 distinct categories:\n","\n","- Research code (goes in the LightningModule).\n","- Engineering code (you delete, and is handled by the Trainer).\n","- Non-essential research code (logging, etc... this goes in Callbacks).\n","\n","Here's an example of how to refactor your research code into a [LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html):\n","\n","![](https://github.com/PyTorchLightning/pytorch-lightning/raw/master/docs/source/_static/images/general/pl_quick_start_full_compressed.gif)\n","\n","---\n","\n","References:\n","\n","- Description from the [Lightning docs](https://pytorch-lightning.readthedocs.io/en/latest/)"]},{"cell_type":"markdown","metadata":{"id":"cPCGPEtHFK4w"},"source":["## Weights and Biases\n","\n","We will use Weights and Biases to log metrics and sample images in this section.\n","\n","As you will see, it is nicely integrated in PyTorch Lightning and requires minimal effort to set it up. Remember that even if Weights and Biases is free for academic use, there are many other loggers you can use (even integrated in [Lightning](https://pytorch-lightning.readthedocs.io/en/latest/api_references.html#loggers))"]},{"cell_type":"code","metadata":{"id":"tNN2ZeOXFMma"},"source":["!wandb login"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6XbDKKBSOyo"},"source":["## Download datasets\n","\n","The official repository of (Zhu et al) provides scripts to download the datasets they used in their experiments.\n","\n","We are going to use those scripts to download the `maps` and `ukiyoe2photo` dataset. We will not import any python code from the repository.\n","\n","\n","All the dataset are composed of (at least) four folders:\n","\n","- `trainA`: train images from distribution A\n","- `trainB`: train images from distribution B\n","- `testA`: test images from distribution A\n","- `testB`: test images from distribution B"]},{"cell_type":"code","metadata":{"id":"65J113PHSPs1"},"source":["# Slow download... :)\n","!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n","!cd pytorch-CycleGAN-and-pix2pix; ./datasets/download_cyclegan_dataset.sh ukiyoe2photo\n","!cd pytorch-CycleGAN-and-pix2pix; ./datasets/download_cyclegan_dataset.sh maps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVoCXbWIaCOm"},"source":["> **How to read code**\n",">\n","> Some code cells will be quite long, don't be scared.\n","> There won't be line-by-line comments or extended explanations in markdown, since the code is self-documenting.\n",">\n","> The code is documented and typed.\n","> 1. Read the docstrings and look at the types to get a rough idea of *what* a function does.\n","> 2. Look at the variable names that are usually meaningful enough to understand *what* the code is doing at a finer level.\n","> 3. Only after that, read the whole code to understand the details of *how* it does that.\n",">\n","> *Curiosity*: probably we will have [statically checked tensor shapes](https://github.com/pytorch/pytorch/issues/26889) (with generic types) in PyTorch! That will be a huge improvement in code readability :]"]},{"cell_type":"markdown","metadata":{"id":"lguee8Og7s54"},"source":["## Dataset definition\n","\n","Let's define the `Dataset` to read the data we just downloaded.\n","\n","Keep in mind that this is an `unpaired` task, thus we do not have $(a_i, b_i)$ couples. The $i$-sample will be made of $x_i$ and a random $y$.\n","\n","To ease the visualization and see how the image generation evolves with time, we add a parameter `fixed_pairs` that if set to `True` returns always the same couple."]},{"cell_type":"code","metadata":{"id":"Ng6SINrcyVcN"},"source":["class DatasetUnpaired(Dataset):\n","\n","    def __init__(self,\n","                 folderA: Path,\n","                 folderB: Path,\n","                 transform: Optional[Callable] = None,\n","                 fixed_pairs: bool = False,\n","        ) -> None:\n","        \"\"\"\n","        Dataset to handle unpaired images, i.e. the number of images in folderA\n","        and in folderB may be different.\n","\n","        :param folderA: path to the folder that contains the A images\n","        :param folderB: path to the folder that contains the B images\n","        :param tranform: tranform to apply to the images\n","        \"\"\"\n","        super().__init__()\n","        self.folderA: Path = Path(folderA)\n","        self.folderB: Path = Path(folderB)\n","\n","        if not (folderA.is_dir() and folderB.is_dir()):\n","            raise RuntimeError(f\"The folders are not valid!\\n\\t- Folder A: {folderA}\\n\\t- Folder B: {folderB}\")\n","\n","        self.filesA: List[Path] = list(sorted(folderA.rglob('*.jpg')))\n","        self.filesB: List[Path] = list(sorted(folderB.rglob('*.jpg')))\n","\n","        if not self.filesA:\n","            raise RuntimeError(\"Empty image lists for folderA!\")\n","\n","        if not self.filesB:\n","            raise RuntimeError(\"Empty image lists for folderB!\")\n","\n","        self.filesA_num: int = len(self.filesA)\n","        self.filesB_num: int = len(self.filesB)\n","\n","        self.transform: Optional[Callable] = transform\n","        self.fixed_pairs: bool = fixed_pairs\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Since it is unpaired, it is not well defined.\n","        We will use the maximum number of images between folderA and folderB\n","\n","        :returns: maximum number between #imagesA and #imagesB\n","        \"\"\"\n","        return max(self.filesA_num, self.filesB_num)\n","\n","    def pil_loader(self, path: Path) -> PIL.Image:\n","        \"\"\" PIL loader implementation from the Pytorch's ImageDataset class\n","        https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n","\n","        :param path: the path to an image\n","        :returns: an image converted into RGB format\n","        \"\"\"\n","        # open path as file to avoid ResourceWarning\n","        # (https://github.com/python-pillow/Pillow/issues/835)\n","        with path.open('rb') as f:\n","            img = PIL.Image.open(f)\n","            return img.convert('RGB')\n","\n","    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Return a random sample imageA-imageB\n","\n","        :param index: index of the sample (not relevant)\n","        :returns: a dictionary containing:\n","                    - A: the imageA\n","                    - B: the imageB\n","                    - pathA: the path to the imageA\n","                    - pathB: the path to the imageB\n","        \"\"\"\n","\n","        # Enforce a valid index for `filesA`\n","        fileA = self.filesA[index % self.filesA_num]\n","\n","        if self.fixed_pairs:\n","            # When e.g. testing use reproducible samples\n","            fileB = self.filesB[index % self.filesB_num]\n","\n","        else:\n","            # When training, get a random image from filesB\n","            fileB = self.filesB[random.randint(0, self.filesB_num - 1)]\n","\n","        imageA = self.pil_loader(fileA)\n","\n","        imageB = self.pil_loader(fileB)\n","\n","        if self.transform is not None:\n","            imageA = self.transform(imageA)\n","            imageB = self.transform(imageB)\n","\n","        return {\n","            'A': imageA,\n","            'A_path': str(fileA),\n","            'B': imageB,\n","            'B_path': str(fileB),\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZwmk6D1nAJ5"},"source":["## Data selection\n","\n","The official CycleGAN repository provides many different datasets, to showcase different applications.\n","\n","In this tutorial you can choose to use the `ukiyo-e` or the `maps` dataset.\n","\n","*The choice you make now will persist for the rest of the notebook!*"]},{"cell_type":"code","metadata":{"cellView":"form","id":"sOob9TZxpKC6"},"source":["#@title visualization utility functions\n","\n","def plot_images(images,\n","                images_per_row: int,\n","                border: float = 3.,\n","                pad_value: float = 1,\n","                title = 'Some images'):\n","    \"\"\"\n","    Visualize many images in a nice grid\n","\n","    :param images: the images to visualize\n","    :param images_per_row: number of images per row\n","    :param border: the border size of the grid\n","    :param pad_value: border color\n","    :param title: the title of plot\n","    \"\"\"\n","    # Matplolib plot, much faster for static images\n","    # First visualise the original images\n","    plt.figure(figsize = (17, 17))\n","    plt.imshow(torchvision.utils.make_grid(images,images_per_row,border,pad_value=pad_value).permute(1, 2, 0))\n","    plt.title(title)\n","    plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWYuo4ntoFO7"},"source":["# @title select the dataset to use  { run: \"auto\" }\n","dataset_name = \"maps\"  # @param [\"maps\", \"ukiyoe2photo\"]\n","\n","trainA = Path(f\"pytorch-CycleGAN-and-pix2pix/datasets/{dataset_name}/trainA\")\n","trainB = Path(f\"pytorch-CycleGAN-and-pix2pix/datasets/{dataset_name}/trainB\")\n","testA = Path(f\"pytorch-CycleGAN-and-pix2pix/datasets/{dataset_name}/testA\")\n","testB = Path(f\"pytorch-CycleGAN-and-pix2pix/datasets/{dataset_name}/testB\")\n","\n","\n","visualize_batch_idx = 3  # @param {type:\"slider\", min:1, max:50, step:1}\n","\n","\n","import torchvision.utils\n","import matplotlib.pyplot as plt\n","\n","\n","loader = DataLoader(\n","    DatasetUnpaired(\n","        testA,\n","        testB,\n","        transform=transforms.Compose([transforms.ToTensor()]),\n","        fixed_pairs=True,\n","    ),\n","    batch_size=10,\n","    shuffle=False,\n",")\n","\n","load_iter = iter(loader)\n","\n","# ugly :]\n","for _ in range(visualize_batch_idx):\n","    batch = next(load_iter)\n","\n","\n","plot_images(batch[\"A\"], images_per_row=5, border=30, pad_value=1, title=\"A images\")\n","plot_images(batch[\"B\"], images_per_row=5, border=30, pad_value=1, title=\"B images\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qrPp4cORXnv"},"source":["## Hyperparameters\n","\n","Let's define the hyperparameters that we are going to use"]},{"cell_type":"code","metadata":{"id":"EojuBrXUUsyi"},"source":["# The dataclass are fancy classes to hold data\n","\n","# Working with dataclasses is particularly comfortable\n","# since you can specify types and get autocomplete/suggestion\n","# of the available hyperparameters\n","\n","@dataclass\n","class Config:\n","    #dataset_name: str = \"ukiyoe2photo\"  # name of the dataset\n","\n","    # Run we did:\n","    # map: 200 epochs, 100 decay\n","    # ukiyo: 40 epochs, 20 decay (due to time contraints)\n","    # They took ~7 hours each on a 2080ti\n","    n_epochs: int = 200  # number of epochs of training\n","    decay_epoch: int = 100  # epoch from which to start lr decay\n","\n","    img_height: int = 128  # size of image height # default 256x256\n","    img_width: int = 128  # size of image width\n","\n","    batch_size: int = 1  # size of the batches\n","    lr: float = 0.0002  # adam: learning rate\n","    b1: float = 0.5  # adam: decay of first order momentum of gradient\n","    b2: float = 0.999  # adam: decay of first order momentum of gradient\n","\n","    channels: int = 3  # number of image channels\n","    n_residual_blocks: int = 6  # number of residual blocks in generator # original 9\n","    lambda_cyc: float = 10.0  # cycle loss weight\n","    lambda_id: float = 5.0  # identity loss weight\n","\n","    n_cpu: int = 8  # number of cpu threads to use for the dataloaders\n","\n","    log_images: int = min(25, 100)  # number of images to log\n","\n","\n","cfg = Config()\n","pprint(asdict(cfg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oEMM07u14SG"},"source":["# Hyperparameters are just attributes of an object\n","cfg.batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mMEXXadcDsUh"},"source":["## Model sub-components"]},{"cell_type":"markdown","metadata":{"id":"CCNlfNnQObvk"},"source":["### Residual block\n","\n","This is a generic residual block. We already saw residual networks in the Convolutional Neural Networks lecture."]},{"cell_type":"code","metadata":{"id":"um_5gc5rOeWL"},"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_features: int) -> None:\n","        \"\"\"\n","        A generic residual block.\n","\n","        The input is transformed by a block,\n","        then, the transformation is summed up to the original input\n","\n","        :param in_features: number of input features\n","        \"\"\"\n","        super().__init__()\n","\n","        self.block = nn.Sequential(\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_features, in_features, 3),\n","            nn.InstanceNorm2d(in_features),\n","            nn.ReLU(inplace=True),\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_features, in_features, 3),\n","            nn.InstanceNorm2d(in_features),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param x: tensor with shape [batch, channels, w, h]\n","\n","        :returns: tensor with shape [batch, channels, w, h]\n","        \"\"\"\n","        return x + self.block(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVvYn7t7x0WG"},"source":["# Example of inner working\n","res = ResidualBlock(in_features=3)\n","\n","batch = torch.rand(10, 3, 128, 128)\n","batch.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MB0sMv2ox8cY"},"source":["res(batch).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0En31Z6hP9uH"},"source":["### Generator\n","\n","The image-conditioned generator we are going to use.\n","At high level it performs an image downsampling and then an upsampling."]},{"cell_type":"code","metadata":{"id":"PGhZVM_6QJtS"},"source":["class GeneratorResNet(nn.Module):\n","    def __init__(self, input_shape: Sequence[int], num_residual_blocks: int) -> None:\n","        \"\"\"\n","        Image-conditioned image generator.\n","\n","        It takes in input an image and produces another image.\n","\n","        :param input_shape: shape of expected input image\n","        :param num_residual_blocks: number of residual blocks to use\n","        \"\"\"\n","        super().__init__()\n","\n","        channels = input_shape[0]\n","\n","        # Initial convolution block\n","        out_features = 64\n","        model = [\n","            nn.ReflectionPad2d(channels),\n","            nn.Conv2d(channels, out_features, 7),\n","            nn.InstanceNorm2d(out_features),\n","            nn.ReLU(inplace=True),\n","        ]\n","        in_features = out_features\n","\n","        # Downsampling\n","        for _ in range(2):\n","            out_features *= 2\n","            model += [\n","                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n","                nn.InstanceNorm2d(out_features),\n","                nn.ReLU(inplace=True),\n","            ]\n","            in_features = out_features\n","\n","        # Residual blocks\n","        for _ in range(num_residual_blocks):\n","            model += [ResidualBlock(out_features)]\n","\n","        # Upsampling\n","        for _ in range(2):\n","            out_features //= 2\n","            model += [\n","                nn.Upsample(scale_factor=2),\n","                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n","                nn.InstanceNorm2d(out_features),\n","                nn.ReLU(inplace=True),\n","            ]\n","            in_features = out_features\n","\n","        # Output layer\n","        model += [\n","            nn.ReflectionPad2d(channels),\n","            nn.Conv2d(out_features, channels, 7),\n","            nn.Tanh(),\n","        ]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param x: tensor with shape [batch, channels, w, h]\n","\n","        :returns: tensor with shape [batch, channels, w, h]\n","        \"\"\"\n","        return self.model(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhM8bVYoxNfZ"},"source":["# Example of inner working\n","g = GeneratorResNet(input_shape=[3, 50, 50], num_residual_blocks=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HK-qOJvSxPG3"},"source":["batch = torch.rand(2, 3, 128, 128)\n","batch.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoZdu2U-xZnV"},"source":["g(batch).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JeUGFFipQUNo"},"source":["### Discriminator\n","\n","The discriminator we are going to use.\n","\n","It tries to predict $1$ for *real* images and $0$ for *fake* images.\n","In practice it does not try to predict a single $1$ or $0$ but a $3\\times 3$ matrix of ones or zeros."]},{"cell_type":"code","metadata":{"id":"dhdx65ZmQWOu"},"source":["class Discriminator(nn.Module):\n","    def __init__(self, input_shape: Sequence[int]) -> None:\n","        \"\"\"\n","        Discriminator that tries to infer if an image is:\n","        - fake, i.e. it has been generated by a generator\n","        - real, i.e. it has not been generated by a generator\n","\n","        :param input_shape: shape of the expected image\n","        \"\"\"\n","        super().__init__()\n","\n","        channels, height, width = input_shape\n","\n","        # Calculate output shape of image discriminator (PatchGAN)\n","        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n","\n","        def discriminator_block(\n","            in_filters: int, out_filters: int, normalize: bool = True\n","        ) -> Sequence[nn.Module]:\n","            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n","            if normalize:\n","                layers.append(nn.InstanceNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *discriminator_block(channels, 64, normalize=False),\n","            *discriminator_block(64, 128),\n","            *discriminator_block(128, 256),\n","            *discriminator_block(256, 512),\n","            nn.ZeroPad2d((1, 0, 1, 0)),\n","            nn.Conv2d(512, 1, 4, padding=1)\n","        )\n","\n","    def forward(self, img: torch.tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param img: tensor with shape [batch, channels, w, h]\n","\n","        :returns: tensor with shape [batch, 1, 3, 3]\n","        \"\"\"\n","        return self.model(img)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H82BhbZSvvvV"},"source":["# Example of inner working\n","d = Discriminator(input_shape=[3, 50, 50])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ij822LQev0PI"},"source":["batch = torch.rand(2, 3, 50, 50)\n","batch.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiASj7L_v4j-"},"source":["# Single output channel!\n","# The aim of the discriminator is to predict all ones if the image is real\n","# and all zeros if the the image is fake.\n","d(batch).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nz91MjGDQ7Zs"},"source":["### Image Buffer\n","\n","This is the training trick they use to increase the robustness and reduce the model oscillation.\n","\n","They don't update the discriminator using the images generated by the latest generator. Instead, they keep a history of generated images and randomly swap some images generated by the latest generator with the old ones.\n","This buffer has length 50.\n","\n"]},{"cell_type":"code","metadata":{"id":"qGL26noAQ9Dp"},"source":["class ReplayBuffer:\n","    def __init__(self, max_size: int = 50) -> None:\n","        \"\"\"\n","        Image buffer to increase the robustness of the generator.\n","\n","        Once it is full, i.e. it contains max_size images, each image in a given batch\n","        is swapped with probability p=0.5 with another one contained in the buffer.\n","\n","        \"\"\"\n","        assert (\n","            max_size > 0\n","        ), \"Empty buffer or trying to create a black hole. Be careful.\"\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Fill the buffer with each element in data.\n","        If the buffer is full, with p=0.5 swap each element in data with\n","        another in the buffer.\n","\n","        :param data: tensor with shape [batch, ...]\n","\n","        :returns: tensor with shape [batch, ...]\n","        \"\"\"\n","        to_return = []\n","\n","        for i in range(data.shape[0]):\n","            element = data[[i], ...]\n","\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","\n","            elif random.uniform(0, 1) > 0.5:\n","                i = random.randint(0, self.max_size - 1)\n","                self.data[i], element = element, self.data[i]\n","\n","            to_return.append(element)\n","\n","        return torch.cat(to_return)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fQ5aDu8ueFc"},"source":["# Example of inner working\n","b = ReplayBuffer(max_size=5)\n","batch_s = 0\n","batch_size = 5\n","batch_e = batch_s + batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnN0bP8lulTy"},"source":["# Execute multiple times this cell!\n","a = torch.arange(batch_s, batch_e)[..., None]\n","batch_s = batch_e\n","batch_e = batch_s + batch_size\n","batch = b.push_and_pop(a)\n","print(f\"Input batch:\\n{a}\\n\\nOutput batch:\\n{batch}\\n\\nHidden buffer state:\\n{b.data}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCcVxwE4QrGV"},"source":["### LR Scheduler\n","\n","\n","They use a learning rate scheduler. It keeps the same learning rate for the first 100 epochs, then, linearly decay the rate to zero over the next 100 epochs.\n","\n","This is a parametric implementation of this idea, where it is possible to specify the total number of epochs and when to start the linear decay."]},{"cell_type":"code","metadata":{"id":"d9RhSTVPQsW4"},"source":["class LambdaLR:\n","    def __init__(self, n_epochs: int, decay_start_epoch: int) -> None:\n","        \"\"\"\n","        Linearly decay the leraning rate to 0, starting from `decay_start_epoch`\n","        to the final epoch.\n","\n","        In practice\n","\n","        :param n_epochs: total number of epochs\n","        :param decay_start_epoch: epoch in which the learning rate starts to decay\n","        \"\"\"\n","        assert (\n","            n_epochs - decay_start_epoch\n","        ) > 0, \"Decay must start before the training session ends!\"\n","        self.n_epochs = n_epochs\n","        self.decay_start_epoch = decay_start_epoch\n","\n","    def step(self, epoch: int) -> float:\n","        \"\"\"\n","        One step of lr decay:\n","        - if `epoch < self.decay_start_epoch` it doesn't change the learning rate.\n","        - Otherwise, it linearly decay the lr to reach zero\n","\n","        :param epoch: current epoch\n","        :returns: learning rate multiplicative factor\n","        \"\"\"\n","        return 1.0 - max(0, epoch - self.decay_start_epoch) / (\n","            self.n_epochs - self.decay_start_epoch\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PwC67H0swbH"},"source":["# Example of the inner workings\n","n_epochs = 10\n","decay_from = 3\n","lr = LambdaLR(n_epochs, decay_from)\n","for i in range(n_epochs + 1):\n","    if i == decay_from:\n","        print(\"\\tStarting to decay\")\n","    print(lr.step(i))\n","    if i == n_epochs:\n","        print(\"\\tEnd of the decay\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i0xZ_pMVrTrQ"},"source":["## CycleGAN lightning module\n","\n","This is the main model. It encapsulates all the logic into a clear and well defined framework, as defined by Lightning.\n","\n","The main methods of every Lightning model are:\n","\n","- `train_dataloader` and `val_dataloader`: defines the dataloader for the train and test set\n","\n","- `configure_optimizers`: configure optimizers and schedulers. For each couple (optimizer, scheduler) there will be one call to `training_step` with the appropriate `optimizer_idx` to identify the optimizer.\n","\n","- `training_step`: defines what happens in a single training step\n","\n","- `validation_step`: defines what happens in a single validation step\n","\n","- `validation_epoch_end`: receive in input an aggregation of all the output of the `validation_step`. It is useful to compute metrics and log examples."]},{"cell_type":"code","metadata":{"id":"mCHvUISG5Rnj"},"source":["class CycleGAN(pl.LightningModule):\n","    def __init__(\n","        self,\n","        hparams: Union[Dict, Config],\n","        trainA_folder: Path,\n","        trainB_folder: Path,\n","        testA_folder: Path,\n","        testB_folder: Path,\n","    ) -> None:\n","        \"\"\"\n","        The CycleGAN model.\n","\n","        :param hparams: dictionary that contains all the hyperparameters\n","        :param trainA_folder: Path to the folder that contains the trainA images\n","        :param trainB_folder: Path to the folder that contains the trainB images\n","        :param testA_folder: Path to the folder that contains the testA images\n","        :param testB_folder: Path to the folder that contains the testB images\n","        \"\"\"\n","        super().__init__()\n","        self.save_hyperparameters(asdict(hparams) if not isinstance(hparams, Mapping) else hparams)\n","\n","        # Dataset paths\n","        self.trainA_folder = trainA_folder\n","        self.trainB_folder = trainB_folder\n","        self.testA_folder = testA_folder\n","        self.testB_folder = testB_folder\n","\n","        # Expected image shape\n","        self.input_shape = (self.hparams[\"channels\"], self.hparams[\"img_height\"], self.hparams[\"img_width\"])\n","\n","        # Generators A->B and B->A\n","        self.G_AB = GeneratorResNet(self.input_shape, self.hparams[\"n_residual_blocks\"])\n","        self.G_BA = GeneratorResNet(self.input_shape, self.hparams[\"n_residual_blocks\"])\n","\n","        # Discriminators\n","        self.D_A = Discriminator(self.input_shape)\n","        self.D_B = Discriminator(self.input_shape)\n","\n","        # Initialize weights\n","        # https://pytorch.org/docs/stable/nn.html?highlight=nn%20module%20apply#torch.nn.Module.apply\n","        self.G_AB.apply(self.weights_init_normal)\n","        self.G_BA.apply(self.weights_init_normal)\n","        self.D_A.apply(self.weights_init_normal)\n","        self.D_B.apply(self.weights_init_normal)\n","\n","        # Image Normalizations\n","        self.image_transforms = transforms.Compose(\n","            [\n","                transforms.Resize(int(self.hparams[\"img_height\"] * 1.12), Image.BICUBIC),\n","                transforms.RandomCrop((self.hparams[\"img_height\"], self.hparams[\"img_width\"])),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","            ]\n","        )\n","\n","        # Image Normalization for the validation: remove source of randomness\n","        self.val_image_transforms = transforms.Compose(\n","            [\n","                transforms.Resize(int(self.hparams[\"img_height\"] * 1.12), Image.BICUBIC),\n","                transforms.CenterCrop((self.hparams[\"img_height\"], self.hparams[\"img_width\"])),\n","                # transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","            ]\n","        )\n","\n","        # Image buffers\n","        self.fake_A_buffer = ReplayBuffer()\n","        self.fake_B_buffer = ReplayBuffer()\n","\n","        # Forward pass cache to avoid re-doing some computation\n","        self.fake_A = None\n","        self.fake_B = None\n","\n","        # Losses\n","        self.mse = torch.nn.MSELoss()\n","        self.l1 = torch.nn.L1Loss()\n","\n","        # Ignore this.\n","        # It avoids wandb logging when lighting does a sanity check on the validation\n","        self.is_sanity = True\n","\n","    def forward(self, x: torch.Tensor, a_to_b: bool) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass for this model.\n","\n","        This is not used while training!\n","\n","        :param x: input of the forward pass with shape [batch, channel, w, h]\n","        :param a_to_b: if True uses the mapping A->B, otherwise uses B->A\n","\n","        :returns: the translated image with shape [batch, channel, w, h]\n","        \"\"\"\n","        if a_to_b:\n","            return self.G_AB(x)\n","        else:\n","            return self.G_BA(x)\n","\n","    def weights_init_normal(self, m: nn.Module) -> None:\n","        \"\"\"\n","        Initialize the weights with a gaussian N(0, 0.02) as described in the paper.\n","\n","        :param m: the module that contains the weights to initialise\n","        \"\"\"\n","        classname = m.__class__.__name__\n","        if classname.find(\"Conv\") != -1:\n","            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","            if hasattr(m, \"bias\") and m.bias is not None:\n","                torch.nn.init.constant_(m.bias.data, 0.0)\n","        elif classname.find(\"BatchNorm2d\") != -1:\n","            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","            torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","    def train_dataloader(self) -> DataLoader:\n","        \"\"\" Create the train set DataLoader\n","\n","        :returns: the train set DataLoader\n","        \"\"\"\n","        train_loader = DataLoader(\n","            DatasetUnpaired(\n","                self.trainA_folder, self.trainB_folder, transform=self.image_transforms\n","            ),\n","            batch_size=self.hparams[\"batch_size\"],\n","            shuffle=True,\n","            num_workers=2,\n","            pin_memory=True,\n","        )\n","        return train_loader\n","\n","    def val_dataloader(self, custom_batch_size: Optional[int] = None) -> DataLoader:\n","        \"\"\" Create the validation set DataLoader.\n","\n","        It is deterministic.\n","        It does not shuffle and does not use random transformation on each image.\n","\n","        :returns: the validation set DataLoader\n","        \"\"\"\n","        test_loader = DataLoader(\n","            DatasetUnpaired(\n","                self.testA_folder,\n","                self.testB_folder,\n","                transform=self.val_image_transforms,\n","                fixed_pairs=True,\n","            ),\n","            batch_size=custom_batch_size if custom_batch_size is not None else 32,\n","            shuffle=False,\n","            num_workers=2,\n","            pin_memory=True,\n","        )\n","        return test_loader\n","\n","    def configure_optimizers(\n","        self,\n","    ) -> Tuple[Sequence[optim.Optimizer], Sequence[Dict[str, Any]]]:\n","        \"\"\" Instantiate the optimizers and schedulers.\n","\n","        We have three optimizers (and relative schedulers):\n","\n","        - Optimizer with index 0: optimizes the parameters of both generators\n","        - Optimizer with index 1: optimizes the parameters of D_A\n","        - Optimizer with index 2: optimizes the parameters of D_B\n","\n","        Each scheduler implements a linear decay to 0 after `cfg.hparams[\"decay_epoch\"]`\n","\n","        :returns: the optimizers and relative schedulers (look at the return type!)\n","        \"\"\"\n","        # Optimizers\n","        optimizer_G = torch.optim.Adam(\n","            itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()),\n","            lr=self.hparams[\"lr\"],\n","            betas=(self.hparams[\"b1\"], self.hparams[\"b2\"]),\n","        )\n","        optimizer_D_A = torch.optim.Adam(\n","            self.D_A.parameters(), lr=self.hparams[\"lr\"], betas=(self.hparams[\"b1\"], self.hparams[\"b2\"])\n","        )\n","        optimizer_D_B = torch.optim.Adam(\n","            self.D_B.parameters(), lr=self.hparams[\"lr\"], betas=(self.hparams[\"b1\"], self.hparams[\"b2\"])\n","        )\n","\n","        # Schedulers for each optimizers\n","        lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n","            optimizer_G,\n","            lr_lambda=LambdaLR(self.hparams[\"n_epochs\"], self.hparams[\"decay_epoch\"]).step,\n","        )\n","        lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n","            optimizer_D_A,\n","            lr_lambda=LambdaLR(self.hparams[\"n_epochs\"], self.hparams[\"decay_epoch\"]).step,\n","        )\n","        lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n","            optimizer_D_B,\n","            lr_lambda=LambdaLR(self.hparams[\"n_epochs\"], self.hparams[\"decay_epoch\"]).step,\n","        )\n","\n","        return (\n","            [optimizer_G, optimizer_D_A, optimizer_D_B],\n","            [\n","                {\"scheduler\": lr_scheduler_G, \"interval\": \"epoch\", \"frequency\": 1},\n","                {\"scheduler\": lr_scheduler_D_A, \"interval\": \"epoch\", \"frequency\": 1},\n","                {\"scheduler\": lr_scheduler_D_B, \"interval\": \"epoch\", \"frequency\": 1},\n","            ],\n","        )\n","\n","    def criterion_GAN(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        \"\"\" The loss criterion for GAN losses\n","\n","        :param x: tensor with any shape\n","        :param y: tensor with any shape\n","\n","        :returns: the mse between x and y\n","        \"\"\"\n","        return self.mse(x, y)\n","\n","    def criterion_cycle(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        \"\"\" The loss criterion for Cycle losses\n","\n","        :param x: tensor with any shape\n","        :param y: tensor with any shape\n","\n","        :returns: the l1 between x and y\n","        \"\"\"\n","        return self.l1(x, y)\n","\n","    def criterion_identity(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        \"\"\" The loss criterion for Identity losses\n","\n","        :param x: tensor with any shape\n","        :param y: tensor with any shape\n","\n","        :returns: the l1 between x and y\n","        \"\"\"\n","        return self.l1(x, y)\n","\n","    def identity_loss(self, image: torch.Tensor, generator: nn.Module) -> torch.Tensor:\n","        \"\"\" Implements the identity loss for the given generator\n","\n","        :param generator: a generator module that maps X -> Y\n","        :param image: an image in the Y distribution with shape [batch, channel, w, h]\n","\n","        :returns: the identity loss for these (generator, image)\n","        \"\"\"\n","        return self.criterion_identity(generator(image), image)\n","\n","    def gan_loss(\n","        self,\n","        generator: nn.Module,\n","        discriminator: nn.Module,\n","        image: torch.Tensor,\n","        expected_label: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\" Implements the GAN loss for the given generator and discriminator\n","\n","        :param image: the input image with shape [batch, channle, w, h]\n","        :param generator: the generator module to use to translate the image from X -> Y\n","        :param discriminator: the discriminator that tries to distinguish fake and real images\n","        :expected_label: tensor with shape compatible to the discriminator's output.\n","                         It is full of ones when training the generator. We feed a fake\n","                         image to the discriminator and we expect to get ones\n","                         (for the discriminator this is an error!)\n","\n","        :returns: the GAN loss for these (image, generator, discriminator)\n","        \"\"\"\n","        fake_image = generator(image)\n","        predicted_label = discriminator(fake_image)\n","        loss_GAN = self.criterion_GAN(predicted_label, expected_label)\n","        return loss_GAN, fake_image\n","\n","    def cycle_loss(\n","        self,\n","        fake_image: torch.Tensor,\n","        reverse_generator: nn.Module,\n","        original_image: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\" Implements the cycle consistency loss\n","\n","        It takes in input a fake image, to avoid repeated computation,\n","        thus it only needs the reverse mapping that produced that fake image.\n","\n","        :param fake_image: a image produced by a mapping X->Y with shape [batch, channel, w, h]\n","        :param reverse_generator: the generator module that maps Y->X\n","        :param original_image: the original image in X with shape [batch, channel, w, h]\n","                               to compare with the reconstructed fake image\n","\n","        :returns: the cycle consistency loss for this (fake_image, reverse_generator, original_image)\n","        \"\"\"\n","        recovered_image = reverse_generator(fake_image)\n","        return self.criterion_cycle(recovered_image, original_image)\n","\n","    def discriminator_loss(\n","        self,\n","        discriminator: nn.Module,\n","        proposed_image: torch.Tensor,\n","        expected_label: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\" Implements the loss used to train the discriminator\n","\n","        :param discriminator: the discriminator model to train\n","        :param proposed_image: the fake or real image proposed with shape [batch, channel, w, h]\n","        :param expected_label: tensor with shape compatible to the discriminator's output,\n","                               full of zeros if the proposed image is fake\n","                               full of ones if the proposed image is real\n","\n","        :returns: the discriminator loss for this (discriminator, proposed_image, expected_label)\n","        \"\"\"\n","        predicted_label = discriminator(proposed_image)\n","        return self.criterion_GAN(predicted_label, expected_label)\n","\n","    def training_step(\n","        self, batch: Dict[str, torch.Tensor], batch_nb: int, optimizer_idx: int\n","    ) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]:\n","        \"\"\" Implements a single training step\n","\n","        The parameter `optimizer_idx` identifies with optimizer \"called\" this training step,\n","        this we can change the behaviour of the training depending on which optimizers\n","        is currently performing the optimization\n","\n","        :param batch: current training batch\n","        :param batch_nb: the index of the current batch\n","        :param optimizer_idx: the index of the optimizer in use, see the function `configure_optimizers`\n","\n","        :returns: the total loss for the current training step, together with other information for the\n","                  logging and possibly the progress bar\n","        \"\"\"\n","        # Unpack the batch\n","        real_A = batch[\"A\"]\n","        real_B = batch[\"B\"]\n","\n","        # Adversarial ground truths\n","        valid = torch.ones(\n","            (real_A.size(0), *self.D_A.output_shape), device=real_A.device\n","        )\n","        fake = torch.zeros(\n","            (real_A.size(0), *self.D_A.output_shape), device=real_A.device\n","        )\n","\n","        # The first optimizer is for the two generators!\n","        if optimizer_idx == 0:\n","\n","            # Identity A and B loss\n","            loss_id_A = self.identity_loss(real_A, self.G_BA)\n","            loss_id_B = self.identity_loss(real_B, self.G_AB)\n","            loss_identity = self.hparams[\"lambda_id\"] * ((loss_id_A + loss_id_B) / 2)\n","\n","            # GAN A loss and GAN B loss\n","            loss_GAN_AB, self.fake_B = self.gan_loss(self.G_AB, self.D_B, real_A, valid)\n","            loss_GAN_BA, self.fake_A = self.gan_loss(self.G_BA, self.D_A, real_B, valid)\n","            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n","\n","            # Cycle loss: A -> B -> A  and  B -> A -> B\n","            loss_cycle_A = self.cycle_loss(self.fake_B, self.G_BA, real_A)\n","            loss_cycle_B = self.cycle_loss(self.fake_A, self.G_AB, real_B)\n","            loss_cycle = self.hparams[\"lambda_cyc\"] * ((loss_cycle_A + loss_cycle_B) / 2)\n","\n","            # Total loss\n","            loss_G = loss_GAN + loss_cycle + loss_identity\n","\n","            self.log_dict({\n","                    \"total_loss_generators\": loss_G,\n","                    \"loss_GAN\": loss_GAN,\n","                    \"loss_cycle\": loss_cycle,\n","                    \"loss_identity\": loss_identity,\n","                }\n","            )\n","            return loss_G\n","\n","        # The second optimizer is to train the D_A discriminator\n","        elif optimizer_idx == 1:\n","\n","            # Real loss\n","            loss_real = self.discriminator_loss(self.D_A, real_A, valid)\n","\n","            # Fake loss (on batch of previously generated samples)\n","            loss_fake = self.discriminator_loss(\n","                self.D_A, self.fake_A_buffer.push_and_pop(self.fake_A).detach(), fake\n","            )\n","\n","            # Total loss\n","            loss_D_A = (loss_real + loss_fake) / 2\n","            self.log_dict({\n","                    \"total_D_A\": loss_D_A,\n","                    \"loss_D_A_real\": loss_real,\n","                    \"loss_D_A_fake\": loss_fake,\n","                }\n","            )\n","            return loss_D_A\n","\n","\n","        # The second optimizer is to train the D_B discriminator\n","        elif optimizer_idx == 2:\n","\n","            # Real loss\n","            loss_real = self.discriminator_loss(self.D_B, real_B, valid)\n","\n","            # Fake loss (on batch of previously generated samples)\n","            loss_fake = self.discriminator_loss(\n","                self.D_B, self.fake_B_buffer.push_and_pop(self.fake_B).detach(), fake\n","            )\n","\n","            # Total loss\n","            loss_D_B = (loss_real + loss_fake) / 2\n","\n","            self.log_dict({\n","                    \"total_D_B\": loss_D_B,\n","                    \"loss_D_B_real\": loss_real,\n","                    \"loss_D_B_fake\": loss_fake,\n","                }\n","            )\n","            return loss_D_B\n","\n","        raise RuntimeError(\"There is an error in the optimizers configuration!\")\n","\n","    def get_image_examples(\n","        self, real: torch.Tensor, fake: torch.Tensor\n","    ) -> Sequence[wandb.Image]:\n","        \"\"\"\n","        Given real and \"fake\" translated images, produce a nice coupled images to log\n","\n","        :param real: the real images with shape [batch, channel, w, h]\n","        :param fake: the fake image with shape [batch, channel, w, h]\n","\n","        :returns: a sequence of wandb.Image to log and visualize the performance\n","        \"\"\"\n","        example_images = []\n","        for i in range(real.shape[0]):\n","            couple = torchvision.utils.make_grid(\n","                [real[i], fake[i]],\n","                nrow=2,\n","                normalize=True,\n","                scale_each=True,\n","                pad_value=1,\n","                padding=4,\n","            )\n","            example_images.append(\n","                wandb.Image(couple.permute(1, 2, 0).detach().cpu().numpy(), mode=\"RGB\")\n","            )\n","        return example_images\n","\n","    def validation_step(\n","        self, batch: Dict[str, torch.Tensor], batch_idx: int\n","    ) -> Dict[str, Union[torch.Tensor,Sequence[wandb.Image]]]:\n","        \"\"\" Implements a single validation step\n","\n","        In each validation step some translation examples are produced and a\n","        validation loss that uses the cycle consistency is computed\n","\n","        :param batch: the current validation batch\n","        :param batch_idx: the index of the current validation batch\n","\n","        :returns: the loss and example images\n","        \"\"\"\n","\n","        real_B = batch[\"B\"]\n","        fake_A = self.G_BA(real_B)\n","        images_BA = self.get_image_examples(real_B, fake_A)\n","\n","        real_A = batch[\"A\"]\n","        fake_B = self.G_AB(real_A)\n","        images_AB = self.get_image_examples(real_A, fake_B)\n","\n","        ####\n","\n","        real_A = batch[\"A\"]\n","        real_B = batch[\"B\"]\n","\n","        fake_B = self.G_AB(real_A)\n","        fake_A = self.G_BA(real_B)\n","\n","        # Cycle loss A -> B -> A\n","        recov_A = self.G_BA(fake_B)\n","        loss_cycle_A = self.criterion_cycle(recov_A, real_A)\n","\n","        # Cycle loss B -> A -> B\n","        recov_B = self.G_AB(fake_A)\n","        loss_cycle_B = self.criterion_cycle(recov_B, real_B)\n","\n","        # Cycle loss aggregation\n","        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n","        loss_cycle = self.hparams[\"lambda_cyc\"] * loss_cycle\n","\n","        # Total loss\n","        loss_G = loss_cycle\n","\n","        return {\"val_loss\": loss_G, \"images_BA\": images_BA, \"images_AB\": images_AB}\n","\n","    def validation_epoch_end(\n","        self, outputs: List[Dict[str, torch.Tensor]]\n","    ) -> Dict[str, Union[torch.Tensor, Dict[str, Union[torch.Tensor,Sequence[wandb.Image]]]]]:\n","        \"\"\" Implements the behaviouir at the end of a validation epoch\n","\n","        Currently it gathers all the produced examples and log them to wandb,\n","        limiting the logged examples to `hparams[\"log_images\"]`.\n","\n","        Then computes the mean of the losses and returns it.\n","        Updates the progress bar label with this loss.\n","\n","        :param outputs: a sequence that aggregates all the outputs of the validation steps\n","\n","        :returns: the aggregated validation loss and information to update the progress bar\n","        \"\"\"\n","        images_AB = []\n","        images_BA = []\n","\n","        for x in outputs:\n","            images_AB.extend(x[\"images_AB\"])\n","            images_BA.extend(x[\"images_BA\"])\n","\n","        images_AB = images_AB[: self.hparams[\"log_images\"]]\n","        images_BA = images_BA[: self.hparams[\"log_images\"]]\n","\n","        if not self.is_sanity:  # ignore if it not a real validation epoch. The first one is not.\n","            print(f\"Logged {len(images_AB)} images for each category.\")\n","\n","            self.logger.experiment.log(\n","                {f\"images_AB\": images_AB, f\"images_BA\": images_BA,},\n","                step=self.global_step,\n","            )\n","        self.is_sanity = False\n","\n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        self.log_dict({\"val_loss\": avg_loss})\n","        return {\"val_loss\": avg_loss}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xU4hRnMn2v78"},"source":["## Training\n","\n","Since we followed the `Lightning Module` template, we can exploit the `Trainer` to implement the train-loop.\n","\n","It is extremely easy to do so. After the training, we log the trained model on wandb.\n","\n","\n","> ⏱⏱⏱ *long training time* ⏱⏱⏱\n",">\n","> Pay attention that, depending on the hyperparameters that you choose, the training may take a *lot* of time.\n",">\n","> We did `200` epochs for the `map` dataset, and only `40` for the `ukiyoe2photo`. We reduced the image size to `128x128` (from the original `256x256`) to shorten the training time.\n",">\n","> Each run took $\\approx7$ hours to complete on a 2080 ti GPU.\n","> Keep in mind that if you use the GPUs provided by Colab the training times are about $\\times 5$ slower.\n",">\n","> **In the next section, you can load pre-trained models and look at the statitics of the runs on the W&B dashboard.**"]},{"cell_type":"code","metadata":{"id":"Jphiq4irQ4Ef"},"source":["# ⏱⏱⏱ slow executing cell ⏱⏱⏱\n","# Suggested to use pre-trained models!\n","\n","# Instantiate the model\n","gan_model = CycleGAN(hparams=cfg,\n","                     trainA_folder=trainA,\n","                     trainB_folder=trainB,\n","                     testA_folder=testA,\n","                     testB_folder=testB)\n","\n","# Define the logger\n","# https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n","wandb_logger = WandbLogger(project=\"CycleGAN Tutorial 2021\", log_model=True)\n","\n","## Currently it does not log the model weights, there is a bug in wandb and/or lightning.\n","wandb_logger.experiment.watch(gan_model, log='all', log_freq=100)\n","\n","# Define the trainer\n","trainer = pl.Trainer(logger=wandb_logger,\n","                     max_epochs=cfg.n_epochs,\n","                     gpus=1,\n","                     limit_val_batches=.2,\n","                     val_check_interval=0.25)\n","\n","# Start the training\n","trainer.fit(gan_model)\n","\n","# Log the trained model\n","trainer.save_checkpoint('model.pth')\n","wandb.save('model.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4vFWAMLF0DQh"},"source":["## Pre-trained models\n","\n","We have pretrained two models for the two datasets in these tutorial.\n","\n","The project is public on Weights and Biases, thus you can inspect the hyperparameters, statistics and example images in each one of the two run that produced these two models. In the next sections we'll see in detail what this model is able to produce!\n","\n","\n","- [W&B run](https://app.wandb.ai/lucmos/CycleGAN%20Tutorial/runs/2vce8p04/overview?workspace=user-lucmos) for the model trained on the ukiyoe2photo dataset\n","\n","- [W&B run](https://app.wandb.ai/lucmos/CycleGAN%20Tutorial/runs/1p5mk2ia/overview?workspace=user-lucmos) for the model trained on the maps dataset\n","\n"]},{"cell_type":"code","metadata":{"id":"p9Oskkj0ob7S"},"source":["!wget https://api.wandb.ai/files/lucmos/CycleGAN%20Tutorial/1p5mk2ia/model.pth -O map-model.pth\n","!wget https://api.wandb.ai/files/lucmos/CycleGAN%20Tutorial/2vce8p04/model2.pth -O ukiyoe-model.pth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUafcoCZsWHr"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IG5lXX4xGeb"},"source":["You can choose which model to use in the following sections. There are three possibilities:\n","- The model you **just trained**\n","- The **pre-trained `ukiyoe2photo` model**\n","- The **pre-trained `map` model**"]},{"cell_type":"code","metadata":{"id":"wwGTd6PooWLK"},"source":["#@title Load model weights  { run: \"auto\" }\n","\n","load_model = \"Load model trained on Maps\" #@param [\"Use your own model just trained\", \"Load model trained on Maps\", \"Load model trained on Ukiyo-e\"]\n","\n","if load_model == \"Use your own model just trained\":\n","    loaded_gan_model = gan_model\n","    print('Continuing to use your own model.')\n","elif load_model == \"Load model trained on Maps\":\n","    loaded_gan_model = CycleGAN.load_from_checkpoint('map-model.pth',\n","                                                    # ugly workaround to load old lightning checkpoint :[\n","                                                    hparams=torch.load('map-model.pth')['hparams'],\n","                                                    trainA_folder=trainA,\n","                                                    trainB_folder=trainB,\n","                                                    testA_folder=testA,\n","                                                    testB_folder=testB)\n","    print('Maps model loaded.')\n","elif load_model == \"Load model trained on Ukiyo-e\":\n","    loaded_gan_model = CycleGAN.load_from_checkpoint('ukiyoe-model.pth',\n","                                                    # ugly workaround to load old lightning checkpoint :[\n","                                                    hparams=torch.load('ukiyoe-model.pth')['hparams'],\n","                                                    trainA_folder=trainA,\n","                                                    trainB_folder=trainB,\n","                                                    testA_folder=testA,\n","                                                    testB_folder=testB)\n","    print('Ukiyo-e model loaded.')\n","\n","loaded_gan_model = loaded_gan_model.cuda()\n","\n","print(f\"\\nRemember to select the right dataset at the beginning!\\nDataset currently selected: **{dataset_name}**\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dO6myjn4t9L9"},"source":["## Playground: validation exploration\n","\n","Let's see how this CycleGAN behave!\n","\n","In the next cell you can select:\n","\n","- The input image to feed the generator: $a \\in A$ or $b \\in B$\n","- The generator direction: $A \\to B$ or $B \\to A$"]},{"cell_type":"code","metadata":{"id":"R1q3rQmZvulk"},"source":["# Precompute batches\n","\n","loader = loaded_gan_model.val_dataloader(custom_batch_size=10)\n","batches = [x for x in loader]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"3pInLVuO1nRJ"},"source":["#@title Playgrond: explore validation set  { run: \"auto\" }\n","\n","visualize_batch_idx = 34  #@param {type:\"slider\", min:1, max:100, step:1}\n","\n","\n","batch_extractor = {\n","    'A': lambda x: x['A'],\n","    'B': lambda x: x['B'],\n","}\n","generator_input =  'A' #@param [\"A\", \"B\"]\n","batch = batches[visualize_batch_idx]\n","gen_input = batch_extractor[generator_input](batch)\n","\n","\n","generators = {\n","    'A to B': loaded_gan_model.G_AB,\n","    'B to A': loaded_gan_model.G_BA,\n","}\n","generator_direction =  'A to B' #@param [\"A to B\", \"B to A\"]\n","generator = generators[generator_direction]\n","\n","\n","plot_images(gen_input, images_per_row=5, border = 5, pad_value=1, title='Generator input batch')\n","plot_images(generator(gen_input.cuda()).cpu().detach(), images_per_row=5, border=5, pad_value=1, title='Generator output batch')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PF1ES-D1ox1W"},"source":["## **EXERCISE**\n",">\n","> - What happens if you select a generator $X \\to Y$ and use $y \\in Y$ as the input? Why?\n","> - Do the two mapping directions $A \\to B$ and $B \\to A$ have the same complexity? When using the `ukiyoe2photo` dataset which direction is harder? Why?\n","> - Which hyperparameter would you change to improve the fidelity of the translated image with respect to the original image?"]},{"cell_type":"markdown","metadata":{"id":"hBAuPmCk_oQy"},"source":["## Performance examples\n","\n","All the examples below consist of an image pair:\n","- On the left there is the input image of the generator.\n","- On the right the translated left image.\n","\n","\n","You can look up this recap and the one in the next section from the W&B dashboard:\n","\n","- [Ukiyo-e run](https://app.wandb.ai/lucmos/CycleGAN%20Tutorial/runs/2vce8p04?workspace=user-lucmos)\n","- [Map run](https://app.wandb.ai/lucmos/CycleGAN%20Tutorial/runs/1p5mk2ia?workspace=user-lucmos)\n","\n","### Photo $\\to$ Ukiyo-e\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/photo2ukiyoe.png)\n","\n","### Ukiyo-e $\\to$ Photo\n","\n","That tree looks good!\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/ukiyoe2photo.png)\n","\n","### Satellite view $\\to$ Map\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/sat2map.png)\n","\n","### Map $\\to$ Satellite view\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/map2sat.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i1W59bnKuE4h"},"source":["## Performance examples over time\n","\n","Here there are some grid visualization, from the W&B dashboard, that let us visualise how the translation performance got better in function of the number of training steps.\n","\n","If we pay attention to some images, we can see the model oscillations: a typical behaviour of GANs.\n","\n","### Photo $\\to$ Ukiyo-e\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/photo2ukiyoe_time.png)\n","\n","### Ukiyo-e $\\to$ Photo\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/ukiyoe2photo_time.png)\n","\n","### Satellite view $\\to$ Map\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/sat2map_time.png)\n","\n","### Map $\\to$ Satellite view\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/map2sat_time.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1UJb2we4h5g3"},"source":["## Playground: online image translation\n","\n","In the next cell you can translate any RGB image on the web, just insert the url of the image you want to translate in the `custom_image_url`."]},{"cell_type":"code","metadata":{"cellView":"form","id":"9pvnHQJxitA1"},"source":["#@title Styling online images  { run: \"auto\" }\n","\n","urls = {\n","        'lake': 'https://media-cdn.tripadvisor.com/media/photo-s/01/4b/8c/20/hopfen-am-see.jpg',\n","        'sea': 'https://media-cdn.tripadvisor.com/media/photo-s/01/53/4d/5d/see-photo-album-for-more.jpg',\n","        'department': 'https://web.uniroma1.it/i3s/sites/default/files/pictures/salariaok.jpg',\n","        'department ...artistic?': 'https://www.di.uniroma1.it/sites/default/files/pictures/salaria.jpg',\n","        'colosseo': 'https://i.pinimg.com/originals/88/db/b4/88dbb44ed72207fcd8a1e9a26cbfc58d.jpg',\n","        'colosseo2': 'https://d9k3q4j9.stackpathcdn.com/wp-content/uploads/2016/10/Colosseo-laptop_1040_529-815x500.jpeg',\n","        'map': 'https://agenziauscite.openstreetmap.it/img/colosseo_nokia_maps.png',\n","        'turing': 'https://www.focus.it/site_stored/imgs/0003/005/h_00528110.630x360.jpg',\n","        'piramide': 'https://i.guim.co.uk/img/media/e3d9827f235ac40064f15d7df25024aec60500cb/0_134_5616_3370/master/5616.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=56f9da8e992f2558c4709614daf82a69',\n","        'vertical forest': 'https://i.imgur.com/mYUXjEg.png'\n","}\n","\n","\n","\n","example_image = \"colosseo\" #@param [\"department\", \"department ...artistic?\", \"lake\", \"sea\", \"colosseo\", \"colosseo2\", \"map\", 'turing', 'piramide', 'vertical forest']\n","image_url = \"\" #@param {type:\"string\"}\n","url = image_url if image_url else urls[example_image]\n","\n","generators = {\n","    'A to B': loaded_gan_model.G_AB,\n","    'B to A': loaded_gan_model.G_BA,\n","}\n","\n","generator_direction =  'A to B' #@param [\"A to B\", \"B to A\"]\n","generator = generators[generator_direction]\n","\n","force_image_resize = False #@param {type:\"boolean\"}\n","\n","\n","def styling_from_url(url, gen, image_transforms):\n","    img =  Image.open(urlopen(url))\n","\n","    rimg = image_transforms(img)[None, ...].cuda()\n","    rimg_u = gen(rimg)\n","\n","    imgs = torchvision.utils.make_grid(\n","        [rimg.squeeze(), rimg_u.squeeze()],\n","        nrow=2,\n","        normalize=True,\n","        pad_value=1,\n","        ).permute(1, 2, 0).detach().cpu()\n","    return imgs\n","\n","img_transforms = transforms.Compose(\n","            [\n","                transforms.Resize([cfg.img_height, cfg.img_width], Image.BICUBIC),\n","                loaded_gan_model.val_image_transforms\n","            ]\n","        )\n","\n","\n","img = styling_from_url(url, generator, img_transforms if force_image_resize else loaded_gan_model.val_image_transforms)\n","plt.imshow(img)\n","plt.axis('off')\n","plt.title('Generator input                 Generator output')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xYU2e8ApPIi5"},"source":["## **EXERCISE**\n","> Implement another application of the CycleGAN besides the `maps` and `ukiyoe2photo` we saw in this section.\n",">\n","> To reduce the long training times you may:\n","> - Try to reduce the float precision of the model weights/use the TPUs. The Trainer class is able to do this automagically [through a parameter](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/mnist-tpu-training.html).\n","> - Try to re-use one of the two pretrained models provided in this notebook and do **transfer learning** on the new dataset, e.g. freezing some layers and fine-tuning the others (success is not guaranteed!).\n",">\n","> You can use the pre-made datasets from the [official repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix), e.g. to translate horses in zebras\n",">\n","> ![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/horse2zebra.gif)\n",">\n","> If you feel brave you can use any two dataset of images with different but intuitively correlated distributions you can find online.\n",">\n","> *Bonus question*: why does the grass change colour in the previous gif?"]},{"cell_type":"markdown","metadata":{"id":"I1dpw44VszE-"},"source":["# Adversarial examples\n","\n","We have heard many times the [pompous claim](https://www.forbes.com/sites/michaelthomsen/2015/02/19/microsofts-deep-learning-project-outperforms-humans-in-image-recognition/#3a6934a2740b) of deep neural networks being better than humans in the task of recognizing object in images. If such a statement undoubtely does its job in pointing out to the laymen the huge leap forward made by neural networks in this and a multitude of other tasks, the deep learning practitioner should know its [extent](http://karpathy.github.io/2012/10/22/state-of-computer-vision/).\n","\n","Today we will see spectacular failures of state-of-the-art deep neural networks right in object recongition; imperceptible but targeted changes to correctly classified examples lead to surprising misclassifications from a human point of view, we call them *adversarial examples*.\n","\n","<img src=\"https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/10/pics/bus.jpeg\" width=\"400\">\n","\n","### [Supervised learning: powerful feature learning yet still fragile](https://www.qualcomm.com/news/onq/2020/05/13/far-ai-can-see-what-we-still-need-build-human-level-intelligence?fbclid=IwAR12fZ-hh7LghpK_GxkR2c_XiFiMTsJ1dZSYSxSWwS6jLIl2Ia8wk68Wnrk)\n","\n","Apart from the obvious security concerns that adversarial examples raise - consider even for a moment the application of autonomous driving - this vulnerability casts many doubts on the degree of generalization reached by DNNs. After seeing the image above, we have also probably updated our judgement about the *intelligence* needed to solve the task of object recognition in images.\n","\n","Indeed this is genuine progress, we need to understand better *what is* intelligence before making an artificial intelligence."]},{"cell_type":"markdown","metadata":{"id":"oZcBHGV8zw_p"},"source":["## The fast gradient sign attack\n","\n","Today we will experiment with the Fast Gradient Sign Attack (FGSM) on a large ResNet50 pretrained on ImageNet. This attack is described by Goodfellow et al. in the very readable paper [*Explaining and Harnessing Adversarial Examples*](https://arxiv.org/abs/1412.6572), upon which this tutorial is based.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hkTSm28A0Ah_"},"source":["### The linear explanation of adversarial examples\n","\n","As in the paper, we start explaining the existence of adversarial examples in high dimensional linear models.\n","\n","Let $x$ be a correctly classified example and $\\tilde{x} = x + \\eta$ its adversarial, obtained adding a small perturbation $\\eta$. How can we build such a $\\eta$ and how big should it be to lead to a misclassification?\n","\n","The output of a linear model for the input $x$ can be written as $w^\\top x$, so for the adversarial example:\n","\n","$$w^\\top \\tilde{x} = w^\\top x + w^ \\top \\eta$$\n","\n","Respect to the original input, the adversarial perturbation causes the output to grow by $w^\\top \\eta$. We can maximize this increase by taking $\\eta = \\epsilon \\, \\text{sign}(w)$, where $\\epsilon$ controls the magnitude of the perturbation, remember we want it to be small.\n","\n",">**EXERCISE**: A typical constraint to keep the perturbation small is $\\|\\eta \\|_\\infty < k$.\n","Can you imagine why the infinite norm is a reasonable bound to a perturbation? Remember that $\\|\\eta \\|_\\infty = \\max_{i}((\\eta_1, ... , \\eta_n))$. Answer in the paper, section 3.\n","\n","Let's make some considerations on the magnitude of the perturbation when $\\eta = \\epsilon \\, \\text{sign}(w)$.\n","\n","$w^\\top \\eta$ will be proportional to $\\epsilon m n$, where $m$ is the average absolute value of the weight vector $w$ and $n$ is the dimensionaliy of the input. Notice that $\\|\\eta \\|_\\infty$ does not growth with the dimensionality of the problem, so the change in the output caused by the perturbation grows linearly with $n$, i.e. many infinitesimal changes to the input add up to a huge change in the output in a high-dimensional problem.\n","\n","Notice also how the average magnitude of the weights $m$ is involved, a model with small weights is less vulnerable than one with large weights.\n","\n",">**EXERCISE:** Do we have studied methods which promote actively small weights? Which was the rationale behind such methods and how it relates to this new vulnerability concept?"]},{"cell_type":"markdown","metadata":{"id":"SGzRdotuht29"},"source":["### Linear perturbation of non-linear models\n","\n","I know what are you thinking right now...\n","\n",">*Wait! All these arguments are simple and convincing but in the end do not apply to DNNs, since DNNs are highly non-linear models! Am I wrong?*\n","\n","Actually DNNs are definetely non-linear respect to their parameters, but are quite linear respect to their inputs. More precisely, DNNs are piecewise linear, with RELU activations, and these linear intervals are much bigger than we expect.\n","\n","The piecewise linearity of a DNN using RELU as activation function should not be so surprising. Nevertheless this apply also to more non-linear models such as networks using sigmoids as activation, since these are designed to spend most of their time in the linear regime of the sigmoid, where gradients does not go to zero, for instance using batch normalization. This linearity is a key-ingredient for an efficient gradient-based optimization.\n","\n","Are DNNs so much linear to be vulnerable to such kind of attack?\n","\n","We will experiment it in a minute. Let $f_\\theta$ be our DNN with parameters $\\theta$, if its output $y$ was linear respect to the input $x$ then $y = f_\\theta(x) \\sim w^\\top x$ for some $w(\\theta)$. Let $J(\\theta, x, y)$ be the cost used to train the neural network, which will be in a form:\n","\n","$$J(\\theta, x, y) = \\|f_\\theta(x) - y\\| \\sim \\| w^\\top x - y\\|$$\n","\n","now if we take the gradient respect to the input $x$:\n","\n","$$\\nabla_x J(\\theta, x, y) \\sim \\nabla_x(\\| w^\\top x - y\\|) = w$$\n","\n","so we can attack our DNN using the perturbation defined above $\\eta = \\epsilon \\, \\text{sign}(w)$ taking:\n","\n","$$\\eta = \\epsilon \\, \\text{sign}(\\nabla_x J(\\theta, x, y))$$\n","\n","We refer to this as the **fast gradient sign attack**, note that such a gradient can be computed very efficiently using backpropagation.\n","\n","This kind of attack will work only if DNNs are sufficiently linear, let's see!"]},{"cell_type":"markdown","metadata":{"id":"Fxoo8kBiF9CV"},"source":["## Experimenting with adversarial examples on Imagenette\n"]},{"cell_type":"markdown","metadata":{"id":"x2jEQEPG9VKv"},"source":["### Goliath\n","Let's start by loading a state-of-the-art model in object classification.\n","\n","Ladies and gentlemen, directly from the pretrained models on torchvision...\n","\n","**ResNet50**\n","\n","Capable of a quite remarkable [76.15%](https://pytorch.org/vision/stable/models.html) top-1 accuracy on the ImageNet classification task (1000 classes).\n"]},{"cell_type":"code","metadata":{"id":"9tsN_-XRa5vZ"},"source":["model = torchvision.models.resnet50(pretrained=True)\n","model.eval()\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzSn8CAs-dDx"},"source":["### Imagenette, a convenient subset of ImageNet\n","\n","The full ImageNet dataset weighs approximately 300 GB.\n","\n","A very small fraction of it would be sufficient for our experiment, so we will download [Imagenette](https://github.com/fastai/imagenette), a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n","\n","Our model takes in input 224x224 images, so the version resized to 320 pixel is enough for us."]},{"cell_type":"code","metadata":{"id":"jMgGJ0N2KzrW"},"source":["!wget 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nwoYnUrCgYh"},"source":["Let's unpack it and load in a Pytorch Dataset transformed just as the pretrained model wants."]},{"cell_type":"code","metadata":{"id":"bVB61LkaLLIk"},"source":["!tar -xzf '/content/imagenette2-320.tgz'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hx5itJVGKYPo"},"source":["test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),  # this transform perform always the same crop in the center of the image, we do not want to augment the validation dataset\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","testset = torchvision.datasets.ImageFolder(root='/content/imagenette2-320/val', transform=test_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jL0PNXPJQ3YY"},"source":["A dictionary containing the human readable labels of ImageNet will turn useful."]},{"cell_type":"code","metadata":{"id":"b0cnOk3YQuQx"},"source":["!wget 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch2/imagenet_classes.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mF4ur3HRQsMY"},"source":["class_names = {}\n","names_class = {}\n","label = 0\n","with open('/content/imagenet_classes.txt') as f:\n","    for line in f:\n","        class_string = line.split(',')[0].replace('\\n', '')\n","        class_names[label] = class_string\n","        names_class[class_string] = label\n","        label += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFbFFKAGZF_i"},"source":["sub_class_names = ['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4O2zRD9Am9O"},"source":["Actually we do not need all the images in Imagenette for our experiment. Let's take only 50 samples evenly separated in index to grasp every class. The ```Subset``` class of PyTorch will make it very easy.\n"]},{"cell_type":"code","metadata":{"id":"YGRSeIe3MT2g"},"source":["print('test set size: {}'.format(len(testset)))\n","indices = list(range(0, len(testset), len(testset) // 50 ))\n","sub_testset = torch.utils.data.Subset(testset, indices)\n","print('subtest set size: {}'.format(len(sub_testset)))\n","\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=True, num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_Zu7LEAEzhm"},"source":["As always, let's visualize some samples to get an idea of the classification task.\n","\n","Finally we work with something more than a toy-model capable to process only postage-stamp sized images.\n","\n","We are ready for the fall of the giant."]},{"cell_type":"code","metadata":{"cellView":"form","id":"U2pwsBrwOO9X"},"source":["# @title Visualize samples function\n","\n","def visualize_samples(inputs, title=None):\n","    \"\"\"\n","    Visualization of transformed samples, a standard call:\n","        inputs, classes = next(iter(dataloaders['train']))\n","        visualize_samples(inputs)\n","    Arguments:\n","    batch_of_samples -- a batch from the dataloader; a PyTorch tensor of shape (batch_size, 3, 224, 224)\n","\n","    Return:\n","    None (A nice plot)\n","    \"\"\"\n","\n","    # Make a grid from batch\n","    inp = torchvision.utils.make_grid(inputs)\n","\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)  # plotly accepts the colour information both in the 0-1 range and in the 0-255 range\n","    fig = px.imshow(inp, title=title)\n","    fig.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pse1HjhpOSVW"},"source":["# Get a batch of training data\n","inputs, classes = next(iter(test_loader))\n","\n","visualize_samples(inputs, title=f'Ground truth: {[sub_class_names[x] for x in classes]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKvHafsUt4dW"},"source":["### The fall of the giant\n","\n","Let's craft our slingshot, the function defining the FGSM attack.\n","\n","$$\\tilde{x} = x + \\epsilon \\, \\text{sign}(\\nabla_x J(\\theta, x, y))$$\n","\n"]},{"cell_type":"code","metadata":{"id":"iY5w-vDnt4da"},"source":["def fgsm_attack(image, epsilon, data_grad):\n","\n","    perturbed_image = image + epsilon * data_grad.sign()\n","\n","    return perturbed_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1eC2YACrt4dj"},"source":["Now it's time to wrap a convenient test function. We will run our attack tracking the accuracy of the model on the adversarial dataset over several $\\epsilon$.\n","\n","For each $\\epsilon$ we will save five miscalssified images to look later at their appearence, we expect them to be almost indistinguishable by human eye from the original ones.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_fTPDk0Yt4dl"},"source":["def test( model, device, test_loader, epsilon ):\n","\n","    # Accuracy counter\n","    correct = 0\n","    old_target_item = 0\n","    adv_examples = []\n","\n","    # Loop over all examples in test set\n","    for data, target in tqdm(test_loader):\n","\n","        # The target of the reduced dataset should be mapped to the one of ImageNet\n","        target.data = torch.tensor([names_class[sub_class_names[target.item()]]])\n","        data, target = data.to(device), target.to(device)\n","\n","        # Differently fromt training, we require the gradient respect to the input\n","        data.requires_grad = True\n","\n","        # We collect the prediction on the original sample\n","        output = model(data)\n","        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","\n","        # If the initial prediction is wrong, dont bother attacking, just move on\n","        if init_pred.item() != target.item():\n","            continue\n","\n","        # We will use negative log likelihood since this pretrained model comes equipped with a final LogSoftmax layer\n","        loss = F.nll_loss(output, target)\n","\n","        model.zero_grad()\n","        loss.backward()\n","\n","        # We have to collect the gradient respect to the input\n","        data_grad = data.grad.data\n","\n","        # We get the perturbed image from the FGSM function\n","        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n","\n","        # We collect the prediction on the adversarial example\n","        output = model(perturbed_data)\n","\n","        # We want to track the accuracy and keep some perturbed images\n","        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","        if final_pred.item() == target.item():\n","            correct += 1\n","            # Special case for saving 0 epsilon examples\n","            if (epsilon == 0) and (len(adv_examples) < 5) and final_pred.item() != old_target_item:\n","                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n","                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n","                old_target_item = final_pred.item()\n","        else:\n","            # Save some adv examples for visualization later\n","            if len(adv_examples) < 5:\n","                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n","                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n","\n","    # Calculate final accuracy for this epsilon\n","    final_acc = correct/float(len(test_loader))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n","\n","    # Return the accuracy and an adversarial example\n","    return final_acc, adv_examples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_x3tTP6jt4d-"},"source":["Time to attack!\n","\n","Let's define a bunch of $\\epsilon$, we will track the accuracy of the model on our dataset for each one.\n","\n","> **EXERCISE:** How do you expect the trend of accuracy vs $\\epsilon$?\n","\n","We will include the $\\epsilon=0$ case which represents the original test accuracy,\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"u9rzDEF5-K_j"},"source":["epsilons = [0, 0.0005, 0.0013, 0.002, 0.004, 0.006, 0.008, 0.01, 0.1]\n","\n","# in this example we will compute the adversarial example of one sample at a time, so we redefine a testloader woth batch_size=1\n","test_loader = torch.utils.data.DataLoader(sub_testset, batch_size=1,\n","                                         shuffle=False, num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTFf3mFGt4d_"},"source":["accuracies = []\n","examples = []\n","\n","# Run test for each epsilon\n","for eps in epsilons:\n","    acc, ex = test(model, device, test_loader, eps)\n","    accuracies.append(acc)\n","    examples.append(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WpkNOdKot4eD"},"source":["### Results\n","\n","Let's take a look at the accuracy trend respect to $\\epsilon$, as expected we register a degradation of the performance as $\\epsilon$ increase.\n","\n","The trend is almost linear in a finite range of $\\epsilon$, then we register a slowdown in the approach to zero accuracy.\n","\n",">**EXERCISE**: What do you expect to happen for negative values of $\\epsilon$? Try it, how do you explain this behaviour?\n","\n",">**EXERCISE**: Are you able to reach a 0 accuracy? Which is the minimum $\\epsilon$ needed for such a result? Have you noticed something unexpected, how do you explain this behaviour?\n","\n","\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Kx89Q5nwt4eE"},"source":["# @title Accuracy vs epsilon plot\n","\n","plt.figure(figsize=(20,5))\n","plt.plot(epsilons, accuracies, \"*-\")\n","plt.yticks(np.arange(0, 1.1, step=0.1))\n","plt.xticks(np.arange(0, 0.105, step=0.005))\n","plt.title(\"Accuracy vs Epsilon\")\n","plt.xlabel(\"Epsilon\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7NLnhDht4eJ"},"source":["It is now the moment to look at some perturbed images!\n","\n","As expected the perturbations are almost imperceptible, humans have no problem at all in identifying such images, we have a chance to spot something only for the higher $\\epsilon$s, where the accuracy of the model is already below 30%.\n","\n","The giant has fallen.\n","\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"KkKY4GQot4eL"},"source":["# @title A nice plot of some misclassified perturbed images for each epsilon\n","cnt = 0\n","plt.figure(figsize=(20,40))\n","for i in range(len(epsilons)):\n","    for j in range(len(examples[i])):\n","        cnt += 1\n","        plt.subplot(len(epsilons),len(examples[0]),cnt)\n","        plt.xticks([], [])\n","        plt.yticks([], [])\n","        if j == 0:\n","            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n","        orig,adv,ex = examples[i][j]\n","\n","        ex = ex.transpose((1, 2, 0))\n","        mean = np.array([0.485, 0.456, 0.406])\n","        std = np.array([0.229, 0.224, 0.225])\n","        ex = std * ex + mean\n","        ex = np.clip(ex, 0, 1)\n","        ex = ex.transpose((2, 0, 1))\n","\n","        plt.title(\"{} -> {}\".format(class_names[orig], class_names[adv]))\n","        plt.imshow(np.moveaxis(ex, 0, -1))\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yxx5XuiclzQe"},"source":[">**EXERCISE:** Visualize the adversarial perturbation for some images, i.e. $\\tilde{x} - x$\n","\n",">**EXERCISE**: A nice feature of the FGSM attack is its reduced cost, we can perform it very efficiently through backpropagation. Nevertheless in this notebook we have processed each input separately, can you arrange the code to work with ```batch_size``` > 1 and be even more efficient?\n","\n","*tutorial on adversarial examples adapted from [this](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html) PyTorch tutorial*\n"]}]}