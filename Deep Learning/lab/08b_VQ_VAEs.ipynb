{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning & Applied AI\n",
        "\n",
        "We recommend going through the notebook using Google Colaboratory.\n",
        "\n",
        "# Tutorial 8b: VQ-VAEs\n",
        "\n",
        "In this tutorial, we will cover a modern variant of VAEs that triggered a great shift in generative AI!\n",
        "\n",
        "Author:\n",
        "\n",
        "- Prof. Emanuele Rodol√†\n",
        "\n",
        "Course:\n",
        "\n",
        "- Website and notebooks will be available at https://github.com/erodola/DLAI-s2-2025/"
      ],
      "metadata": {
        "id": "M_8WNSsqoKsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: From VAEs to VQ-VAEs\n",
        "\n",
        "Welcome! In this lab, you will implement your first **Vector-Quantized Variational Autoencoder (VQ-VAE)**, an important extension of VAEs that combines ideas from **discrete latent variables** and **vector quantization**.\n",
        "\n",
        "You already know that in a standard VAE, the encoder maps an image to a single continuous latent vector, typically sampled from a Gaussian distribution.\n",
        "\n",
        "VQ-VAEs change this in two key ways:\n",
        "\n",
        "1. Instead of a single continuous vector, **each image is mapped to a small 2D grid of discrete codes** (one code per local region).\n",
        "\n",
        "2. Instead of Gaussian sampling, each code is selected from a fixed learned **codebook of discrete embeddings**.\n",
        "\n",
        "This method is simple yet very powerful: it allows models to learn more structured, symbolic, and compressed representations.  \n",
        "\n",
        "VQ-VAEs are especially useful for tasks like generation (e.g., speech, images, and video), and later inspired models like DALL¬∑E and VQVAE-2!\n",
        "\n",
        "In this lab, you will **read the original VQ-VAE paper**:  \n",
        "> **[VQ-VAE: Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)** by Oord et al. (2017)\n",
        "\n",
        "As you read and implement, focus on these **key differences** compared to classic VAEs:\n",
        "- **No KL divergence**: no need for continuous latent regularization\n",
        "- **Discrete codebook lookup**: latents snap to nearest embedding vectors.\n",
        "- **Latent space is spatial**: encoder outputs a grid (e.g., 7√ó7), not just a vector.\n",
        "- **Commitment loss**: a special loss term encourages the encoder to commit to a single embedding without fluctuating too much.\n",
        "- **Straight-through estimator**: allows backpropagation through discrete code assignments.\n",
        "\n",
        "During this lab, you will:\n",
        "- Implement a simple VQ layer from scratch.\n",
        "- Train a basic VQ-VAE on the MNIST dataset.\n",
        "- Visualize how well your VQ-VAE can reconstruct digits using only discrete latents!\n",
        "\n",
        "Good luck ‚Äî and have fun exploring this new generation of representation learning! üöÄ"
      ],
      "metadata": {
        "id": "2XjWf4QuTSEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Koh2I-CpH_D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST Dataset\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
        "    batch_size=128, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "pkaGMSXcICKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below you'll find a barebones implementation of convolutional `Encoder` and `Decoder` to start experimenting (should be enough for MNIST digits). Feel free to change to your own architecture!"
      ],
      "metadata": {
        "id": "4cYujsq8gmiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, hidden_dim, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim, embedding_dim, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_dim, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(hidden_dim, 1, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = torch.sigmoid(self.conv2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "5hKzWlBlIJFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, focus on completing the `VectorQuantizer`'s `forward` method below.\n",
        "\n",
        "You need to compute distances, find nearest embeddings, and apply the straight-through estimator. Don't worry about getting perfect reconstructions ‚Äî if the digits look recognizable, you're good."
      ],
      "metadata": {
        "id": "pJwVTSY3VZ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Quantizer (‚úèÔ∏è your solution here)\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embeddings.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (batch, channel, height, width)\n",
        "        z_flattened = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z_flattened.view(-1, self.embedding_dim)\n",
        "\n",
        "        # TODO: Compute distances (L2) between z_flattened and embeddings\n",
        "        # distances = ‚úèÔ∏è\n",
        "\n",
        "        # TODO: Find closest embeddings\n",
        "        # encoding_indices = ‚úèÔ∏è\n",
        "        # encodings = ‚úèÔ∏è\n",
        "\n",
        "        # TODO: Quantize and unflatten\n",
        "        # quantized = ‚úèÔ∏è\n",
        "\n",
        "        # TODO: Compute loss\n",
        "        # e_latent_loss = ‚úèÔ∏è\n",
        "        # loss = ‚úèÔ∏è\n",
        "\n",
        "        # Straight-through estimator trick\n",
        "        # quantized = z + (quantized - z).detach()\n",
        "\n",
        "        # return quantized, loss\n",
        "        raise NotImplementedError(\"Fill in the VectorQuantizer forward pass!\")"
      ],
      "metadata": {
        "id": "M7EkNIohIcWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution üëÄ\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embeddings.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (batch, channel, height, width)\n",
        "        z_flattened = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z_flattened.view(-1, self.embedding_dim)\n",
        "\n",
        "        # Compute distances\n",
        "        distances = (torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
        "                     + torch.sum(self.embeddings.weight**2, dim=1)\n",
        "                     - 2 * torch.matmul(z_flattened, self.embeddings.weight.t()))\n",
        "\n",
        "        # Find nearest embeddings\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
        "\n",
        "        # Quantize\n",
        "        quantized = torch.matmul(encodings, self.embeddings.weight)\n",
        "        quantized = quantized.view(z.shape[0], z.shape[2], z.shape[3], self.embedding_dim)\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), z)  # push encoder outputs closer to embeddings\n",
        "        q_latent_loss = F.mse_loss(quantized, z.detach())  # improve embeddings to match encoder outputs\n",
        "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized = z + (quantized - z).detach()\n",
        "\n",
        "        return quantized, loss"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DTQ8tFzvKe--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test your `VectorQuantizer` layer in the full model below:"
      ],
      "metadata": {
        "id": "du1VXNcUiwZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full VQ-VAE Model\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, embedding_dim=64, num_embeddings=512, commitment_cost=0.25):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim, embedding_dim)\n",
        "        self.vq = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
        "        self.decoder = Decoder(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        quantized, vq_loss = self.vq(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return x_recon, vq_loss"
      ],
      "metadata": {
        "id": "hbEBfN-NIghv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's a training loop for you to use:"
      ],
      "metadata": {
        "id": "ofYPooy3i2dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VQVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, vq_loss = model(data)\n",
        "        recon_loss = F.mse_loss(recon_batch, data)\n",
        "        loss = recon_loss + vq_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
        "\n",
        "    print(f'====> Epoch: {epoch} Average loss: {total_loss / len(train_loader):.4f}')"
      ],
      "metadata": {
        "id": "IGArGX9EIp_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwdsbxP8VG8f"
      },
      "outputs": [],
      "source": [
        "# Visualize Reconstructions\n",
        "\n",
        "model.eval()\n",
        "data_iter = iter(train_loader)\n",
        "images, _ = next(data_iter)\n",
        "images = images.to(device)\n",
        "\n",
        "reconstructions, _ = model(images)\n",
        "\n",
        "n = 8\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(images[i][0].cpu().detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(reconstructions[i][0].cpu().detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Make it more efficient\n",
        "\n",
        "Try to run the model again with the following tweaks:\n",
        "\n",
        "- Smaller codebook: for example, 128 embeddings instead of 512\n",
        "\n",
        "- Smaller embedding dimension: e.g. 32 instead of 64\n",
        "\n",
        "- Fewer channels in the encoder/decoder: e.g. 64 instead of 128\n",
        "\n",
        "- Train for less epochs (3 instead of 5)\n",
        "\n",
        "You should be able to train a working VQ-VAE for MNIST in **under 20 seconds** on GPU!"
      ],
      "metadata": {
        "id": "5vhKI0fbjdtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Sample new images\n",
        "\n",
        "We have seen that we can sample the latent spaces of AEs and VAEs to **generate new data points**. Can this be done for VQ-VAEs as well? Let's explore this possibility here.\n",
        "\n",
        "First, keep in mind that the encoder downsamples the input image, e.g. to a **latent 7√ó7 grid**. Each position in this grid corresponds to one code from the discrete codebook.\n",
        "\n",
        "Write the code to do the following:\n",
        "\n",
        "- Randomly pick integers in [0, num_embeddings) to create a new latent grid.\n",
        "\n",
        "- Map those integers to their embedding vectors.\n",
        "\n",
        "- Reshape this back into (batch_size, embedding_dim, 7, 7).\n",
        "\n",
        "- Finally, pass this through the decoder to generate images!"
      ],
      "metadata": {
        "id": "WczSVxMTk20H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# make sure these values correspond to the ones you chose earlier\n",
        "latent_height = 7\n",
        "latent_width = 7\n",
        "embedding_dim = 64\n",
        "num_embeddings = 512\n",
        "\n",
        "# Randomly sample latent codes, map to embeddings, decode\n",
        "# ‚úèÔ∏è\n",
        "\n",
        "# Visualize generated images\n",
        "# ‚úèÔ∏è"
      ],
      "metadata": {
        "id": "YdgomeUZmz3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution üëÄ\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# make sure these values correspond to the ones you chose earlier\n",
        "latent_height = 7\n",
        "latent_width = 7\n",
        "embedding_dim = 64\n",
        "num_embeddings = 512\n",
        "\n",
        "# Randomly sample latent codes\n",
        "num_samples = 8  # how many new images to generate\n",
        "random_codes = torch.randint(0, num_embeddings, (num_samples, latent_height, latent_width))\n",
        "\n",
        "# Map random codes to embeddings\n",
        "# (num_samples x latent_height x latent_width x embedding_dim)\n",
        "embeddings = model.vq.embeddings.weight\n",
        "\n",
        "# Prepare quantized latent grid\n",
        "quantized_latents = embeddings[random_codes]  # (num_samples, 7, 7, embedding_dim)\n",
        "quantized_latents = quantized_latents.permute(0, 3, 1, 2).contiguous()  # (num_samples, embedding_dim, 7, 7)\n",
        "\n",
        "# Decode sampled latents into images\n",
        "with torch.no_grad():\n",
        "    generated_images = model.decoder(quantized_latents.to(device))\n",
        "\n",
        "# Visualize generated images\n",
        "n = num_samples\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(generated_images[i][0].cpu().detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Generated Samples from Random Discrete Latents', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j33lr0oxQF-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are getting bullet holes on a wall like in old gangster movies, **don't panic!** You are safe here.\n",
        "\n",
        "Simply put: the \"holes\" are valid codes and the \"wall\" represents untrained codes.\n",
        "\n",
        "In VQ-VAEs, not all codes are used equally. If you sample randomly, you might pick codes that were never trained, leading to garbage outputs. Instead, sampling from **used codes** focuses generation on the parts of the latent space where the model actually learned something meaningful!\n",
        "\n",
        "> **TL;DR** You want to avoid sampling from \"dead\" areas of the codebook that were never trained properly.\n",
        "\n",
        "Therefore, you could:\n",
        "\n",
        "1. First collect the set of indices that were used by the encoder during training.\n",
        "\n",
        "2. Then sample from that set only.\n",
        "\n",
        "We're doing this for you in the code block below:"
      ],
      "metadata": {
        "id": "l10cYloZbS90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Build a set of used code indices\n",
        "# (Encode a big batch of real images to find used codes)\n",
        "\n",
        "model.eval()\n",
        "used_codes_set = set()\n",
        "\n",
        "for i, (data, _) in enumerate(train_loader):\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        z = model.encoder(data)\n",
        "\n",
        "    z_flattened = z.permute(0, 2, 3, 1).contiguous().view(-1, model.vq.embedding_dim)\n",
        "    distances = (torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
        "                 + torch.sum(model.vq.embeddings.weight**2, dim=1)\n",
        "                 - 2 * torch.matmul(z_flattened, model.vq.embeddings.weight.t()))\n",
        "    encoding_indices = torch.argmin(distances, dim=1)\n",
        "\n",
        "    used_codes_set.update(encoding_indices.cpu().numpy())\n",
        "\n",
        "    if i > 10:\n",
        "        break  # Only scan a few batches to save time\n",
        "\n",
        "used_codes = torch.tensor(list(used_codes_set))\n",
        "\n",
        "print(f\"Found {len(used_codes)} used codes out of {model.vq.num_embeddings} total codes.\")\n",
        "\n",
        "# 2. Now sample from used_codes only\n",
        "num_samples = 8\n",
        "latent_height = 7\n",
        "latent_width = 7\n",
        "\n",
        "random_codes = used_codes[torch.randint(0, len(used_codes), (num_samples, latent_height, latent_width))]\n",
        "\n",
        "# 3. Map sampled codes back to embeddings\n",
        "embeddings = model.vq.embeddings.weight\n",
        "\n",
        "quantized_latents = embeddings[random_codes]  # (num_samples, 7, 7, embedding_dim)\n",
        "quantized_latents = quantized_latents.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "# 4. Decode\n",
        "with torch.no_grad():\n",
        "    generated_images = model.decoder(quantized_latents.to(device))\n",
        "\n",
        "# 5. Visualize\n",
        "n = num_samples\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(generated_images[i][0].cpu().detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Generated Samples (from used latent codes)', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hd81iSvqbpc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No more holes, but looks like garbage! **What's happening now?**\n",
        "\n",
        "We're sampling codes that were actually used by the encoder, but still **assembling them randomly without any spatial coherence**.\n",
        "\n",
        "It's exactly why full generative models like VQ-VAE-2 or DALL¬∑E use autoregressive models over the codes to sample coherent patterns. We'll study autoregressive models later down the course!\n",
        "\n",
        "In practice, in the original VQ-VAE paper, they train a PixelCNN model on the discrete latent codes to sample spatially coherent code grids."
      ],
      "metadata": {
        "id": "R-XlaTpwcID-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We come to the following, very important take-home message:\n",
        "\n",
        "> ***Separate learning representations (e.g. VQ-VAE) and modeling their distribution (e.g. PixelCNN).***"
      ],
      "metadata": {
        "id": "bc1Rlgcsc6Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Interpolate between two images in the latent space!\n",
        "\n",
        "In this final task, you'll do the prototypical test from AE literature, with the VQ-VAE twist:\n",
        "\n",
        "- Pick two real images from the dataset.\n",
        "\n",
        "- Encode both images into their discrete latent codes.\n",
        "\n",
        "- Create a sequence of blended latent codes between them.\n",
        "\n",
        "- Decode each blend and visualize the transformation!\n",
        "\n",
        "Note that since the latent space is discrete, you can't simply take an average ‚Äî but you can crossfade between the two codes patch-by-patch!"
      ],
      "metadata": {
        "id": "6bMBQ6h6aFzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick two images from the dataset\n",
        "\n",
        "model.eval()\n",
        "data_iter = iter(train_loader)\n",
        "images, _ = next(data_iter)\n",
        "images = images.to(device)\n",
        "\n",
        "img1 = images[0:1]\n",
        "img2 = images[10:11]\n",
        "\n",
        "# Visualize originals\n",
        "\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Image 1')\n",
        "plt.imshow(img1[0][0].cpu().detach(), cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Image 2')\n",
        "plt.imshow(img2[0][0].cpu().detach(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Encode both images\n",
        "# ‚úèÔ∏è\n",
        "\n",
        "# Flatten and find nearest embeddings\n",
        "# ‚úèÔ∏è\n",
        "\n",
        "# Interpolate between codes\n",
        "num_steps = 8\n",
        "interpolated_images = []\n",
        "# ‚úèÔ∏è\n",
        "\n",
        "# Plot interpolation results\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i, img in enumerate(interpolated_images):\n",
        "    ax = plt.subplot(1, num_steps, i + 1)\n",
        "    plt.imshow(img[0][0].cpu().detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Latent Space Interpolation', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lWGaeoU0rvlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution üëÄ\n",
        "\n",
        "# Pick two images from the dataset\n",
        "\n",
        "model.eval()\n",
        "data_iter = iter(train_loader)\n",
        "images, _ = next(data_iter)\n",
        "images = images.to(device)\n",
        "\n",
        "img1 = images[0:1]\n",
        "img2 = images[10:11]\n",
        "\n",
        "# Visualize originals\n",
        "\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Image 1')\n",
        "plt.imshow(img1[0][0].cpu().detach(), cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Image 2')\n",
        "plt.imshow(img2[0][0].cpu().detach(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Encode both images\n",
        "with torch.no_grad():\n",
        "    z1 = model.encoder(img1)\n",
        "    z2 = model.encoder(img2)\n",
        "\n",
        "# Flatten and find nearest embeddings\n",
        "def encode_to_indices(z, model):\n",
        "    z_flat = z.permute(0,2,3,1).contiguous().view(-1, model.vq.embedding_dim)\n",
        "    distances = (torch.sum(z_flat**2, dim=1, keepdim=True)\n",
        "                 + torch.sum(model.vq.embeddings.weight**2, dim=1)\n",
        "                 - 2 * torch.matmul(z_flat, model.vq.embeddings.weight.t()))\n",
        "    indices = torch.argmin(distances, dim=1)\n",
        "    return indices.view(7, 7)\n",
        "\n",
        "codes1 = encode_to_indices(z1, model)\n",
        "codes2 = encode_to_indices(z2, model)\n",
        "\n",
        "# Interpolate between codes\n",
        "num_steps = 8\n",
        "interpolated_images = []\n",
        "\n",
        "for alpha in torch.linspace(0, 1, steps=num_steps):\n",
        "    # For each location, randomly pick from img1 or img2 codes based on alpha\n",
        "    mask = (torch.rand_like(codes1.float()) < alpha).long()\n",
        "    blended_codes = codes1 * (1 - mask) + codes2 * mask\n",
        "\n",
        "    # Map codes to embeddings\n",
        "    blended_latents = model.vq.embeddings(blended_codes.flatten())\n",
        "    blended_latents = blended_latents.view(1, 7, 7, model.vq.embedding_dim)\n",
        "    blended_latents = blended_latents.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "    # Decode to image\n",
        "    with torch.no_grad():\n",
        "        img = model.decoder(blended_latents.to(device))\n",
        "    interpolated_images.append(img)\n",
        "\n",
        "# 4. Plot interpolation results\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i, img in enumerate(interpolated_images):\n",
        "    ax = plt.subplot(1, num_steps, i + 1)\n",
        "    plt.imshow(img[0][0].cpu().detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Latent Space Interpolation', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1E6P27XXaSVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You've just built the core of a modern generative model.** üöÄ\n",
        "\n",
        "With a few more steps, like learning a prior over the discrete codes, you could generate entirely new images, just like models behind DALL¬∑E!\n",
        "\n",
        "You're closer to state-of-the-art generative AI than you might think. Keep experimenting ‚Äî the next breakthrough could come from you!"
      ],
      "metadata": {
        "id": "XIWWkTpkszPf"
      }
    }
  ]
}