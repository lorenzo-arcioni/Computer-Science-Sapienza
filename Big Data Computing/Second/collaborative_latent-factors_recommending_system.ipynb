{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2350192,"sourceType":"datasetVersion","datasetId":1418899}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lorenzoarcioni/collaborative-latent-factors-recommending-system?scriptVersionId=218933887\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":" # Collaborative Latent-Factors-Based Filtering for Movie Recommendations (Incomplete)\n\n## Introduction\n\nThe latent factor approach in recommendation systems utilizes matrix factorization techniques to uncover hidden patterns in user-item interactions. These methods predict user preferences by mapping both users and items to a shared latent space where their interactions can be represented by their proximity or alignment. Latent factor models, such as Singular Value Decomposition (SVD), are widely used in this context.\n\nKey features of the latent factor approach:\n- Captures underlying relationships between users and items.\n- Handles sparse datasets effectively by reducing dimensionality.\n- Improves scalability compared to neighborhood-based methods.\n\nIn this notebook, we will explore the latent factor approach to build a movie recommendation system using matrix factorization.\n\n### Mathematical Background\n\nThe latent factor approach works by:\n- Represent users and items in a shared lower dimensional latent space (i.e., as a vector of latent factors).\n- Such vectros are inferred (i.e., learned) from the observed ratings.\n- High correlation between user and item latent factors indicates a possible recomendation.\n- Map both users and items to the latent space and then predict ratings based on the inner product in the latent space.\n\nSo formally we have:\n- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n- $\\vec x_u \\in R^d$ is the latent factor vector for user $u$. Each $\\vec x_u[k] \\in R$ measure the extent of interest user $u$ has in items exhibiting latent factor $k$.\n- $\\vec w_i \\in R^d$ is the latent factor vector for item $i$. Each $\\vec w_i[k] \\in R$ measure the extent of interest item $i$ has in users exhibiting latent factor $k$.\n\nEssentially, $d$ hidden features to describe both users and items.\n\nThus, $r_{u,i}$ is the rating given by user $u$ to item $i$ and $\\hat{r}_{u,i} = \\vec x_u \\cdot \\vec w_i = \\sum_{k=1}^d \\vec x_u[k] \\cdot \\vec w_i[k]$ is the predicted rating for user $u$ and item $i$.\n\nThe problem is to approximate the user-item matrix $M \\in \\mathbb R^{n \\times m}$ with the product of a user latent factor matrix $X \\in \\mathbb R^{n \\times d}$ and an item latent factor matrix $W^T \\in \\mathbb R^{d \\times m}$. So\n\n$$\nM \\approx X \\cdot W^T.\n$$\n\n## Dataset Description\n\nWe use two datasets for this analysis:\n\n1. **Movies Dataset**:\n   - `Movie_ID`: Unique identifier for each movie.\n   - `Title`: Name of the movie.\n   - `Year`: Year the movie was released.\n\n2. **Ratings Dataset**:\n   - `User_ID`: Unique identifier for each user.\n   - `Movie_ID`: Identifier for the movie rated by the user.\n   - `Rating`: Numeric rating provided by the user (e.g., on a scale of 1-5).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\n# Load the datasets\nmovies_file = \"/kaggle/input/netflix-movie-rating-dataset/Netflix_Dataset_Movie.csv\"\nratings_file = \"/kaggle/input/netflix-movie-rating-dataset/Netflix_Dataset_Rating.csv\"\n\nratings = pd.read_csv(ratings_file)  # Columns: User_ID, Rating, Movie_ID\nmovies  = pd.read_csv(movies_file)    # Columns: Movie_ID, Year, Name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:51:30.06603Z","iopub.execute_input":"2025-01-23T14:51:30.066415Z","iopub.status.idle":"2025-01-23T14:51:36.777713Z","shell.execute_reply.started":"2025-01-23T14:51:30.066386Z","shell.execute_reply":"2025-01-23T14:51:36.776429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Formal Definition\n\n- $U = \\{u_1, u_2, \\dots, u_n\\}$ is the set of users.\n- $U_i = \\{u \\in U \\mid r_{u,i} \\neq 0\\}$ is the set of users who have rated item $i$\n- $I = \\{i_1, i_2, \\dots, i_m\\}$ is the set of items.\n- $I_u = \\{i \\in I \\mid r_{u,i} \\neq 0\\}$ is the set of items rated by user $u$\n- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n- $r_{u,i}$ is the rating given by user $u$ for item $i$ (equal to 0 if not rated).\n- $D = \\{(u_j, i_j)\\}_{j=1}^{N}$ is the set of user-item pairs (our dataset).\n- $I_D = \\{i \\in I \\mid \\exists (u, i) \\in D\\}$ is the set of items in the dataset.\n- $U_D = \\{u \\in U \\mid \\exists (u, i) \\in D\\}$ is the set of users in the dataset.","metadata":{}},{"cell_type":"markdown","source":"## User-Based Collaborative Filtering\n\n### 1. Data Preprocessing\n- **User-Item Matrix Creation**: Convert the ratings dataset into a user-item matrix, where rows represent users and columns represent movies. Missing ratings are filled with zeros. Each rating is represented by a number from 1 to 5.\n  $$ M[u, i] = r_{u,i} \\in R$$\n  Where:\n  - $u \\in U$\n  - $i \\in I$\n  - $r_{u,i}$ is the rating given by user $u$ for movie $i$.\n\n- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format for memory optimization:\n  $$M_{\\{\\text{sparse}\\}} = \\text{sparse}(M)$$","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a user-item matrix\nuser_item_matrix = ratings.pivot(index='User_ID', columns='Movie_ID', values='Rating')\n\n# Fill missing values with 0 (can use NaN for some algorithms)\nuser_item_matrix.fillna(0, inplace=True) # It is not the case for this dataset\n\n# Convert the DataFrame to a sparse matrix\nsparse_user_item = csr_matrix(user_item_matrix.values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:51:36.778865Z","iopub.execute_input":"2025-01-23T14:51:36.779157Z","iopub.status.idle":"2025-01-23T14:51:59.633805Z","shell.execute_reply.started":"2025-01-23T14:51:36.779134Z","shell.execute_reply":"2025-01-23T14:51:59.632618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Determine the Loss Function and Computing Its Gradient\n\nAssuming we have access to the dataset $D$ of observed ratings, the matrix $M$ is partially known and filled with those observations. To actually learn the latent factors, we need to choose a loss function to optimize. In our case, we choose squared error (SE):\n\n$$\nL(X, W) = \\frac{1}{2} \\left[ \\sum_{(u, i) \\in D} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda (\\sum_{u \\in U_D} \\|\\vec x_u\\|^2 + \\sum_{i \\in I_D} \\|\\vec w_i\\|^2)\\right]\n$$\n\nThus, \n\n$$X^*, W^* = \\argmin_{X, W} \\ L(X, W).$$\n\n#### Loss Function\nThe loss function in matrix notation is defined in terms of matrices as:\n$$\nL(X, W) = \\frac{1}{2} \\left[ \\| M - X W^T \\|_F^2 + \\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right) \\right],\n$$\nwhere:\n- $M \\in \\mathbb{R}^{n \\times m}$ is the observed rating matrix, with $M_{u,i} = r_{u,i}$ if user $u$ has rated item $i$, and 0 otherwise.\n- $X \\in \\mathbb{R}^{n \\times d}$ represents the user latent factors (each row corresponds to a user vector $X_u$).\n- $W \\in \\mathbb{R}^{m \\times d}$ represents the item latent factors (each row corresponds to an item vector $W_i$).\n- $\\| \\cdot \\|_F$ is the Frobenius norm.\n\nThe prediction matrix is:\n$$\n\\hat{M} = X W^T.\n$$\n\nThe loss consists of:\n1. The reconstruction error:\n$$\n\\| M - X W^T \\|_F^2 = \\sum_{(u, i) \\in D} (r_{u,i} - X_u W_i^T)^2.\n$$\n2. The regularization terms:\n$$\n\\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right).\n$$\n\n---\n\n#### Computing the Gradients\n\n##### Gradient with respect to $X$\n\n1. Differentiate the reconstruction error term:\n$$\n\\frac{\\partial}{\\partial X} \\frac{1}{2} \\| M - X W^T \\|_F^2 = -(M - X W^T) W.\n$$\n\n2. Differentiate the regularization term:\n$$\n\\frac{\\partial}{\\partial X} \\frac{\\lambda}{2} \\|X\\|_F^2 = \\lambda X.\n$$\n\n3. Combine the two terms:\n$$\n\\frac{\\partial L}{\\partial X} = -(M - X W^T) W + \\lambda X.\n$$\n\n---\n\n##### Gradient with respect to $W$\n\n1. Differentiate the reconstruction error term:\n$$\n\\frac{\\partial}{\\partial W} \\frac{1}{2} \\| M - X W^T \\|_F^2 = -(M - X W^T)^T X.\n$$\n\n2. Differentiate the regularization term:\n$$\n\\frac{\\partial}{\\partial W} \\frac{\\lambda}{2} \\|W\\|_F^2 = \\lambda W.\n$$\n\n3. Combine the two terms:\n$$\n\\frac{\\partial L}{\\partial W} = -(M - X W^T)^T X + \\lambda W.\n$$","metadata":{}},{"cell_type":"code","source":"# Step 2: Creating the matrix M\n\n# Dimensions of the user-item matrix\nnum_users, num_items = user_item_matrix.shape\n\n# Create a mask for observed entries in R\nM = user_item_matrix.values \nmask = M > 0  # Boolean mask for observed entries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:51:59.635531Z","iopub.execute_input":"2025-01-23T14:51:59.635824Z","iopub.status.idle":"2025-01-23T14:51:59.941799Z","shell.execute_reply.started":"2025-01-23T14:51:59.6358Z","shell.execute_reply":"2025-01-23T14:51:59.940655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Optimize the Loss Function with Stochastic Gradient Descent\n\nIn order to optimize the loss function, we use Stochastic Gradient Descent (SGD).\n\n#### Explanation of the SGD Algorithm (Matrix Form)\n\n##### 1. Initialization\n- Matrices $X$ (users' latent factors) and $W$ (items' latent factors) are initialized randomly with small values.\n- $X \\in \\mathbb{R}^{m \\times d}$, where $m$ is the number of users and $d$ is the number of latent factors.\n- $W \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of items.\n\n##### 2. Gradient Computation\n- Define the prediction matrix:\n  $$\n  \\hat{M} = X W^T\n  $$\n- Compute the error matrix (only for observed entries in $M$):\n  $$\n  E = \\begin{cases}\n  M_{ui} - \\hat{M}_{ui} \\quad &\\text{if} \\ M_{ui} > 0\\\\\n  0 \\quad &\\text{otherwise}\n  \\end{cases},\n  $$\n  where $E_{ui} = 0$ for unobserved entries of $M$.\n\n- Gradients for $X$ and $W$:\n  $$\n  \\nabla_X = - E W + \\lambda X\n  $$\n  $$\n  \\nabla_W = - E^T X + \\lambda W\n  $$\n\n##### 3. Updates\n- Update the latent factor matrices $X$ and $W$ simultaneously:\n  $$\n  X \\leftarrow X - \\eta \\nabla_X\n  $$\n  $$\n  W \\leftarrow W - \\eta \\nabla_W\n  $$\n- Here, $\\eta$ is the learning rate.\n\n##### 4. Loss Tracking\n- The total loss for each epoch combines the squared error and the regularization terms:\n  $$\n  L = \\| M - X W^T \\|_F^2 + \\lambda (\\|X\\|_F^2 + \\|W\\|_F^2)\n  $$\n- This tracks the reconstruction error and ensures that the latent factor matrices do not grow too large (controlled by the regularization term).\n\n##### 5. Optimization Loop\n- Repeat the following steps for a fixed number of epochs or until the loss converges:\n  1. Compute the error matrix $E$.\n  2. Compute the gradients $\\nabla_X$ and $\\nabla_W$ using matrix operations.\n  3. Update $X$ and $W$ using the gradients.\n  4. Track and print the loss for each epoch.\n\n---\n\n##### Notes\n- This implementation only updates $X$ and $W$ for the observed entries of $M$ using matrix masking.\n- The hyperparameters ($\\eta$, $d$, and $\\lambda$) should be tuned based on the dataset for optimal performance.","metadata":{}},{"cell_type":"code","source":"!pip install tqdm_joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:53:17.60044Z","iopub.execute_input":"2025-01-23T14:53:17.600846Z","iopub.status.idle":"2025-01-23T14:53:24.71097Z","shell.execute_reply.started":"2025-01-23T14:53:17.600805Z","shell.execute_reply":"2025-01-23T14:53:24.709668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport numpy as np\nfrom tqdm import tqdm\nfrom tqdm_joblib import tqdm_joblib  # Per gestire le barre di avanzamento con joblib\n\n# Set the numpy seed\nnp.random.seed(42)\n\n# Hyperparameters\nnum_factors = 300  # Number of latent factors (k)\nreg_lambda = 0.0001  # Regularization term (lambda)\ngradient_clip = 10.0  # Gradient clipping threshold\nnum_epochs = 30 # Number of epochs\nlearning_rate = 0.001 # Learning rate (eta)\n\n# Initialize X and W with small random values\nX = 0.1 * np.random.randn(num_users, num_factors)\nW = 0.1 * np.random.randn(num_items, num_factors)\n\n# Loss history\nloss_history = []\n\n# Function to compute gradients for a single user (for X) or item (for W)\ndef compute_gradient_X(u):\n    rated_items = mask[u, :]  # Mask for items rated by user u\n    W_rated = W[rated_items]  # Subset of W for rated items\n    M_rated = M[u, rated_items]  # Subset of M for rated items\n\n    # Compute error and gradient for user u\n    E_u = M_rated - X[u, :] @ W_rated.T\n    grad = -E_u @ W_rated + reg_lambda * X[u, :]\n    return np.clip(grad, -gradient_clip, gradient_clip)\n\ndef compute_gradient_W(i):\n    rated_users = mask[:, i]  # Mask for users who rated item i\n    X_rated = X[rated_users]  # Subset of X for rated users\n    M_rated = M[rated_users, i]  # Subset of M for rated users\n\n    # Compute error and gradient for item i\n    E_i = M_rated - X_rated @ W[i, :].T\n    grad = -E_i.T @ X_rated + reg_lambda * W[i, :]\n    return np.clip(grad, -gradient_clip, gradient_clip)\n\n# SGD Loop\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    \n    # Compute the predicted matrix\n    M_hat = X @ W.T\n\n    # Compute the error matrix for observed entries only\n    E = np.multiply(mask, M - M_hat)\n\n    # Parallel computation of gradients with progress bars\n    grad_X = np.zeros_like(X)\n    grad_W = np.zeros_like(W)\n\n    # Update X with a shared progress bar\n    print(\"  Gradient wrt X...\")\n    with tqdm_joblib(tqdm(desc=\"    Users\", total=num_users, leave=True, disable=True)) as _:\n        grad_X = np.array(\n            Parallel(n_jobs=-1)(delayed(compute_gradient_X)(u) for u in range(num_users))\n        )\n\n    # Update W with a shared progress bar\n    print(\"  Gradient wrt W...\")\n    with tqdm_joblib(tqdm(desc=\"    Items\", total=num_items, leave=True, disable=True)) as _:\n        grad_W = np.array(\n            Parallel(n_jobs=-1)(delayed(compute_gradient_W)(i) for i in range(num_items))\n        )\n\n    # Apply updates\n    X -= learning_rate * grad_X\n    W -= learning_rate * grad_W\n\n    # Compute the total loss\n    reconstruction_loss = np.sum(np.multiply(mask, E) ** 2)\n    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n    total_loss = reconstruction_loss + regularization_loss\n\n    # Append the total loss to the history\n    loss_history.append(total_loss)\n\n    # Print the loss for the current epoch\n    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n    print(f\"  Total Loss: {total_loss:.4f}\")\n\n    # Debugging: Check mean and std of gradients\n    print(f\"  Gradient X: mean={np.mean(grad_X):.4f}, std={np.std(grad_X):.4f}\")\n    print(f\"  Gradient W: mean={np.mean(grad_W):.4f}, std={np.std(grad_W):.4f}\")\n\n# Salvataggio delle matrici X e W in formato .npy\nnp.save(\"X_matrix_sgd.npy\", X)\nnp.save(\"W_matrix_sgd.npy\", W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:57:24.900716Z","iopub.execute_input":"2025-01-23T14:57:24.90119Z","execution_failed":"2025-01-23T15:03:36.946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\n# Plot the loss history\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-')\nplt.xlabel('Epoch')\nplt.ylabel('Total Loss')\nplt.title('Loss over Epochs for SGD')\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:17:03.743109Z","iopub.execute_input":"2025-01-22T15:17:03.743467Z","iopub.status.idle":"2025-01-22T15:17:03.93871Z","shell.execute_reply.started":"2025-01-22T15:17:03.743436Z","shell.execute_reply":"2025-01-22T15:17:03.937851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimize the Loss Function with Alternating Least Squares (ALS)\n\nAlternating Least Squares (ALS) is an optimization method for matrix factorization that alternates between updating the user latent factors ($X$) and the item latent factors ($W$).\n\n#### Objective\nThe goal is to minimize the following loss function:\n$$\nL(X, W) = \\|M - XW^T\\|_F^2 + \\lambda (\\|X\\|_F^2 + \\|W\\|_F^2)\n$$\nWhere:\n- $M$: User-item interaction matrix.\n- $X$: User latent factor matrix ($m \\times k$).\n- $W$: Item latent factor matrix ($n \\times k$).\n- $\\lambda$: Regularization parameter.\n\n#### ALS Algorithm\n1. **Initialization**:\n   - Start with random values for $X$ and $W$.\n2. **Alternating Updates**:\n   - Fix $W$, solve for $X$:\n     $$\n     X_u = (W^T W + \\lambda I)^{-1} W^T M_u\n     $$\n   - Fix $X$, solve for $W$:\n     $$\n     W_i = (X^T X + \\lambda I)^{-1} X^T M_i\n     $$\n3. **Convergence**:\n   - Iterate until the loss stabilizes or a set number of epochs is reached.","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport numpy as np\nfrom tqdm import tqdm\nfrom tqdm_joblib import tqdm_joblib  # Progress bars synchronized with joblib\n\n# Set the numpy seed\nnp.random.seed(42)\n\n# Hyperparameters\nnum_factors = 500  # Number of latent factors (k)\nreg_lambda = 0.0001  # Regularization term (lambda)\nnum_epochs = 60 # Number of epochs\n\n# Initialize X and W with small random values\nX = np.random.normal(scale=0.01, size=(num_users, num_factors))\nW = np.random.normal(scale=0.01, size=(num_items, num_factors))\n\n# List to store the loss values for each iteration\nloss_history = []\n\ndef update_user(u, W, M, mask, reg_lambda, num_factors):\n    \"\"\"Update a single user's latent factors.\"\"\"\n    rated_items = mask[u, :]  # Mask for items rated by user u\n    W_rated = W[rated_items, :]\n    M_rated = M[u, rated_items]\n    \n    A = W_rated.T @ W_rated + reg_lambda * np.eye(num_factors)\n    b = W_rated.T @ M_rated\n    return np.linalg.solve(A, b)\n\ndef update_item(i, X, M, mask, reg_lambda, num_factors):\n    \"\"\"Update a single item's latent factors.\"\"\"\n    rated_users = mask[:, i]  # Mask for users who rated item i\n    X_rated = X[rated_users, :]\n    M_rated = M[rated_users, i]\n    \n    A = X_rated.T @ X_rated + reg_lambda * np.eye(num_factors)\n    b = X_rated.T @ M_rated\n    return np.linalg.solve(A, b)\n\n# ALS iterations\nfor iteration in range(num_epochs):\n    print(f\"Iteration {iteration + 1}/{num_epochs}\")\n    \n    # Update X by fixing W (Parallelized) with synchronized progress bar\n    print(\"  Updating X...\")\n    with tqdm_joblib(tqdm(desc=\"    Users\", total=num_users, leave=True, disable=True)) as _:\n        X = np.array(Parallel(n_jobs=-1)(\n            delayed(update_user)(u, W, M, mask, reg_lambda, num_factors) for u in range(num_users)\n        ))\n    \n    # Update W by fixing X (Parallelized) with synchronized progress bar\n    print(\"  Updating W...\")\n    with tqdm_joblib(tqdm(desc=\"    Items\", total=num_items, leave=True, disable=True)) as _:\n        W = np.array(Parallel(n_jobs=-1)(\n            delayed(update_item)(i, X, M, mask, reg_lambda, num_factors) for i in range(num_items)\n        ))\n    \n    # Compute the loss\n    M_hat = X @ W.T\n    reconstruction_loss = np.sum(np.multiply(mask, (M - M_hat) ** 2))\n    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n    total_loss = reconstruction_loss + regularization_loss\n    \n    # Save the loss in history\n    loss_history.append(total_loss)\n    \n    # Print loss for the current iteration\n    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n    print(f\"  Total Loss: {total_loss:.4f}\")\n\n# Salvataggio delle matrici X e W in formato .npy\nnp.save(\"X_matrix_als.npy\", X)\nnp.save(\"W_matrix_als.npy\", W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:20:13.764044Z","iopub.execute_input":"2025-01-22T15:20:13.764449Z","iopub.status.idle":"2025-01-22T15:20:23.711609Z","shell.execute_reply.started":"2025-01-22T15:20:13.764394Z","shell.execute_reply":"2025-01-22T15:20:23.710263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\n# Plot the loss history\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-')\nplt.xlabel('Epoch')\nplt.ylabel('Total Loss')\nplt.title('Loss over Epochs for ALS')\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T22:05:13.30844Z","iopub.execute_input":"2025-01-21T22:05:13.308751Z","iopub.status.idle":"2025-01-21T22:05:13.472608Z","shell.execute_reply.started":"2025-01-21T22:05:13.308716Z","shell.execute_reply":"2025-01-21T22:05:13.471826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recommend_movies(user_id, M_hat, user_item_matrix, movies, top_n=10):\n    \"\"\"\n    Recommends movies for a given user based on the predicted matrix M_hat.\n\n    Parameters:\n    - user_id: ID of the user to whom the recommendations will be made.\n    - M_hat: Predicted user-item matrix (num_users x num_items).\n    - user_item_matrix: Original user-item matrix (Pandas DataFrame) with ratings.\n    - movies: DataFrame containing movie details (Movie_ID, Name, Year).\n    - top_n: Number of recommendations to return (default is 10).\n\n    Returns:\n    - recommendations: DataFrame containing the top_n recommended movies.\n    \"\"\"\n    # Map user_id to the corresponding index in M_hat\n    user_index = user_item_matrix.index.get_loc(user_id)\n    \n    # Get the predicted ratings for the user\n    predicted_ratings = M_hat[user_index]\n\n    # Get the user's original ratings\n    original_ratings = user_item_matrix.loc[user_id]\n\n    # Find movies that the user has not rated (those with a rating of 0)\n    unrated_movies = original_ratings[original_ratings == 0].index\n\n    # Map the unrated movies to the correct columns in M_hat\n    unrated_predictions = {\n        movie_id: predicted_ratings[user_item_matrix.columns.get_loc(movie_id)]\n        for movie_id in unrated_movies\n    }\n\n    # Sort the predicted ratings for unrated movies in descending order\n    sorted_predictions = sorted(unrated_predictions.items(), key=lambda x: x[1], reverse=True)\n\n    # Get the top_n movie IDs\n    top_movie_ids = [movie_id for movie_id, _ in sorted_predictions[:top_n]]\n\n    # Retrieve movie details for the top_n recommendations\n    recommendations = movies[movies['Movie_ID'].isin(top_movie_ids)]\n\n    # Create a copy of the DataFrame to avoid the SettingWithCopyWarning\n    recommendations = recommendations.copy()\n    \n    # Add the Predicted_Rating column\n    recommendations['Predicted_Rating'] = [unrated_predictions[movie_id] for movie_id in recommendations['Movie_ID']]\n    \n    # Sort recommendations by predicted rating (optional, for clarity)\n    recommendations = recommendations.sort_values(by='Predicted_Rating', ascending=False)\n\n    return recommendations[['Movie_ID', 'Name', 'Year', 'Predicted_Rating']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T22:06:40.648797Z","iopub.execute_input":"2025-01-21T22:06:40.649099Z","iopub.status.idle":"2025-01-21T22:06:40.655461Z","shell.execute_reply.started":"2025-01-21T22:06:40.649076Z","shell.execute_reply":"2025-01-21T22:06:40.65451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Model Testing\nThe function is tested with a sample user to generate personalized recommendations.","metadata":{}},{"cell_type":"code","source":"# Step 4: Test the recommendation function\nuser_id_to_test = 774868  # Select a test user\nnum_recommendations = 5   # Number of recommendations\n\ntry:\n    # Exec the recommendation function\n    recommendations = recommend_movies(user_id_to_test, M_hat, user_item_matrix, movies, top_n=num_recommendations)\n    \n    # Show the results\n    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test}:\")\n    print(recommendations[['Name', 'Year', 'Predicted_Rating', 'Movie_ID']])\nexcept KeyError as e:\n    print(f\"Error: User ID {user_id_to_test} not found in the dataset.\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T22:06:46.677857Z","iopub.execute_input":"2025-01-21T22:06:46.678181Z","iopub.status.idle":"2025-01-21T22:06:46.706761Z","shell.execute_reply.started":"2025-01-21T22:06:46.67814Z","shell.execute_reply":"2025-01-21T22:06:46.70593Z"}},"outputs":[],"execution_count":null}]}