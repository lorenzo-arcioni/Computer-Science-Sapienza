{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Collaborative Latent-Factors-Based Filtering for Movie Recommendations (Incomplete)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The latent factor approach in recommendation systems utilizes matrix factorization techniques to uncover hidden patterns in user-item interactions. These methods predict user preferences by mapping both users and items to a shared latent space where their interactions can be represented by their proximity or alignment. Latent factor models, such as Singular Value Decomposition (SVD), are widely used in this context.\n",
    "\n",
    "Key features of the latent factor approach:\n",
    "- Captures underlying relationships between users and items.\n",
    "- Handles sparse datasets effectively by reducing dimensionality.\n",
    "- Improves scalability compared to neighborhood-based methods.\n",
    "\n",
    "In this notebook, we will explore the latent factor approach to build a movie recommendation system using matrix factorization.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "The latent factor approach works by:\n",
    "- Represent users and items in a shared lower dimensional latent space (i.e., as a vector of latent factors).\n",
    "- Such vectros are inferred (i.e., learned) from the observed ratings.\n",
    "- High correlation between user and item latent factors indicates a possible recomendation.\n",
    "- Map both users and items to the latent space and then predict ratings based on the inner product in the latent space.\n",
    "\n",
    "So formally we have:\n",
    "- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n",
    "- $\\vec x_u \\in R^d$ is the latent factor vector for user $u$. Each $\\vec x_u[k] \\in R$ measure the extent of interest user $u$ has in items exhibiting latent factor $k$.\n",
    "- $\\vec w_i \\in R^d$ is the latent factor vector for item $i$. Each $\\vec w_i[k] \\in R$ measure the extent of interest item $i$ has in users exhibiting latent factor $k$.\n",
    "\n",
    "Essentially, $d$ hidden features to describe both users and items.\n",
    "\n",
    "Thus, $r_{u,i}$ is the rating given by user $u$ to item $i$ and $\\hat{r}_{u,i} = \\vec x_u \\cdot \\vec w_i = \\sum_{k=1}^d \\vec x_u[k] \\cdot \\vec w_i[k]$ is the predicted rating for user $u$ and item $i$.\n",
    "\n",
    "The problem is to approximate the user-item matrix $M \\in \\mathbb R^{n \\times m}$ with the product of a user latent factor matrix $X \\in \\mathbb R^{n \\times d}$ and an item latent factor matrix $W^T \\in \\mathbb R^{d \\times m}$. So\n",
    "\n",
    "$$\n",
    "M \\approx X \\cdot W^T.\n",
    "$$\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "We use two datasets for this analysis:\n",
    "\n",
    "1. **Movies Dataset**:\n",
    "   - `Movie_ID`: Unique identifier for each movie.\n",
    "   - `Title`: Name of the movie.\n",
    "   - `Year`: Year the movie was released.\n",
    "\n",
    "2. **Ratings Dataset**:\n",
    "   - `User_ID`: Unique identifier for each user.\n",
    "   - `Movie_ID`: Identifier for the movie rated by the user.\n",
    "   - `Rating`: Numeric rating provided by the user (e.g., on a scale of 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load the dataset\n",
    "ratings = pd.read_csv(\"./data/Netflix_Dataset_Rating.csv\")  # Columns: User_ID, Rating, Movie_ID\n",
    "movies  = pd.read_csv(\"./data/Netflix_Dataset_Movie.csv\")    # Columns: Movie_ID, Year, Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "- $U = \\{u_1, u_2, \\dots, u_n\\}$ is the set of users.\n",
    "- $U_i = \\{u \\in U \\mid r_{u,i} \\neq 0\\}$ is the set of users who have rated item $i$\n",
    "- $I = \\{i_1, i_2, \\dots, i_m\\}$ is the set of items.\n",
    "- $I_u = \\{i \\in I \\mid r_{u,i} \\neq 0\\}$ is the set of items rated by user $u$\n",
    "- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n",
    "- $r_{u,i}$ is the rating given by user $u$ for item $i$ (equal to 0 if not rated).\n",
    "- $D = \\{(u_j, i_j)\\}_{j=1}^{N}$ is the set of user-item pairs (our dataset).\n",
    "- $I_D = \\{i \\in I \\mid \\exists (u, i) \\in D\\}$ is the set of items in the dataset.\n",
    "- $U_D = \\{u \\in U \\mid \\exists (u, i) \\in D\\}$ is the set of users in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **User-Item Matrix Creation**: Convert the ratings dataset into a user-item matrix, where rows represent users and columns represent movies. Missing ratings are filled with zeros. Each rating is represented by a number from 1 to 5.\n",
    "  $$ M[u, i] = r_{u,i} \\in R$$\n",
    "  Where:\n",
    "  - $u \\in U$\n",
    "  - $i \\in I$\n",
    "  - $r_{u,i}$ is the rating given by user $u$ for movie $i$.\n",
    "\n",
    "- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format for memory optimization:\n",
    "  $$M_{\\{\\text{sparse}\\}} = \\text{sparse}(M)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a user-item matrix\n",
    "user_item_matrix = ratings.pivot(index='User_ID', columns='Movie_ID', values='Rating')\n",
    "\n",
    "# Fill missing values with 0 (can use NaN for some algorithms)\n",
    "user_item_matrix.fillna(0, inplace=True) # It is not the case for this dataset\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determine the Loss Function and Computing Its Gradient\n",
    "\n",
    "Assuming we have access to the dataset $D$ of observed ratings, the matrix $M$ is partially known and filled with those observations. To actually learn the latent factors, we need to choose a loss function to optimize. In our case, we choose squared error (SE):\n",
    "\n",
    "$$\n",
    "L(X, W) = \\frac{1}{2} \\left[ \\sum_{(u, i) \\in D} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda (\\sum_{u \\in U_D} \\|\\vec x_u\\|^2 + \\sum_{i \\in I_D} \\|\\vec w_i\\|^2)\\right]\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$X^*, W^* = \\argmin_{X, W} \\ L(X, W).$$\n",
    "\n",
    "#### Loss Function\n",
    "The loss function in matrix notation is defined in terms of matrices as:\n",
    "$$\n",
    "L(X, W) = \\frac{1}{2} \\left[ \\| M - X W^T \\|_F^2 + \\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right) \\right],\n",
    "$$\n",
    "where:\n",
    "- $M \\in \\mathbb{R}^{n \\times m}$ is the observed rating matrix, with $M_{u,i} = r_{u,i}$ if user $u$ has rated item $i$, and 0 otherwise.\n",
    "- $X \\in \\mathbb{R}^{n \\times d}$ represents the user latent factors (each row corresponds to a user vector $X_u$).\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ represents the item latent factors (each row corresponds to an item vector $W_i$).\n",
    "- $\\| \\cdot \\|_F$ is the Frobenius norm.\n",
    "\n",
    "The prediction matrix is:\n",
    "$$\n",
    "\\hat{M} = X W^T.\n",
    "$$\n",
    "\n",
    "The loss consists of:\n",
    "1. The reconstruction error:\n",
    "$$\n",
    "\\| M - X W^T \\|_F^2 = \\sum_{(u, i) \\in D} (r_{u,i} - X_u W_i^T)^2.\n",
    "$$\n",
    "2. The regularization terms:\n",
    "$$\n",
    "\\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Computing the Gradients\n",
    "\n",
    "##### Gradient with respect to $X$\n",
    "\n",
    "1. Differentiate the reconstruction error term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X} \\frac{1}{2} \\| M - X W^T \\|_F^2 = -(M - X W^T) W.\n",
    "$$\n",
    "\n",
    "2. Differentiate the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X} \\frac{\\lambda}{2} \\|X\\|_F^2 = \\lambda X.\n",
    "$$\n",
    "\n",
    "3. Combine the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = -(M - X W^T) W + \\lambda X.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Gradient with respect to $W$\n",
    "\n",
    "1. Differentiate the reconstruction error term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\frac{1}{2} \\| M - X W^T \\|_F^2 = -(M - X W^T)^T X.\n",
    "$$\n",
    "\n",
    "2. Differentiate the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\frac{\\lambda}{2} \\|W\\|_F^2 = \\lambda W.\n",
    "$$\n",
    "\n",
    "3. Combine the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = -(M - X W^T)^T X + \\lambda W.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setting up the model parameters\n",
    "\n",
    "# Hyperparameters\n",
    "num_factors = 250  # Number of latent factors (k)\n",
    "learning_rate = 0.001  # Learning rate (eta)\n",
    "reg_lambda = 0.0001  # Regularization term (lambda)\n",
    "num_epochs = 25  # Number of epochs\n",
    "gradient_clip = 10.0  # Gradient clipping threshold\n",
    "\n",
    "# Dimensions of the user-item matrix\n",
    "num_users, num_items = user_item_matrix.shape\n",
    "\n",
    "# Initialize latent factor matrices X and W with small random values\n",
    "X = np.random.normal(scale=0.01, size=(num_users, num_factors))\n",
    "W = np.random.normal(scale=0.01, size=(num_items, num_factors))\n",
    "\n",
    "# Create a mask for observed entries in R\n",
    "M = user_item_matrix.values \n",
    "mask = M > 0  # Boolean mask for observed entries\n",
    "\n",
    "# List to store loss values for plotting\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimize the Loss Function with Stochastic Gradient Descent\n",
    "\n",
    "In order to optimize the loss function, we use Stochastic Gradient Descent (SGD).\n",
    "\n",
    "#### Explanation of the SGD Algorithm (Matrix Form)\n",
    "\n",
    "##### 1. Initialization\n",
    "- Matrices $X$ (users' latent factors) and $W$ (items' latent factors) are initialized randomly with small values.\n",
    "- $X \\in \\mathbb{R}^{m \\times d}$, where $m$ is the number of users and $d$ is the number of latent factors.\n",
    "- $W \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of items.\n",
    "\n",
    "##### 2. Gradient Computation\n",
    "- Define the prediction matrix:\n",
    "  $$\n",
    "  \\hat{M} = X W^T\n",
    "  $$\n",
    "- Compute the error matrix (only for observed entries in $M$):\n",
    "  $$\n",
    "  E = \\begin{cases}\n",
    "  M_{ui} - \\hat{M}_{ui} \\quad &\\text{if} \\ M_{ui} > 0\\\\\n",
    "  0 \\quad &\\text{otherwise}\n",
    "  \\end{cases},\n",
    "  $$\n",
    "  where $E_{ui} = 0$ for unobserved entries of $M$.\n",
    "\n",
    "- Gradients for $X$ and $W$:\n",
    "  $$\n",
    "  \\nabla_X = - E W + \\lambda X\n",
    "  $$\n",
    "  $$\n",
    "  \\nabla_W = - E^T X + \\lambda W\n",
    "  $$\n",
    "\n",
    "##### 3. Updates\n",
    "- Update the latent factor matrices $X$ and $W$ simultaneously:\n",
    "  $$\n",
    "  X \\leftarrow X - \\eta \\nabla_X\n",
    "  $$\n",
    "  $$\n",
    "  W \\leftarrow W - \\eta \\nabla_W\n",
    "  $$\n",
    "- Here, $\\eta$ is the learning rate.\n",
    "\n",
    "##### 4. Loss Tracking\n",
    "- The total loss for each epoch combines the squared error and the regularization terms:\n",
    "  $$\n",
    "  L = \\| M - X W^T \\|_F^2 + \\lambda (\\|X\\|_F^2 + \\|W\\|_F^2)\n",
    "  $$\n",
    "- This tracks the reconstruction error and ensures that the latent factor matrices do not grow too large (controlled by the regularization term).\n",
    "\n",
    "##### 5. Optimization Loop\n",
    "- Repeat the following steps for a fixed number of epochs or until the loss converges:\n",
    "  1. Compute the error matrix $E$.\n",
    "  2. Compute the gradients $\\nabla_X$ and $\\nabla_W$ using matrix operations.\n",
    "  3. Update $X$ and $W$ using the gradients.\n",
    "  4. Track and print the loss for each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "##### Notes\n",
    "- This implementation only updates $X$ and $W$ for the observed entries of $M$ using matrix masking.\n",
    "- The hyperparameters ($\\eta$, $d$, and $\\lambda$) should be tuned based on the dataset for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  Reconstruction Loss: 243096443.3037\n",
      "  Regularization Loss: 0.3623\n",
      "  Total Loss: 243096443.6660\n",
      "  Gradient X: mean=0.0077, std=0.4150\n",
      "  Gradient W: mean=-0.0823, std=3.6455\n",
      "Epoch 2/25\n",
      "  Reconstruction Loss: 243085588.2208\n",
      "  Regularization Loss: 0.3640\n",
      "  Total Loss: 243085588.5848\n",
      "  Gradient X: mean=-0.0389, std=0.8452\n",
      "  Gradient W: mean=0.0466, std=5.2258\n",
      "Epoch 3/25\n",
      "  Reconstruction Loss: 243022754.1014\n",
      "  Regularization Loss: 0.3729\n",
      "  Total Loss: 243022754.4743\n",
      "  Gradient X: mean=-0.0426, std=2.1268\n",
      "  Gradient W: mean=-0.3657, std=7.5047\n",
      "Epoch 4/25\n",
      "  Reconstruction Loss: 242668486.6514\n",
      "  Regularization Loss: 0.4090\n",
      "  Total Loss: 242668487.0604\n",
      "  Gradient X: mean=-0.1634, std=3.8837\n",
      "  Gradient W: mean=-0.2031, std=9.1671\n",
      "Epoch 5/25\n",
      "  Reconstruction Loss: 241588463.8133\n",
      "  Regularization Loss: 0.5041\n",
      "  Total Loss: 241588464.3174\n",
      "  Gradient X: mean=-0.2049, std=5.4751\n",
      "  Gradient W: mean=-0.4861, std=9.6866\n",
      "Epoch 6/25\n",
      "  Reconstruction Loss: 239412121.5275\n",
      "  Regularization Loss: 0.6903\n",
      "  Total Loss: 239412122.2178\n",
      "  Gradient X: mean=-0.2932, std=6.7090\n",
      "  Gradient W: mean=-0.2655, std=9.8873\n",
      "Epoch 7/25\n",
      "  Reconstruction Loss: 235924296.7227\n",
      "  Regularization Loss: 0.9953\n",
      "  Total Loss: 235924297.7180\n",
      "  Gradient X: mean=-0.3002, std=7.6401\n",
      "  Gradient W: mean=-0.3357, std=9.9343\n",
      "Epoch 8/25\n",
      "  Reconstruction Loss: 231029064.8042\n",
      "  Regularization Loss: 1.4409\n",
      "  Total Loss: 231029066.2451\n",
      "  Gradient X: mean=-0.3180, std=8.3313\n",
      "  Gradient W: mean=-0.2300, std=9.9694\n",
      "Epoch 9/25\n",
      "  Reconstruction Loss: 224709608.5892\n",
      "  Regularization Loss: 2.0432\n",
      "  Total Loss: 224709610.6324\n",
      "  Gradient X: mean=-0.3025, std=8.8408\n",
      "  Gradient W: mean=-0.2519, std=9.9811\n",
      "Epoch 10/25\n",
      "  Reconstruction Loss: 217003480.7906\n",
      "  Regularization Loss: 2.8132\n",
      "  Total Loss: 217003483.6038\n",
      "  Gradient X: mean=-0.2954, std=9.2076\n",
      "  Gradient W: mean=-0.2202, std=9.9889\n",
      "Epoch 11/25\n",
      "  Reconstruction Loss: 207986907.1420\n",
      "  Regularization Loss: 3.7577\n",
      "  Total Loss: 207986910.8997\n",
      "  Gradient X: mean=-0.2791, std=9.4654\n",
      "  Gradient W: mean=-0.2198, std=9.9922\n",
      "Epoch 12/25\n",
      "  Reconstruction Loss: 197764085.3263\n",
      "  Regularization Loss: 4.8802\n",
      "  Total Loss: 197764090.2065\n",
      "  Gradient X: mean=-0.2662, std=9.6400\n",
      "  Gradient W: mean=-0.2143, std=9.9945\n",
      "Epoch 13/25\n",
      "  Reconstruction Loss: 186460761.9500\n",
      "  Regularization Loss: 6.1823\n",
      "  Total Loss: 186460768.1324\n",
      "  Gradient X: mean=-0.2548, std=9.7543\n",
      "  Gradient W: mean=-0.2126, std=9.9959\n",
      "Epoch 14/25\n",
      "  Reconstruction Loss: 174219920.3084\n",
      "  Regularization Loss: 7.6646\n",
      "  Total Loss: 174219927.9730\n",
      "  Gradient X: mean=-0.2461, std=9.8271\n",
      "  Gradient W: mean=-0.2131, std=9.9965\n",
      "Epoch 15/25\n",
      "  Reconstruction Loss: 161199499.3267\n",
      "  Regularization Loss: 9.3271\n",
      "  Total Loss: 161199508.6538\n",
      "  Gradient X: mean=-0.2398, std=9.8728\n",
      "  Gradient W: mean=-0.2144, std=9.9969\n",
      "Epoch 16/25\n",
      "  Reconstruction Loss: 147571789.6933\n",
      "  Regularization Loss: 11.1695\n",
      "  Total Loss: 147571800.8629\n",
      "  Gradient X: mean=-0.2355, std=9.9011\n",
      "  Gradient W: mean=-0.2162, std=9.9970\n",
      "Epoch 17/25\n",
      "  Reconstruction Loss: 133523613.2811\n",
      "  Regularization Loss: 13.1918\n",
      "  Total Loss: 133523626.4729\n",
      "  Gradient X: mean=-0.2329, std=9.9183\n",
      "  Gradient W: mean=-0.2179, std=9.9970\n",
      "Epoch 18/25\n",
      "  Reconstruction Loss: 119256819.0502\n",
      "  Regularization Loss: 15.3934\n",
      "  Total Loss: 119256834.4436\n",
      "  Gradient X: mean=-0.2315, std=9.9275\n",
      "  Gradient W: mean=-0.2196, std=9.9970\n",
      "Epoch 19/25\n",
      "  Reconstruction Loss: 104988760.8716\n",
      "  Regularization Loss: 17.7737\n",
      "  Total Loss: 104988778.6453\n",
      "  Gradient X: mean=-0.2311, std=9.9303\n",
      "  Gradient W: mean=-0.2211, std=9.9969\n",
      "Epoch 20/25\n",
      "  Reconstruction Loss: 90952589.8631\n",
      "  Regularization Loss: 20.3312\n",
      "  Total Loss: 90952610.1943\n",
      "  Gradient X: mean=-0.2313, std=9.9256\n",
      "  Gradient W: mean=-0.2228, std=9.9969\n",
      "Epoch 21/25\n",
      "  Reconstruction Loss: 77397711.4524\n",
      "  Regularization Loss: 23.0628\n",
      "  Total Loss: 77397734.5152\n",
      "  Gradient X: mean=-0.2320, std=9.9089\n",
      "  Gradient W: mean=-0.2240, std=9.9970\n",
      "Epoch 22/25\n",
      "  Reconstruction Loss: 64590533.0176\n",
      "  Regularization Loss: 25.9606\n",
      "  Total Loss: 64590558.9782\n",
      "  Gradient X: mean=-0.2329, std=9.8681\n",
      "  Gradient W: mean=-0.2248, std=9.9970\n",
      "Epoch 23/25\n",
      "  Reconstruction Loss: 52814997.7105\n",
      "  Regularization Loss: 29.0034\n",
      "  Total Loss: 52815026.7139\n",
      "  Gradient X: mean=-0.2333, std=9.7719\n",
      "  Gradient W: mean=-0.2247, std=9.9957\n",
      "Epoch 24/25\n",
      "  Reconstruction Loss: 42372400.3031\n",
      "  Regularization Loss: 32.1337\n",
      "  Total Loss: 42372432.4368\n",
      "  Gradient X: mean=-0.2311, std=9.5428\n",
      "  Gradient W: mean=-0.2213, std=9.9880\n",
      "Epoch 25/25\n",
      "  Reconstruction Loss: 33573404.2946\n",
      "  Regularization Loss: 35.2060\n",
      "  Total Loss: 33573439.5007\n",
      "  Gradient X: mean=-0.2204, std=9.0185\n",
      "  Gradient W: mean=-0.2131, std=9.9542\n"
     ]
    }
   ],
   "source": [
    "#4. Start SGD loop\n",
    "\n",
    "# Initialize M_hat as the predicted matrix\n",
    "M_hat = np.ndarray(shape=(num_users, num_items))\n",
    "\n",
    "# Perform SGD for a number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the predicted matrix\n",
    "    M_hat = X @ W.T\n",
    "\n",
    "    # Compute the error matrix for observed entries only\n",
    "    E = np.multiply(mask, M - M_hat)\n",
    "\n",
    "    # Compute gradients with regularization\n",
    "    grad_X = -E @ W + reg_lambda * X\n",
    "    grad_W = -E.T @ X + reg_lambda * W\n",
    "\n",
    "    # Apply gradient clipping to avoid exploding updates\n",
    "    grad_X = np.clip(grad_X, -gradient_clip, gradient_clip)\n",
    "    grad_W = np.clip(grad_W, -gradient_clip, gradient_clip)\n",
    "\n",
    "    # Update X and W\n",
    "    X -= learning_rate * grad_X\n",
    "    W -= learning_rate * grad_W\n",
    "\n",
    "    # Compute the total loss\n",
    "    reconstruction_loss = np.sum(np.multiply(mask, E) ** 2)\n",
    "    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n",
    "    total_loss = reconstruction_loss + regularization_loss\n",
    "\n",
    "    # Append the total loss to the history\n",
    "    loss_history.append(total_loss)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n",
    "    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Check for NaN or Inf values in X or W\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(W)) or np.any(np.isinf(X)) or np.any(np.isinf(W)):\n",
    "        print(\"Numerical instability detected. Terminating training.\")\n",
    "        break\n",
    "\n",
    "    # Debugging: Check mean and std of gradients\n",
    "    print(f\"  Gradient X: mean={np.mean(grad_X):.4f}, std={np.std(grad_X):.4f}\")\n",
    "    print(f\"  Gradient W: mean={np.mean(grad_W):.4f}, std={np.std(grad_W):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Loss over Epochs for SGD')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Loss Function with Alternating Least Squares (ALS)\n",
    "\n",
    "Alternating Least Squares (ALS) is an optimization method for matrix factorization that alternates between updating the user latent factors ($X$) and the item latent factors ($W$).\n",
    "\n",
    "#### Objective\n",
    "The goal is to minimize the following loss function:\n",
    "$$\n",
    "L(X, W) = \\|M - XW^T\\|_F^2 + \\lambda (\\|X\\|_F^2 + \\|W\\|_F^2)\n",
    "$$\n",
    "Where:\n",
    "- $M$: User-item interaction matrix.\n",
    "- $X$: User latent factor matrix ($m \\times k$).\n",
    "- $W$: Item latent factor matrix ($n \\times k$).\n",
    "- $\\lambda$: Regularization parameter.\n",
    "\n",
    "#### ALS Algorithm\n",
    "1. **Initialization**:\n",
    "   - Start with random values for $X$ and $W$.\n",
    "2. **Alternating Updates**:\n",
    "   - Fix $W$, solve for $X$:\n",
    "     $$\n",
    "     X_u = (W^T W + \\lambda I)^{-1} W^T M_u\n",
    "     $$\n",
    "   - Fix $X$, solve for $W$:\n",
    "     $$\n",
    "     W_i = (X^T X + \\lambda I)^{-1} X^T M_i\n",
    "     $$\n",
    "3. **Convergence**:\n",
    "   - Iterate until the loss stabilizes or a set number of epochs is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     A \u001b[38;5;241m=\u001b[39m W_rated\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m W_rated \u001b[38;5;241m+\u001b[39m reg_lambda \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(num_factors)\n\u001b[1;32m     25\u001b[0m     b \u001b[38;5;241m=\u001b[39m W_rated\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m M_rated\n\u001b[0;32m---> 26\u001b[0m     X[u, :] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Update W by fixing X\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_items), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating W\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/numpy/linalg/linalg.py:409\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    407\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 409\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Regularization parameter\n",
    "reg_lambda = 0.1\n",
    "\n",
    "# Initialize X and W with small random values\n",
    "X = np.random.normal(scale=0.01, size=(num_users, num_factors))\n",
    "W = np.random.normal(scale=0.01, size=(num_items, num_factors))\n",
    "\n",
    "# List to store the loss values for each iteration\n",
    "loss_history = []\n",
    "\n",
    "# ALS iterations\n",
    "for iteration in range(num_epochs):\n",
    "    print(f\"Iteration {iteration + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Update X by fixing W\n",
    "    for u in tqdm(range(num_users), desc=\"Updating X\", leave=False):\n",
    "        rated_items = mask[u, :]  # Mask for items rated by user u\n",
    "        W_rated = W[rated_items, :]\n",
    "        M_rated = M[u, rated_items]\n",
    "        \n",
    "        # Solve for X_u\n",
    "        A = W_rated.T @ W_rated + reg_lambda * np.eye(num_factors)\n",
    "        b = W_rated.T @ M_rated\n",
    "        X[u, :] = np.linalg.solve(A, b)\n",
    "    \n",
    "    # Update W by fixing X\n",
    "    for i in tqdm(range(num_items), desc=\"Updating W\", leave=False):\n",
    "        rated_users = mask[:, i]  # Mask for users who rated item i\n",
    "        X_rated = X[rated_users, :]\n",
    "        M_rated = M[rated_users, i]\n",
    "        \n",
    "        # Solve for W_i\n",
    "        A = X_rated.T @ X_rated + reg_lambda * np.eye(num_factors)\n",
    "        b = X_rated.T @ M_rated\n",
    "        W[i, :] = np.linalg.solve(A, b)\n",
    "    \n",
    "    # Compute the loss\n",
    "    M_hat = X @ W.T\n",
    "    reconstruction_loss = np.sum(np.multiply(mask, (M - M_hat) ** 2))\n",
    "    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n",
    "    total_loss = reconstruction_loss + regularization_loss\n",
    "    \n",
    "    # Save the loss in history\n",
    "    loss_history.append(total_loss)\n",
    "    \n",
    "    # Print loss for the current iteration\n",
    "    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n",
    "    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reconstruction Loss: 14209354.5993\n",
      "  Regularization Loss: 32646013.5623\n",
      "  Total Loss: 46855368.1616\n",
      "Iteration 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reconstruction Loss: 1930970.9705\n",
      "  Regularization Loss: 13301795.5319\n",
      "  Total Loss: 15232766.5023\n",
      "Iteration 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reconstruction Loss: 1080141.3410\n",
      "  Regularization Loss: 11073631.2436\n",
      "  Total Loss: 12153772.5846\n",
      "Iteration 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reconstruction Loss: 844501.7577\n",
      "  Regularization Loss: 9655167.7194\n",
      "  Total Loss: 10499669.4771\n",
      "Iteration 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reconstruction Loss: 718022.5975\n",
      "  Regularization Loss: 8659682.9865\n",
      "  Total Loss: 9377705.5840\n",
      "Iteration 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reconstruction Loss: 637268.5822\n",
      "  Regularization Loss: 7902342.9704\n",
      "  Total Loss: 8539611.5526\n",
      "Iteration 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating X:   7%|         | 9736/143458 [00:07<01:37, 1371.74it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Update X by fixing W (Parallelized)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_user\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_users\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUpdating X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Update W by fixing X (Parallelized)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(delayed(update_item)(i, X, M, mask, reg_lambda, num_factors)\n\u001b[1;32m     45\u001b[0m                                  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_items), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating W\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1698\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1699\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Per la barra di avanzamento\n",
    "\n",
    "# Regularization parameter\n",
    "reg_lambda = 0.1\n",
    "\n",
    "# Initialize X and W with small random values\n",
    "X = np.random.normal(scale=0.01, size=(num_users, num_factors))\n",
    "W = np.random.normal(scale=0.01, size=(num_items, num_factors))\n",
    "\n",
    "# List to store the loss values for each iteration\n",
    "loss_history = []\n",
    "\n",
    "def update_user(u, W, M, mask, reg_lambda, num_factors):\n",
    "    \"\"\"Update a single user's latent factors.\"\"\"\n",
    "    rated_items = mask[u, :]  # Mask for items rated by user u\n",
    "    W_rated = W[rated_items, :]\n",
    "    M_rated = M[u, rated_items]\n",
    "    \n",
    "    A = W_rated.T @ W_rated + reg_lambda * np.eye(num_factors)\n",
    "    b = W_rated.T @ M_rated\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def update_item(i, X, M, mask, reg_lambda, num_factors):\n",
    "    \"\"\"Update a single item's latent factors.\"\"\"\n",
    "    rated_users = mask[:, i]  # Mask for users who rated item i\n",
    "    X_rated = X[rated_users, :]\n",
    "    M_rated = M[rated_users, i]\n",
    "    \n",
    "    A = X_rated.T @ X_rated + reg_lambda * np.eye(num_factors)\n",
    "    b = X_rated.T @ M_rated\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "# ALS iterations\n",
    "for iteration in range(num_epochs):\n",
    "    print(f\"Iteration {iteration + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Update X by fixing W (Parallelized)\n",
    "    X = np.array(Parallel(n_jobs=-1)(delayed(update_user)(u, W, M, mask, reg_lambda, num_factors)\n",
    "                                     for u in tqdm(range(num_users), desc=\"Updating X\", leave=False)))\n",
    "    \n",
    "    # Update W by fixing X (Parallelized)\n",
    "    W = np.array(Parallel(n_jobs=-1)(delayed(update_item)(i, X, M, mask, reg_lambda, num_factors)\n",
    "                                     for i in tqdm(range(num_items), desc=\"Updating W\", leave=False)))\n",
    "    \n",
    "    # Compute the loss\n",
    "    M_hat = X @ W.T\n",
    "    reconstruction_loss = np.sum(np.multiply(mask, (M - M_hat) ** 2))\n",
    "    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n",
    "    total_loss = reconstruction_loss + regularization_loss\n",
    "    \n",
    "    # Save the loss in history\n",
    "    loss_history.append(total_loss)\n",
    "    \n",
    "    # Print loss for the current iteration\n",
    "    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n",
    "    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, M_hat, user_item_matrix, movies, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommends movies for a given user based on the predicted matrix M_hat.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: ID of the user to whom the recommendations will be made.\n",
    "    - M_hat: Predicted user-item matrix (num_users x num_items).\n",
    "    - user_item_matrix: Original user-item matrix (Pandas DataFrame) with ratings.\n",
    "    - movies: DataFrame containing movie details (Movie_ID, Name, Year).\n",
    "    - top_n: Number of recommendations to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - recommendations: DataFrame containing the top_n recommended movies.\n",
    "    \"\"\"\n",
    "    # Map user_id to the corresponding index in M_hat\n",
    "    user_index = user_item_matrix.index.get_loc(user_id)\n",
    "    \n",
    "    # Get the predicted ratings for the user\n",
    "    predicted_ratings = M_hat[user_index]\n",
    "\n",
    "    # Get the user's original ratings\n",
    "    original_ratings = user_item_matrix.loc[user_id]\n",
    "\n",
    "    # Find movies that the user has not rated (those with a rating of 0)\n",
    "    unrated_movies = original_ratings[original_ratings == 0].index\n",
    "\n",
    "    # Map the unrated movies to the correct columns in M_hat\n",
    "    unrated_predictions = {\n",
    "        movie_id: predicted_ratings[user_item_matrix.columns.get_loc(movie_id)]\n",
    "        for movie_id in unrated_movies\n",
    "    }\n",
    "\n",
    "    # Sort the predicted ratings for unrated movies in descending order\n",
    "    sorted_predictions = sorted(unrated_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top_n movie IDs\n",
    "    top_movie_ids = [movie_id for movie_id, _ in sorted_predictions[:top_n]]\n",
    "\n",
    "    # Retrieve movie details for the top_n recommendations\n",
    "    recommendations = movies[movies['Movie_ID'].isin(top_movie_ids)]\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid the SettingWithCopyWarning\n",
    "    recommendations = recommendations.copy()\n",
    "    \n",
    "    # Add the Predicted_Rating column\n",
    "    recommendations['Predicted_Rating'] = [unrated_predictions[movie_id] for movie_id in recommendations['Movie_ID']]\n",
    "    \n",
    "    # Sort recommendations by predicted rating (optional, for clarity)\n",
    "    recommendations = recommendations.sort_values(by='Predicted_Rating', ascending=False)\n",
    "\n",
    "    return recommendations[['Movie_ID', 'Name', 'Year', 'Predicted_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing\n",
    "The function is tested with a sample user to generate personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for User 774868:\n",
      "                                       Name  Year  Predicted_Rating  Movie_ID\n",
      "2113                                Firefly  2002          5.917396      2114\n",
      "3045      The Simpsons: Treehouse of Horror  1990          5.825943      3046\n",
      "3443  Family Guy: Freakin' Sweet Collection  2004          5.824890      3444\n",
      "4237                              Inu-Yasha  2000          5.817704      4238\n",
      "2056     Buffy the Vampire Slayer: Season 6  2001          5.816484      2057\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the recommendation function\n",
    "user_id_to_test = 774868  # Cambia con un ID utente valido nel dataset\n",
    "num_recommendations = 5   # Numero di raccomandazioni desiderate\n",
    "\n",
    "try:\n",
    "    # Esegui la funzione di raccomandazione\n",
    "    recommendations = recommend_movies(user_id_to_test, M_hat, user_item_matrix, movies, top_n=num_recommendations)\n",
    "    \n",
    "    # Mostra i risultati\n",
    "    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test}:\")\n",
    "    print(recommendations[['Name', 'Year', 'Predicted_Rating', 'Movie_ID']])\n",
    "except KeyError as e:\n",
    "    print(f\"Error: User ID {user_id_to_test} not found in the dataset.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
