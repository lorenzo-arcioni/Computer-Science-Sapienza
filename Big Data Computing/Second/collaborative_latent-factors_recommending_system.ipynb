{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Collaborative Latent-Factors-Based Filtering for Movie Recommendations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The latent factor approach in recommendation systems utilizes matrix factorization techniques to uncover hidden patterns in user-item interactions. These methods predict user preferences by mapping both users and items to a shared latent space where their interactions can be represented by their proximity or alignment. Latent factor models, such as Singular Value Decomposition (SVD), are widely used in this context.\n",
    "\n",
    "Key features of the latent factor approach:\n",
    "- Captures underlying relationships between users and items.\n",
    "- Handles sparse datasets effectively by reducing dimensionality.\n",
    "- Improves scalability compared to neighborhood-based methods.\n",
    "\n",
    "In this notebook, we will explore the latent factor approach to build a movie recommendation system using matrix factorization.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "The latent factor approach works by:\n",
    "- Represent users and items in a shared lower dimensional latent space (ie, as a vector of latent factors).\n",
    "- Such vectros are inferred (ie, learned) from the observed ratings.\n",
    "- High correlation between user and item latent factors indicates a possible recomendation.\n",
    "- Map both users and items to the latent space and then predict ratings based on the inner product in the latent space.\n",
    "\n",
    "So formally we have:\n",
    "- $\\vec x_u \\in R^d$ is the latent factor vector for user $u$. Each $\\vec x_u[k] \\in R$ measure the extent of interest user $u$ has in items exhibiting latent factor $k$.\n",
    "- $\\vec w_i \\in R^d$ is the latent factor vector for item $i$. Each $\\vec w_i[k] \\in R$ measure the extent of interest item $i$ has in users exhibiting latent factor $k$.\n",
    "\n",
    "Essentially, $d$ hidden features to describe both users and items.\n",
    "\n",
    "Thus, $r_{u,i}$ is the rating given by user $u$ to item $i$ and $\\hat{r}_{u,i} = \\vec x_u \\cdot \\vec w_i = \\sum_{k=1}^d \\vec x_u[k] \\cdot \\vec w_i[k]$ is the predicted rating for user $u$ and item $i$.\n",
    "\n",
    "The problem is to approximate the user-item matrix $R \\in \\mathbb R^{m \\times n}$ with the product of a user latent factor matrix $X \\in \\mathbb R^{m \\times d}$ and an item latent factor matrix $W^T \\in \\mathbb R^{d \\times n}$. So\n",
    "\n",
    "$$\n",
    "R \\approx X \\cdot W^T.\n",
    "$$\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "We use two datasets for this analysis:\n",
    "\n",
    "1. **Movies Dataset**:\n",
    "   - `Movie_ID`: Unique identifier for each movie.\n",
    "   - `Title`: Name of the movie.\n",
    "   - `Year`: Year the movie was released.\n",
    "\n",
    "2. **Ratings Dataset**:\n",
    "   - `User_ID`: Unique identifier for each user.\n",
    "   - `Movie_ID`: Identifier for the movie rated by the user.\n",
    "   - `Rating`: Numeric rating provided by the user (e.g., on a scale of 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the dataset\n",
    "ratings = pd.read_csv(\"./data/Netflix_Dataset_Rating.csv\")  # Columns: User_ID, Rating, Movie_ID\n",
    "movies  = pd.read_csv(\"./data/Netflix_Dataset_Movie.csv\")    # Columns: Movie_ID, Year, Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "- $U = \\{u_1, u_2, \\dots, u_n\\}$ is the set of users.\n",
    "- $U_i = \\{u \\in U \\mid r_{u,i} \\neq 0\\}$ is the set of users who have rated item $i$\n",
    "- $I = \\{i_1, i_2, \\dots, i_m\\}$ is the set of items.\n",
    "- $I_u = \\{i \\in I \\mid r_{u,i} \\neq 0\\}$ is the set of items rated by user $u$\n",
    "- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n",
    "- $r_{u,i}$ is the rating given by user $u$ for item $i$ (equal to 0 if not rated).\n",
    "- $D = \\{(u_j, i_j)\\}_{j=1}^{N}$ is the set of user-item pairs (our dataset).\n",
    "- $I_D = \\{i \\in I \\mid \\exists (u, i) \\in D\\}$ is the set of items in the dataset.\n",
    "- $U_D = \\{u \\in U \\mid \\exists (u, i) \\in D\\}$ is the set of users in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **User-Item Matrix Creation**: Convert the ratings dataset into a user-item matrix, where rows represent users and columns represent movies. Missing ratings are filled with zeros. Each rating is represented by a number from 1 to 5.\n",
    "  $$ M[u, i] = r_{u,i} \\in R$$\n",
    "  Where:\n",
    "  - $u \\in U$ is the set of users.\n",
    "  - $i \\in I$ is the set of movies.\n",
    "  - $r_{u,i}$ is the rating given by user $u$ for movie $i$.\n",
    "\n",
    "- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format for memory optimization:\n",
    "  $$M_{\\{\\text{sparse}\\}} = \\text{sparse}(M)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a user-item matrix\n",
    "user_item_matrix = ratings.pivot(index='User_ID', columns='Movie_ID', values='Rating')\n",
    "\n",
    "# Fill missing values with 0 (can use NaN for some algorithms)\n",
    "user_item_matrix.fillna(0, inplace=True) # It is not the case for this dataset\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determine the Loss Function and Computing Its Gradient\n",
    "\n",
    "Assuming we have access to the dataset $D$ of observed atings, the matrix $R$ is partially known and filled with those observations. To actully learn the latent factors, we need to choose a loss function to optimize. In our case, we choose squared error (SE):\n",
    "\n",
    "$$\n",
    "L(X, W) = \\frac{1}{2} \\left[ \\sum_{(u, i) \\in D} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda (\\sum_{u \\in U_D} \\|\\vec x_u\\|^2 + \\sum_{i \\in I_D} \\|\\vec w_i\\|^2)\\right]\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$X^*, W^* = \\argmin_{X, W} \\ L(X, W).$$\n",
    "\n",
    "#### Loss Function\n",
    "The loss function in, matrix notation, is defined in terms of matrices as:\n",
    "$$\n",
    "L(X, W) = \\frac{1}{2} \\left[ \\| R - X W^T \\|_F^2 + \\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right) \\right],\n",
    "$$\n",
    "where:\n",
    "- $R \\in \\mathbb{R}^{m \\times n}$ is the observed rating matrix, with $R_{u,i} = r_{u,i}$ if user $u$ has rated item $i$, and 0 otherwise.\n",
    "- $X \\in \\mathbb{R}^{m \\times d}$ represents the user latent factors (each row corresponds to a user vector $X_u$).\n",
    "- $W \\in \\mathbb{R}^{n \\times d}$ represents the item latent factors (each row corresponds to an item vector $W_i$).\n",
    "- $\\| \\cdot \\|_F$ is the Frobenius norm.\n",
    "\n",
    "The prediction matrix is:\n",
    "$$\n",
    "\\hat{R} = X W^T.\n",
    "$$\n",
    "\n",
    "The loss consists of:\n",
    "1. The reconstruction error:\n",
    "$$\n",
    "\\| R - X W^T \\|_F^2 = \\sum_{(u, i) \\in D} (r_{u,i} - X_u W_i^T)^2.\n",
    "$$\n",
    "2. The regularization terms:\n",
    "$$\n",
    "\\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Computing the Gradients\n",
    "\n",
    "##### Gradient with respect to $X$\n",
    "\n",
    "1. Differentiate the reconstruction error term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X} \\frac{1}{2} \\| R - X W^T \\|_F^2 = -(R - X W^T) W.\n",
    "$$\n",
    "\n",
    "2. Differentiate the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X} \\frac{\\lambda}{2} \\|X\\|_F^2 = \\lambda X.\n",
    "$$\n",
    "\n",
    "3. Combine the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = -(R - X W^T) W + \\lambda X.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Gradient with respect to $W$\n",
    "\n",
    "1. Differentiate the reconstruction error term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\frac{1}{2} \\| R - X W^T \\|_F^2 = -(R - X W^T)^T X.\n",
    "$$\n",
    "\n",
    "2. Differentiate the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\frac{\\lambda}{2} \\|W\\|_F^2 = \\lambda W.\n",
    "$$\n",
    "\n",
    "3. Combine the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = -(R - X W^T)^T X + \\lambda W.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setting up the model parameters\n",
    "\n",
    "# Hyperparameters\n",
    "num_factors = 10  # Number of latent factors (k)\n",
    "learning_rate = 0.01  # Learning rate (eta)\n",
    "reg_lambda = 0.01  # Regularization term (lambda)\n",
    "num_epochs = 200  # Number of epochs\n",
    "gradient_clip = 3.0  # Gradient clipping threshold\n",
    "\n",
    "# Dimensions of the user-item matrix\n",
    "num_users, num_items = user_item_matrix.shape\n",
    "\n",
    "# Initialize latent factor matrices X and W with small random values\n",
    "X = np.random.normal(scale=0.01, size=(num_users, num_factors))\n",
    "W = np.random.normal(scale=0.01, size=(num_items, num_factors))\n",
    "\n",
    "# Create a mask for observed entries in R\n",
    "R = user_item_matrix.values\n",
    "mask = R > 0  # Boolean mask for observed entries\n",
    "\n",
    "# List to store loss values for plotting\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimize the Loss Function with Stochastic Gradient Descent\n",
    "\n",
    "In order to optimize the loss function, we use Stochastic Gradient Descent (SGD).\n",
    "\n",
    "#### Explanation of the SGD Algorithm (Matrix Form)\n",
    "\n",
    "##### 1. Initialization\n",
    "- Matrices $X$ (users' latent factors) and $W$ (items' latent factors) are initialized randomly with small values.\n",
    "- $X \\in \\mathbb{R}^{m \\times d}$, where $m$ is the number of users and $d$ is the number of latent factors.\n",
    "- $W \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of items.\n",
    "\n",
    "##### 2. Gradient Computation\n",
    "- Define the prediction matrix:\n",
    "  $$\n",
    "  \\hat{R} = X W^T\n",
    "  $$\n",
    "- Compute the error matrix (only for observed entries in $R$):\n",
    "  $$\n",
    "  E = \\begin{cases}\n",
    "  R_{ui} > 0 \\quad R_{ui} - \\hat{R}_{ui}\\\\\n",
    "  0 \\quad \\text{otherwise}\n",
    "  \\end{cases},\n",
    "  $$\n",
    "  where $E_{ui} = 0$ for unobserved entries of $R$.\n",
    "\n",
    "- Gradients for $X$ and $W$:\n",
    "  $$\n",
    "  \\nabla_X = - E W + \\lambda X\n",
    "  $$\n",
    "  $$\n",
    "  \\nabla_W = - E^T X + \\lambda W\n",
    "  $$\n",
    "\n",
    "##### 3. Updates\n",
    "- Update the latent factor matrices $X$ and $W$ simultaneously:\n",
    "  $$\n",
    "  X \\leftarrow X - \\eta \\nabla_X\n",
    "  $$\n",
    "  $$\n",
    "  W \\leftarrow W - \\eta \\nabla_W\n",
    "  $$\n",
    "- Here, $\\eta$ is the learning rate.\n",
    "\n",
    "##### 4. Loss Tracking\n",
    "- The total loss for each epoch combines the squared error and the regularization terms:\n",
    "  $$\n",
    "  L = \\| R - X W^T \\|_F^2 + \\lambda (\\|X\\|_F^2 + \\|W\\|_F^2)\n",
    "  $$\n",
    "- This tracks the reconstruction error and ensures that the latent factor matrices do not grow too large (controlled by the regularization term).\n",
    "\n",
    "##### 5. Optimization Loop\n",
    "- Repeat the following steps for a fixed number of epochs or until the loss converges:\n",
    "  1. Compute the error matrix $E$.\n",
    "  2. Compute the gradients $\\nabla_X$ and $\\nabla_W$ using matrix operations.\n",
    "  3. Update $X$ and $W$ using the gradients.\n",
    "  4. Track and print the loss for each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "##### Notes\n",
    "- This implementation only updates $X$ and $W$ for the observed entries of $R$ using matrix masking.\n",
    "- The hyperparameters ($\\eta$, $d$, and $\\lambda$) should be tuned based on the dataset for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  Reconstruction Loss: 243096275.7051\n",
      "  Regularization Loss: 1.8235\n",
      "  Total Loss: 243096277.5286\n",
      "  Gradient X: mean=0.1942, std=0.4241\n",
      "  Gradient W: mean=0.2380, std=2.0803\n",
      "Epoch 2/200\n",
      "  Reconstruction Loss: 243052708.7914\n",
      "  Regularization Loss: 13.0051\n",
      "  Total Loss: 243052721.7965\n",
      "  Gradient X: mean=0.4536, std=2.5594\n",
      "  Gradient W: mean=1.7725, std=2.4045\n",
      "Epoch 3/200\n",
      "  Reconstruction Loss: 241879541.8932\n",
      "  Regularization Loss: 42.3811\n",
      "  Total Loss: 241879584.2743\n",
      "  Gradient X: mean=1.7978, std=2.3992\n",
      "  Gradient W: mean=0.9747, std=2.8355\n",
      "Epoch 4/200\n",
      "  Reconstruction Loss: 238698839.8264\n",
      "  Regularization Loss: 96.5574\n",
      "  Total Loss: 238698936.3839\n",
      "  Gradient X: mean=1.2099, std=2.7012\n",
      "  Gradient W: mean=1.7995, std=2.4000\n",
      "Epoch 5/200\n",
      "  Reconstruction Loss: 233692218.0405\n",
      "  Regularization Loss: 172.9153\n",
      "  Total Loss: 233692390.9558\n",
      "  Gradient X: mean=1.7980, std=2.3993\n",
      "  Gradient W: mean=1.2000, std=2.7495\n",
      "Epoch 6/200\n",
      "  Reconstruction Loss: 226700038.2517\n",
      "  Regularization Loss: 273.8121\n",
      "  Total Loss: 226700312.0638\n",
      "  Gradient X: mean=1.2366, std=2.7022\n",
      "  Gradient W: mean=1.7991, std=2.4007\n",
      "Epoch 7/200\n",
      "  Reconstruction Loss: 218007423.7581\n",
      "  Regularization Loss: 397.3865\n",
      "  Total Loss: 218007821.1446\n",
      "  Gradient X: mean=1.7974, std=2.3992\n",
      "  Gradient W: mean=1.2000, std=2.7495\n",
      "Epoch 8/200\n",
      "  Reconstruction Loss: 207567483.3893\n",
      "  Regularization Loss: 545.0475\n",
      "  Total Loss: 207568028.4368\n",
      "  Gradient X: mean=1.2404, std=2.6973\n",
      "  Gradient W: mean=1.7991, std=2.4007\n",
      "Epoch 9/200\n",
      "  Reconstruction Loss: 195684486.9526\n",
      "  Regularization Loss: 715.8643\n",
      "  Total Loss: 195685202.8168\n",
      "  Gradient X: mean=1.7963, std=2.3989\n",
      "  Gradient W: mean=1.2000, std=2.7495\n",
      "Epoch 10/200\n",
      "  Reconstruction Loss: 182402973.8569\n",
      "  Regularization Loss: 910.2876\n",
      "  Total Loss: 182403884.1445\n",
      "  Gradient X: mean=1.2464, std=2.6896\n",
      "  Gradient W: mean=1.7991, std=2.4007\n",
      "Epoch 11/200\n",
      "  Reconstruction Loss: 168043642.4191\n",
      "  Regularization Loss: 1128.3817\n",
      "  Total Loss: 168044770.8009\n",
      "  Gradient X: mean=1.7941, std=2.3983\n",
      "  Gradient W: mean=1.2013, std=2.7490\n",
      "Epoch 12/200\n",
      "  Reconstruction Loss: 152744845.7632\n",
      "  Regularization Loss: 1369.5724\n",
      "  Total Loss: 152746215.3356\n",
      "  Gradient X: mean=1.2563, std=2.6772\n",
      "  Gradient W: mean=1.7987, std=2.4007\n",
      "Epoch 13/200\n",
      "  Reconstruction Loss: 136841887.3133\n",
      "  Regularization Loss: 1634.9842\n",
      "  Total Loss: 136843522.2975\n",
      "  Gradient X: mean=1.7896, std=2.3971\n",
      "  Gradient W: mean=1.2076, std=2.7462\n",
      "Epoch 14/200\n",
      "  Reconstruction Loss: 120567330.9228\n",
      "  Regularization Loss: 1923.0031\n",
      "  Total Loss: 120569253.9259\n",
      "  Gradient X: mean=1.2775, std=2.6518\n",
      "  Gradient W: mean=1.7982, std=2.4013\n",
      "Epoch 15/200\n",
      "  Reconstruction Loss: 104271818.3275\n",
      "  Regularization Loss: 2235.7926\n",
      "  Total Loss: 104274054.1201\n",
      "  Gradient X: mean=1.7806, std=2.3951\n",
      "  Gradient W: mean=1.2421, std=2.7298\n",
      "Epoch 16/200\n",
      "  Reconstruction Loss: 88270932.5288\n",
      "  Regularization Loss: 2571.5232\n",
      "  Total Loss: 88273504.0521\n",
      "  Gradient X: mean=1.4114, std=2.5315\n",
      "  Gradient W: mean=1.7978, std=2.4017\n",
      "Epoch 17/200\n",
      "  Reconstruction Loss: 72938010.4448\n",
      "  Regularization Loss: 2932.7154\n",
      "  Total Loss: 72940943.1603\n",
      "  Gradient X: mean=1.7768, std=2.3955\n",
      "  Gradient W: mean=1.5220, std=2.5820\n",
      "Epoch 18/200\n",
      "  Reconstruction Loss: 58628400.8142\n",
      "  Regularization Loss: 3319.3761\n",
      "  Total Loss: 58631720.1903\n",
      "  Gradient X: mean=1.7727, std=2.4000\n",
      "  Gradient W: mean=1.7951, std=2.4037\n",
      "Epoch 19/200\n",
      "  Reconstruction Loss: 45773920.3449\n",
      "  Regularization Loss: 3730.6678\n",
      "  Total Loss: 45777651.0126\n",
      "  Gradient X: mean=1.7849, std=2.4036\n",
      "  Gradient W: mean=1.7767, std=2.4171\n",
      "Epoch 20/200\n",
      "  Reconstruction Loss: 34859172.9967\n",
      "  Regularization Loss: 4161.4954\n",
      "  Total Loss: 34863334.4921\n",
      "  Gradient X: mean=1.7577, std=2.4175\n",
      "  Gradient W: mean=1.7165, std=2.4599\n",
      "Epoch 21/200\n",
      "  Reconstruction Loss: 26373624.8467\n",
      "  Regularization Loss: 4586.8746\n",
      "  Total Loss: 26378211.7213\n",
      "  Gradient X: mean=1.6299, std=2.4773\n",
      "  Gradient W: mean=1.4792, std=2.6086\n",
      "Epoch 22/200\n",
      "  Reconstruction Loss: 20623702.1089\n",
      "  Regularization Loss: 4919.4747\n",
      "  Total Loss: 20628621.5836\n",
      "  Gradient X: mean=1.1850, std=2.6487\n",
      "  Gradient W: mean=0.9548, std=2.8383\n",
      "Epoch 23/200\n",
      "  Reconstruction Loss: 17316119.2016\n",
      "  Regularization Loss: 5056.3007\n",
      "  Total Loss: 17321175.5023\n",
      "  Gradient X: mean=0.4217, std=2.7536\n",
      "  Gradient W: mean=0.5425, std=2.9443\n",
      "Epoch 24/200\n",
      "  Reconstruction Loss: 15627025.4855\n",
      "  Regularization Loss: 5127.2791\n",
      "  Total Loss: 15632152.7646\n",
      "  Gradient X: mean=0.2033, std=2.5776\n",
      "  Gradient W: mean=0.4170, std=2.9650\n",
      "Epoch 25/200\n",
      "  Reconstruction Loss: 14894140.5482\n",
      "  Regularization Loss: 5137.1574\n",
      "  Total Loss: 14899277.7055\n",
      "  Gradient X: mean=-0.0092, std=2.5129\n",
      "  Gradient W: mean=0.1234, std=2.9887\n",
      "Epoch 26/200\n",
      "  Reconstruction Loss: 14571713.0738\n",
      "  Regularization Loss: 5174.3060\n",
      "  Total Loss: 14576887.3798\n",
      "  Gradient X: mean=0.0933, std=2.5247\n",
      "  Gradient W: mean=0.1510, std=2.9857\n",
      "Epoch 27/200\n",
      "  Reconstruction Loss: 14477883.3440\n",
      "  Regularization Loss: 5134.5260\n",
      "  Total Loss: 14483017.8700\n",
      "  Gradient X: mean=-0.1746, std=2.6112\n",
      "  Gradient W: mean=-0.1101, std=2.9914\n",
      "Epoch 28/200\n",
      "  Reconstruction Loss: 14434866.8703\n",
      "  Regularization Loss: 5193.2770\n",
      "  Total Loss: 14440060.1473\n",
      "  Gradient X: mean=0.1688, std=2.7131\n",
      "  Gradient W: mean=0.0747, std=2.9928\n",
      "Epoch 29/200\n",
      "  Reconstruction Loss: 14429980.7413\n",
      "  Regularization Loss: 5139.4860\n",
      "  Total Loss: 14435120.2273\n",
      "  Gradient X: mean=-0.2095, std=2.7761\n",
      "  Gradient W: mean=-0.1702, std=2.9919\n",
      "Epoch 30/200\n",
      "  Reconstruction Loss: 14402620.3256\n",
      "  Regularization Loss: 5203.4821\n",
      "  Total Loss: 14407823.8077\n",
      "  Gradient X: mean=0.1856, std=2.8071\n",
      "  Gradient W: mean=0.0739, std=2.9956\n",
      "Epoch 31/200\n",
      "  Reconstruction Loss: 14396360.4062\n",
      "  Regularization Loss: 5142.7182\n",
      "  Total Loss: 14401503.1244\n",
      "  Gradient X: mean=-0.2343, std=2.8255\n",
      "  Gradient W: mean=-0.1827, std=2.9904\n",
      "Epoch 32/200\n",
      "  Reconstruction Loss: 14362164.3643\n",
      "  Regularization Loss: 5211.2845\n",
      "  Total Loss: 14367375.6488\n",
      "  Gradient X: mean=0.1913, std=2.8433\n",
      "  Gradient W: mean=0.0751, std=2.9961\n",
      "Epoch 33/200\n",
      "  Reconstruction Loss: 14343681.0020\n",
      "  Regularization Loss: 5149.9846\n",
      "  Total Loss: 14348830.9866\n",
      "  Gradient X: mean=-0.2433, std=2.8489\n",
      "  Gradient W: mean=-0.1870, std=2.9914\n",
      "Epoch 34/200\n",
      "  Reconstruction Loss: 14301090.0082\n",
      "  Regularization Loss: 5221.3451\n",
      "  Total Loss: 14306311.3533\n",
      "  Gradient X: mean=0.1865, std=2.8619\n",
      "  Gradient W: mean=0.0745, std=2.9959\n",
      "Epoch 35/200\n",
      "  Reconstruction Loss: 14270614.1188\n",
      "  Regularization Loss: 5160.7834\n",
      "  Total Loss: 14275774.9022\n",
      "  Gradient X: mean=-0.2517, std=2.8613\n",
      "  Gradient W: mean=-0.1856, std=2.9903\n",
      "Epoch 36/200\n",
      "  Reconstruction Loss: 14215477.5076\n",
      "  Regularization Loss: 5233.5368\n",
      "  Total Loss: 14220711.0444\n",
      "  Gradient X: mean=0.1758, std=2.8702\n",
      "  Gradient W: mean=0.0618, std=2.9955\n",
      "Epoch 37/200\n",
      "  Reconstruction Loss: 14171631.4774\n",
      "  Regularization Loss: 5174.5429\n",
      "  Total Loss: 14176806.0203\n",
      "  Gradient X: mean=-0.2593, std=2.8664\n",
      "  Gradient W: mean=-0.1894, std=2.9897\n",
      "Epoch 38/200\n",
      "  Reconstruction Loss: 14103877.1727\n",
      "  Regularization Loss: 5249.9662\n",
      "  Total Loss: 14109127.1389\n",
      "  Gradient X: mean=0.1709, std=2.8756\n",
      "  Gradient W: mean=0.0599, std=2.9957\n",
      "Epoch 39/200\n",
      "  Reconstruction Loss: 14056046.3095\n",
      "  Regularization Loss: 5191.1981\n",
      "  Total Loss: 14061237.5075\n",
      "  Gradient X: mean=-0.2721, std=2.8666\n",
      "  Gradient W: mean=-0.2096, std=2.9895\n",
      "Epoch 40/200\n",
      "  Reconstruction Loss: 13973884.9540\n",
      "  Regularization Loss: 5266.6722\n",
      "  Total Loss: 13979151.6262\n",
      "  Gradient X: mean=0.1580, std=2.8711\n",
      "  Gradient W: mean=0.0583, std=2.9928\n",
      "Epoch 41/200\n",
      "  Reconstruction Loss: 13919493.8071\n",
      "  Regularization Loss: 5210.8802\n",
      "  Total Loss: 13924704.6873\n",
      "  Gradient X: mean=-0.2705, std=2.8665\n",
      "  Gradient W: mean=-0.2181, std=2.9863\n",
      "Epoch 42/200\n",
      "  Reconstruction Loss: 13840191.4465\n",
      "  Regularization Loss: 5288.8027\n",
      "  Total Loss: 13845480.2492\n",
      "  Gradient X: mean=0.1555, std=2.8735\n",
      "  Gradient W: mean=0.0456, std=2.9955\n",
      "Epoch 43/200\n",
      "  Reconstruction Loss: 13787271.6349\n",
      "  Regularization Loss: 5232.9685\n",
      "  Total Loss: 13792504.6034\n",
      "  Gradient X: mean=-0.2806, std=2.8633\n",
      "  Gradient W: mean=-0.2086, std=2.9874\n",
      "Epoch 44/200\n",
      "  Reconstruction Loss: 13714462.0854\n",
      "  Regularization Loss: 5309.3431\n",
      "  Total Loss: 13719771.4284\n",
      "  Gradient X: mean=0.1414, std=2.8670\n",
      "  Gradient W: mean=0.0318, std=2.9937\n",
      "Epoch 45/200\n",
      "  Reconstruction Loss: 13661098.1466\n",
      "  Regularization Loss: 5254.8490\n",
      "  Total Loss: 13666352.9957\n",
      "  Gradient X: mean=-0.2831, std=2.8629\n",
      "  Gradient W: mean=-0.2109, std=2.9870\n",
      "Epoch 46/200\n",
      "  Reconstruction Loss: 13594195.5053\n",
      "  Regularization Loss: 5332.6856\n",
      "  Total Loss: 13599528.1909\n",
      "  Gradient X: mean=0.1440, std=2.8685\n",
      "  Gradient W: mean=0.0455, std=2.9930\n",
      "Epoch 47/200\n",
      "  Reconstruction Loss: 13552208.8056\n",
      "  Regularization Loss: 5276.7204\n",
      "  Total Loss: 13557485.5260\n",
      "  Gradient X: mean=-0.2882, std=2.8566\n",
      "  Gradient W: mean=-0.2080, std=2.9870\n",
      "Epoch 48/200\n",
      "  Reconstruction Loss: 13497260.8386\n",
      "  Regularization Loss: 5352.5310\n",
      "  Total Loss: 13502613.3696\n",
      "  Gradient X: mean=0.1409, std=2.8635\n",
      "  Gradient W: mean=0.0408, std=2.9930\n",
      "Epoch 49/200\n",
      "  Reconstruction Loss: 13456044.5120\n",
      "  Regularization Loss: 5297.6031\n",
      "  Total Loss: 13461342.1151\n",
      "  Gradient X: mean=-0.2811, std=2.8576\n",
      "  Gradient W: mean=-0.1918, std=2.9892\n",
      "Epoch 50/200\n",
      "  Reconstruction Loss: 13418772.0710\n",
      "  Regularization Loss: 5371.6457\n",
      "  Total Loss: 13424143.7167\n",
      "  Gradient X: mean=0.1470, std=2.8621\n",
      "  Gradient W: mean=0.0517, std=2.9934\n",
      "Epoch 51/200\n",
      "  Reconstruction Loss: 13385476.8954\n",
      "  Regularization Loss: 5313.7725\n",
      "  Total Loss: 13390790.6678\n",
      "  Gradient X: mean=-0.2815, std=2.8547\n",
      "  Gradient W: mean=-0.1862, std=2.9886\n",
      "Epoch 52/200\n",
      "  Reconstruction Loss: 13356403.2526\n",
      "  Regularization Loss: 5386.7867\n",
      "  Total Loss: 13361790.0393\n",
      "  Gradient X: mean=0.1546, std=2.8617\n",
      "  Gradient W: mean=0.0711, std=2.9922\n",
      "Epoch 53/200\n",
      "  Reconstruction Loss: 13330440.2970\n",
      "  Regularization Loss: 5327.6124\n",
      "  Total Loss: 13335767.9094\n",
      "  Gradient X: mean=-0.2714, std=2.8555\n",
      "  Gradient W: mean=-0.1811, std=2.9889\n",
      "Epoch 54/200\n",
      "  Reconstruction Loss: 13305376.1732\n",
      "  Regularization Loss: 5398.4819\n",
      "  Total Loss: 13310774.6551\n",
      "  Gradient X: mean=0.1528, std=2.8582\n",
      "  Gradient W: mean=0.0737, std=2.9922\n",
      "Epoch 55/200\n",
      "  Reconstruction Loss: 13285708.5598\n",
      "  Regularization Loss: 5341.1589\n",
      "  Total Loss: 13291049.7187\n",
      "  Gradient X: mean=-0.2557, std=2.8562\n",
      "  Gradient W: mean=-0.1613, std=2.9901\n",
      "Epoch 56/200\n",
      "  Reconstruction Loss: 13268485.3020\n",
      "  Regularization Loss: 5412.1094\n",
      "  Total Loss: 13273897.4114\n",
      "  Gradient X: mean=0.1666, std=2.8616\n",
      "  Gradient W: mean=0.0877, std=2.9927\n",
      "Epoch 57/200\n",
      "  Reconstruction Loss: 13251473.9285\n",
      "  Regularization Loss: 5351.9698\n",
      "  Total Loss: 13256825.8983\n",
      "  Gradient X: mean=-0.2585, std=2.8535\n",
      "  Gradient W: mean=-0.1643, std=2.9898\n",
      "Epoch 58/200\n",
      "  Reconstruction Loss: 13240314.4168\n",
      "  Regularization Loss: 5422.6224\n",
      "  Total Loss: 13245737.0392\n",
      "  Gradient X: mean=0.1770, std=2.8598\n",
      "  Gradient W: mean=0.1162, std=2.9931\n",
      "Epoch 59/200\n",
      "  Reconstruction Loss: 13228304.5603\n",
      "  Regularization Loss: 5361.1775\n",
      "  Total Loss: 13233665.7378\n",
      "  Gradient X: mean=-0.2445, std=2.8581\n",
      "  Gradient W: mean=-0.1545, std=2.9916\n",
      "Epoch 60/200\n",
      "  Reconstruction Loss: 13219278.9779\n",
      "  Regularization Loss: 5431.5679\n",
      "  Total Loss: 13224710.5458\n",
      "  Gradient X: mean=0.1853, std=2.8610\n",
      "  Gradient W: mean=0.1218, std=2.9944\n",
      "Epoch 61/200\n",
      "  Reconstruction Loss: 13206791.1802\n",
      "  Regularization Loss: 5371.2667\n",
      "  Total Loss: 13212162.4469\n",
      "  Gradient X: mean=-0.2426, std=2.8592\n",
      "  Gradient W: mean=-0.1527, std=2.9930\n",
      "Epoch 62/200\n",
      "  Reconstruction Loss: 13203801.7443\n",
      "  Regularization Loss: 5442.2376\n",
      "  Total Loss: 13209243.9820\n",
      "  Gradient X: mean=0.1856, std=2.8619\n",
      "  Gradient W: mean=0.1264, std=2.9918\n",
      "Epoch 63/200\n",
      "  Reconstruction Loss: 13193002.8334\n",
      "  Regularization Loss: 5378.7829\n",
      "  Total Loss: 13198381.6163\n",
      "  Gradient X: mean=-0.2345, std=2.8599\n",
      "  Gradient W: mean=-0.1542, std=2.9915\n",
      "Epoch 64/200\n",
      "  Reconstruction Loss: 13191244.5473\n",
      "  Regularization Loss: 5448.6073\n",
      "  Total Loss: 13196693.1546\n",
      "  Gradient X: mean=0.1921, std=2.8624\n",
      "  Gradient W: mean=0.1349, std=2.9934\n",
      "Epoch 65/200\n",
      "  Reconstruction Loss: 13181529.1932\n",
      "  Regularization Loss: 5384.1365\n",
      "  Total Loss: 13186913.3297\n",
      "  Gradient X: mean=-0.2291, std=2.8624\n",
      "  Gradient W: mean=-0.1572, std=2.9924\n",
      "Epoch 66/200\n",
      "  Reconstruction Loss: 13179666.6784\n",
      "  Regularization Loss: 5454.5021\n",
      "  Total Loss: 13185121.1805\n",
      "  Gradient X: mean=0.1923, std=2.8653\n",
      "  Gradient W: mean=0.1418, std=2.9937\n",
      "Epoch 67/200\n",
      "  Reconstruction Loss: 13175454.0857\n",
      "  Regularization Loss: 5388.6783\n",
      "  Total Loss: 13180842.7640\n",
      "  Gradient X: mean=-0.2194, std=2.8619\n",
      "  Gradient W: mean=-0.1555, std=2.9941\n",
      "Epoch 68/200\n",
      "  Reconstruction Loss: 13172855.3783\n",
      "  Regularization Loss: 5458.7640\n",
      "  Total Loss: 13178314.1423\n",
      "  Gradient X: mean=0.1964, std=2.8661\n",
      "  Gradient W: mean=0.1472, std=2.9944\n",
      "Epoch 69/200\n",
      "  Reconstruction Loss: 13170789.1810\n",
      "  Regularization Loss: 5392.3774\n",
      "  Total Loss: 13176181.5584\n",
      "  Gradient X: mean=-0.2148, std=2.8667\n",
      "  Gradient W: mean=-0.1544, std=2.9942\n",
      "Epoch 70/200\n",
      "  Reconstruction Loss: 13168870.5519\n",
      "  Regularization Loss: 5462.5121\n",
      "  Total Loss: 13174333.0640\n",
      "  Gradient X: mean=0.2003, std=2.8688\n",
      "  Gradient W: mean=0.1481, std=2.9950\n",
      "Epoch 71/200\n",
      "  Reconstruction Loss: 13167404.2804\n",
      "  Regularization Loss: 5395.5595\n",
      "  Total Loss: 13172799.8399\n",
      "  Gradient X: mean=-0.2122, std=2.8678\n",
      "  Gradient W: mean=-0.1519, std=2.9947\n",
      "Epoch 72/200\n",
      "  Reconstruction Loss: 13165491.8064\n",
      "  Regularization Loss: 5465.6902\n",
      "  Total Loss: 13170957.4966\n",
      "  Gradient X: mean=0.2015, std=2.8705\n",
      "  Gradient W: mean=0.1480, std=2.9951\n",
      "Epoch 73/200\n",
      "  Reconstruction Loss: 13164913.1972\n",
      "  Regularization Loss: 5398.3656\n",
      "  Total Loss: 13170311.5628\n",
      "  Gradient X: mean=-0.2106, std=2.8708\n",
      "  Gradient W: mean=-0.1523, std=2.9953\n",
      "Epoch 74/200\n",
      "  Reconstruction Loss: 13163459.3012\n",
      "  Regularization Loss: 5468.3146\n",
      "  Total Loss: 13168927.6158\n",
      "  Gradient X: mean=0.2040, std=2.8717\n",
      "  Gradient W: mean=0.1481, std=2.9948\n",
      "Epoch 75/200\n",
      "  Reconstruction Loss: 13162846.9715\n",
      "  Regularization Loss: 5400.7306\n",
      "  Total Loss: 13168247.7021\n",
      "  Gradient X: mean=-0.2098, std=2.8720\n",
      "  Gradient W: mean=-0.1507, std=2.9953\n",
      "Epoch 76/200\n",
      "  Reconstruction Loss: 13162319.5678\n",
      "  Regularization Loss: 5470.8521\n",
      "  Total Loss: 13167790.4199\n",
      "  Gradient X: mean=0.2045, std=2.8737\n",
      "  Gradient W: mean=0.1505, std=2.9954\n",
      "Epoch 77/200\n",
      "  Reconstruction Loss: 13161869.4721\n",
      "  Regularization Loss: 5402.9146\n",
      "  Total Loss: 13167272.3867\n",
      "  Gradient X: mean=-0.2095, std=2.8741\n",
      "  Gradient W: mean=-0.1522, std=2.9952\n",
      "Epoch 78/200\n",
      "  Reconstruction Loss: 13161267.7986\n",
      "  Regularization Loss: 5473.0267\n",
      "  Total Loss: 13166740.8252\n",
      "  Gradient X: mean=0.2053, std=2.8751\n",
      "  Gradient W: mean=0.1512, std=2.9957\n",
      "Epoch 79/200\n",
      "  Reconstruction Loss: 13161042.1788\n",
      "  Regularization Loss: 5404.8355\n",
      "  Total Loss: 13166447.0144\n",
      "  Gradient X: mean=-0.2094, std=2.8754\n",
      "  Gradient W: mean=-0.1512, std=2.9959\n",
      "Epoch 80/200\n",
      "  Reconstruction Loss: 13160592.7189\n",
      "  Regularization Loss: 5474.8620\n",
      "  Total Loss: 13166067.5810\n",
      "  Gradient X: mean=0.2058, std=2.8765\n",
      "  Gradient W: mean=0.1505, std=2.9955\n",
      "Epoch 81/200\n",
      "  Reconstruction Loss: 13160334.7666\n",
      "  Regularization Loss: 5406.5294\n",
      "  Total Loss: 13165741.2960\n",
      "  Gradient X: mean=-0.2090, std=2.8766\n",
      "  Gradient W: mean=-0.1512, std=2.9958\n",
      "Epoch 82/200\n",
      "  Reconstruction Loss: 13160047.0834\n",
      "  Regularization Loss: 5476.5028\n",
      "  Total Loss: 13165523.5861\n",
      "  Gradient X: mean=0.2061, std=2.8774\n",
      "  Gradient W: mean=0.1509, std=2.9958\n",
      "Epoch 83/200\n",
      "  Reconstruction Loss: 13159835.0953\n",
      "  Regularization Loss: 5408.0490\n",
      "  Total Loss: 13165243.1443\n",
      "  Gradient X: mean=-0.2087, std=2.8775\n",
      "  Gradient W: mean=-0.1508, std=2.9960\n",
      "Epoch 84/200\n",
      "  Reconstruction Loss: 13158915.4445\n",
      "  Regularization Loss: 5477.9157\n",
      "  Total Loss: 13164393.3602\n",
      "  Gradient X: mean=0.2061, std=2.8780\n",
      "  Gradient W: mean=0.1508, std=2.9958\n",
      "Epoch 85/200\n",
      "  Reconstruction Loss: 13159431.8978\n",
      "  Regularization Loss: 5409.3625\n",
      "  Total Loss: 13164841.2603\n",
      "  Gradient X: mean=-0.2088, std=2.8780\n",
      "  Gradient W: mean=-0.1508, std=2.9959\n",
      "Epoch 86/200\n",
      "  Reconstruction Loss: 13158511.0677\n",
      "  Regularization Loss: 5479.2414\n",
      "  Total Loss: 13163990.3091\n",
      "  Gradient X: mean=0.2064, std=2.8787\n",
      "  Gradient W: mean=0.1503, std=2.9958\n",
      "Epoch 87/200\n",
      "  Reconstruction Loss: 13158822.7100\n",
      "  Regularization Loss: 5410.5922\n",
      "  Total Loss: 13164233.3022\n",
      "  Gradient X: mean=-0.2085, std=2.8787\n",
      "  Gradient W: mean=-0.1507, std=2.9960\n",
      "Epoch 88/200\n",
      "  Reconstruction Loss: 13158163.0139\n",
      "  Regularization Loss: 5480.4354\n",
      "  Total Loss: 13163643.4492\n",
      "  Gradient X: mean=0.2067, std=2.8792\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 89/200\n",
      "  Reconstruction Loss: 13158488.9324\n",
      "  Regularization Loss: 5411.7485\n",
      "  Total Loss: 13163900.6809\n",
      "  Gradient X: mean=-0.2083, std=2.8793\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 90/200\n",
      "  Reconstruction Loss: 13157857.0076\n",
      "  Regularization Loss: 5481.5692\n",
      "  Total Loss: 13163338.5767\n",
      "  Gradient X: mean=0.2069, std=2.8797\n",
      "  Gradient W: mean=0.1506, std=2.9960\n",
      "Epoch 91/200\n",
      "  Reconstruction Loss: 13158188.4926\n",
      "  Regularization Loss: 5412.8495\n",
      "  Total Loss: 13163601.3421\n",
      "  Gradient X: mean=-0.2081, std=2.8797\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 92/200\n",
      "  Reconstruction Loss: 13157584.7697\n",
      "  Regularization Loss: 5482.6447\n",
      "  Total Loss: 13163067.4144\n",
      "  Gradient X: mean=0.2070, std=2.8800\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 93/200\n",
      "  Reconstruction Loss: 13157912.4968\n",
      "  Regularization Loss: 5413.8994\n",
      "  Total Loss: 13163326.3962\n",
      "  Gradient X: mean=-0.2080, std=2.8802\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 94/200\n",
      "  Reconstruction Loss: 13157312.0937\n",
      "  Regularization Loss: 5483.6914\n",
      "  Total Loss: 13162795.7851\n",
      "  Gradient X: mean=0.2072, std=2.8804\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 95/200\n",
      "  Reconstruction Loss: 13157655.8707\n",
      "  Regularization Loss: 5414.9282\n",
      "  Total Loss: 13163070.7989\n",
      "  Gradient X: mean=-0.2079, std=2.8805\n",
      "  Gradient W: mean=-0.1506, std=2.9961\n",
      "Epoch 96/200\n",
      "  Reconstruction Loss: 13157071.1864\n",
      "  Regularization Loss: 5484.7041\n",
      "  Total Loss: 13162555.8905\n",
      "  Gradient X: mean=0.2073, std=2.8807\n",
      "  Gradient W: mean=0.1506, std=2.9960\n",
      "Epoch 97/200\n",
      "  Reconstruction Loss: 13157411.9657\n",
      "  Regularization Loss: 5415.9205\n",
      "  Total Loss: 13162827.8862\n",
      "  Gradient X: mean=-0.2078, std=2.8808\n",
      "  Gradient W: mean=-0.1508, std=2.9960\n",
      "Epoch 98/200\n",
      "  Reconstruction Loss: 13156838.7627\n",
      "  Regularization Loss: 5485.6932\n",
      "  Total Loss: 13162324.4559\n",
      "  Gradient X: mean=0.2074, std=2.8809\n",
      "  Gradient W: mean=0.1507, std=2.9961\n",
      "Epoch 99/200\n",
      "  Reconstruction Loss: 13157183.3641\n",
      "  Regularization Loss: 5416.8958\n",
      "  Total Loss: 13162600.2599\n",
      "  Gradient X: mean=-0.2078, std=2.8810\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 100/200\n",
      "  Reconstruction Loss: 13156618.6379\n",
      "  Regularization Loss: 5486.6634\n",
      "  Total Loss: 13162105.3013\n",
      "  Gradient X: mean=0.2074, std=2.8811\n",
      "  Gradient W: mean=0.1503, std=2.9959\n",
      "Epoch 101/200\n",
      "  Reconstruction Loss: 13156821.8042\n",
      "  Regularization Loss: 5417.8690\n",
      "  Total Loss: 13162239.6732\n",
      "  Gradient X: mean=-0.2076, std=2.8812\n",
      "  Gradient W: mean=-0.1508, std=2.9959\n",
      "Epoch 102/200\n",
      "  Reconstruction Loss: 13156376.8145\n",
      "  Regularization Loss: 5487.6471\n",
      "  Total Loss: 13161864.4616\n",
      "  Gradient X: mean=0.2075, std=2.8812\n",
      "  Gradient W: mean=0.1503, std=2.9960\n",
      "Epoch 103/200\n",
      "  Reconstruction Loss: 13156584.7587\n",
      "  Regularization Loss: 5418.8383\n",
      "  Total Loss: 13162003.5970\n",
      "  Gradient X: mean=-0.2076, std=2.8813\n",
      "  Gradient W: mean=-0.1511, std=2.9960\n",
      "Epoch 104/200\n",
      "  Reconstruction Loss: 13156158.4402\n",
      "  Regularization Loss: 5488.6185\n",
      "  Total Loss: 13161647.0587\n",
      "  Gradient X: mean=0.2076, std=2.8814\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 105/200\n",
      "  Reconstruction Loss: 13156371.9405\n",
      "  Regularization Loss: 5419.8039\n",
      "  Total Loss: 13161791.7444\n",
      "  Gradient X: mean=-0.2077, std=2.8815\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 106/200\n",
      "  Reconstruction Loss: 13155957.9156\n",
      "  Regularization Loss: 5489.5855\n",
      "  Total Loss: 13161447.5011\n",
      "  Gradient X: mean=0.2076, std=2.8815\n",
      "  Gradient W: mean=0.1506, std=2.9960\n",
      "Epoch 107/200\n",
      "  Reconstruction Loss: 13156160.4662\n",
      "  Regularization Loss: 5420.7644\n",
      "  Total Loss: 13161581.2305\n",
      "  Gradient X: mean=-0.2076, std=2.8816\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 108/200\n",
      "  Reconstruction Loss: 13155765.1355\n",
      "  Regularization Loss: 5490.5523\n",
      "  Total Loss: 13161255.6878\n",
      "  Gradient X: mean=0.2076, std=2.8817\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 109/200\n",
      "  Reconstruction Loss: 13155963.2082\n",
      "  Regularization Loss: 5421.7235\n",
      "  Total Loss: 13161384.9317\n",
      "  Gradient X: mean=-0.2076, std=2.8817\n",
      "  Gradient W: mean=-0.1508, std=2.9961\n",
      "Epoch 110/200\n",
      "  Reconstruction Loss: 13155571.2572\n",
      "  Regularization Loss: 5491.5225\n",
      "  Total Loss: 13161062.7797\n",
      "  Gradient X: mean=0.2077, std=2.8818\n",
      "  Gradient W: mean=0.1508, std=2.9960\n",
      "Epoch 111/200\n",
      "  Reconstruction Loss: 13155747.7646\n",
      "  Regularization Loss: 5422.6885\n",
      "  Total Loss: 13161170.4531\n",
      "  Gradient X: mean=-0.2076, std=2.8819\n",
      "  Gradient W: mean=-0.1511, std=2.9961\n",
      "Epoch 112/200\n",
      "  Reconstruction Loss: 13155341.9531\n",
      "  Regularization Loss: 5492.4973\n",
      "  Total Loss: 13160834.4504\n",
      "  Gradient X: mean=0.2077, std=2.8819\n",
      "  Gradient W: mean=0.1506, std=2.9960\n",
      "Epoch 113/200\n",
      "  Reconstruction Loss: 13155548.8269\n",
      "  Regularization Loss: 5423.6666\n",
      "  Total Loss: 13160972.4934\n",
      "  Gradient X: mean=-0.2076, std=2.8820\n",
      "  Gradient W: mean=-0.1508, std=2.9960\n",
      "Epoch 114/200\n",
      "  Reconstruction Loss: 13155142.6831\n",
      "  Regularization Loss: 5493.4804\n",
      "  Total Loss: 13160636.1635\n",
      "  Gradient X: mean=0.2077, std=2.8820\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 115/200\n",
      "  Reconstruction Loss: 13155359.8325\n",
      "  Regularization Loss: 5424.6484\n",
      "  Total Loss: 13160784.4810\n",
      "  Gradient X: mean=-0.2076, std=2.8821\n",
      "  Gradient W: mean=-0.1508, std=2.9961\n",
      "Epoch 116/200\n",
      "  Reconstruction Loss: 13154901.5262\n",
      "  Regularization Loss: 5494.4814\n",
      "  Total Loss: 13160396.0076\n",
      "  Gradient X: mean=0.2079, std=2.8821\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 117/200\n",
      "  Reconstruction Loss: 13155173.5014\n",
      "  Regularization Loss: 5425.6545\n",
      "  Total Loss: 13160599.1558\n",
      "  Gradient X: mean=-0.2076, std=2.8821\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 118/200\n",
      "  Reconstruction Loss: 13154714.5165\n",
      "  Regularization Loss: 5495.4881\n",
      "  Total Loss: 13160210.0046\n",
      "  Gradient X: mean=0.2078, std=2.8822\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 119/200\n",
      "  Reconstruction Loss: 13154963.3625\n",
      "  Regularization Loss: 5426.6645\n",
      "  Total Loss: 13160390.0270\n",
      "  Gradient X: mean=-0.2076, std=2.8822\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 120/200\n",
      "  Reconstruction Loss: 13154536.2681\n",
      "  Regularization Loss: 5496.5029\n",
      "  Total Loss: 13160032.7710\n",
      "  Gradient X: mean=0.2078, std=2.8822\n",
      "  Gradient W: mean=0.1505, std=2.9960\n",
      "Epoch 121/200\n",
      "  Reconstruction Loss: 13154762.9200\n",
      "  Regularization Loss: 5427.6832\n",
      "  Total Loss: 13160190.6032\n",
      "  Gradient X: mean=-0.2076, std=2.8823\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 122/200\n",
      "  Reconstruction Loss: 13154359.6406\n",
      "  Regularization Loss: 5497.5273\n",
      "  Total Loss: 13159857.1679\n",
      "  Gradient X: mean=0.2078, std=2.8823\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 123/200\n",
      "  Reconstruction Loss: 13154586.4459\n",
      "  Regularization Loss: 5428.7075\n",
      "  Total Loss: 13160015.1534\n",
      "  Gradient X: mean=-0.2076, std=2.8824\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 124/200\n",
      "  Reconstruction Loss: 13154188.9689\n",
      "  Regularization Loss: 5498.5588\n",
      "  Total Loss: 13159687.5277\n",
      "  Gradient X: mean=0.2078, std=2.8824\n",
      "  Gradient W: mean=0.1505, std=2.9961\n",
      "Epoch 125/200\n",
      "  Reconstruction Loss: 13154397.7782\n",
      "  Regularization Loss: 5429.7416\n",
      "  Total Loss: 13159827.5197\n",
      "  Gradient X: mean=-0.2076, std=2.8824\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 126/200\n",
      "  Reconstruction Loss: 13154020.6886\n",
      "  Regularization Loss: 5499.6022\n",
      "  Total Loss: 13159520.2908\n",
      "  Gradient X: mean=0.2078, std=2.8824\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 127/200\n",
      "  Reconstruction Loss: 13154224.8942\n",
      "  Regularization Loss: 5430.7858\n",
      "  Total Loss: 13159655.6800\n",
      "  Gradient X: mean=-0.2076, std=2.8825\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 128/200\n",
      "  Reconstruction Loss: 13153851.0564\n",
      "  Regularization Loss: 5500.6556\n",
      "  Total Loss: 13159351.7120\n",
      "  Gradient X: mean=0.2079, std=2.8825\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 129/200\n",
      "  Reconstruction Loss: 13154046.9083\n",
      "  Regularization Loss: 5431.8410\n",
      "  Total Loss: 13159478.7492\n",
      "  Gradient X: mean=-0.2076, std=2.8825\n",
      "  Gradient W: mean=-0.1505, std=2.9961\n",
      "Epoch 130/200\n",
      "  Reconstruction Loss: 13153672.5106\n",
      "  Regularization Loss: 5501.7197\n",
      "  Total Loss: 13159174.2303\n",
      "  Gradient X: mean=0.2079, std=2.8825\n",
      "  Gradient W: mean=0.1503, std=2.9961\n",
      "Epoch 131/200\n",
      "  Reconstruction Loss: 13153841.9441\n",
      "  Regularization Loss: 5432.9101\n",
      "  Total Loss: 13159274.8542\n",
      "  Gradient X: mean=-0.2076, std=2.8826\n",
      "  Gradient W: mean=-0.1502, std=2.9960\n",
      "Epoch 132/200\n",
      "  Reconstruction Loss: 13153447.1085\n",
      "  Regularization Loss: 5502.8012\n",
      "  Total Loss: 13158949.9097\n",
      "  Gradient X: mean=0.2079, std=2.8825\n",
      "  Gradient W: mean=0.1509, std=2.9960\n",
      "Epoch 133/200\n",
      "  Reconstruction Loss: 13153659.0872\n",
      "  Regularization Loss: 5433.9873\n",
      "  Total Loss: 13159093.0746\n",
      "  Gradient X: mean=-0.2076, std=2.8826\n",
      "  Gradient W: mean=-0.1502, std=2.9961\n",
      "Epoch 134/200\n",
      "  Reconstruction Loss: 13153245.0538\n",
      "  Regularization Loss: 5503.8957\n",
      "  Total Loss: 13158748.9495\n",
      "  Gradient X: mean=0.2079, std=2.8826\n",
      "  Gradient W: mean=0.1508, std=2.9961\n",
      "Epoch 135/200\n",
      "  Reconstruction Loss: 13153490.1697\n",
      "  Regularization Loss: 5435.0831\n",
      "  Total Loss: 13158925.2527\n",
      "  Gradient X: mean=-0.2076, std=2.8826\n",
      "  Gradient W: mean=-0.1506, std=2.9961\n",
      "Epoch 136/200\n",
      "  Reconstruction Loss: 13153079.3772\n",
      "  Regularization Loss: 5505.0004\n",
      "  Total Loss: 13158584.3775\n",
      "  Gradient X: mean=0.2079, std=2.8826\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 137/200\n",
      "  Reconstruction Loss: 13153325.8067\n",
      "  Regularization Loss: 5436.1862\n",
      "  Total Loss: 13158761.9929\n",
      "  Gradient X: mean=-0.2076, std=2.8827\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 138/200\n",
      "  Reconstruction Loss: 13152919.9207\n",
      "  Regularization Loss: 5506.1147\n",
      "  Total Loss: 13158426.0354\n",
      "  Gradient X: mean=0.2080, std=2.8827\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 139/200\n",
      "  Reconstruction Loss: 13153156.6214\n",
      "  Regularization Loss: 5437.3040\n",
      "  Total Loss: 13158593.9253\n",
      "  Gradient X: mean=-0.2076, std=2.8827\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 140/200\n",
      "  Reconstruction Loss: 13152760.8856\n",
      "  Regularization Loss: 5507.2412\n",
      "  Total Loss: 13158268.1268\n",
      "  Gradient X: mean=0.2080, std=2.8827\n",
      "  Gradient W: mean=0.1505, std=2.9961\n",
      "Epoch 141/200\n",
      "  Reconstruction Loss: 13152979.7840\n",
      "  Regularization Loss: 5438.4350\n",
      "  Total Loss: 13158418.2190\n",
      "  Gradient X: mean=-0.2077, std=2.8828\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 142/200\n",
      "  Reconstruction Loss: 13152561.8416\n",
      "  Regularization Loss: 5508.3816\n",
      "  Total Loss: 13158070.2232\n",
      "  Gradient X: mean=0.2080, std=2.8827\n",
      "  Gradient W: mean=0.1506, std=2.9960\n",
      "Epoch 143/200\n",
      "  Reconstruction Loss: 13152806.3796\n",
      "  Regularization Loss: 5439.5778\n",
      "  Total Loss: 13158245.9575\n",
      "  Gradient X: mean=-0.2077, std=2.8828\n",
      "  Gradient W: mean=-0.1508, std=2.9960\n",
      "Epoch 144/200\n",
      "  Reconstruction Loss: 13152396.9696\n",
      "  Regularization Loss: 5509.5337\n",
      "  Total Loss: 13157906.5033\n",
      "  Gradient X: mean=0.2080, std=2.8828\n",
      "  Gradient W: mean=0.1507, std=2.9962\n",
      "Epoch 145/200\n",
      "  Reconstruction Loss: 13152646.8713\n",
      "  Regularization Loss: 5440.7336\n",
      "  Total Loss: 13158087.6048\n",
      "  Gradient X: mean=-0.2077, std=2.8828\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 146/200\n",
      "  Reconstruction Loss: 13152242.2367\n",
      "  Regularization Loss: 5510.6969\n",
      "  Total Loss: 13157752.9336\n",
      "  Gradient X: mean=0.2080, std=2.8828\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 147/200\n",
      "  Reconstruction Loss: 13152487.3082\n",
      "  Regularization Loss: 5441.9005\n",
      "  Total Loss: 13157929.2087\n",
      "  Gradient X: mean=-0.2077, std=2.8828\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 148/200\n",
      "  Reconstruction Loss: 13152087.9764\n",
      "  Regularization Loss: 5511.8716\n",
      "  Total Loss: 13157599.8479\n",
      "  Gradient X: mean=0.2080, std=2.8828\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 149/200\n",
      "  Reconstruction Loss: 13152328.6184\n",
      "  Regularization Loss: 5443.0792\n",
      "  Total Loss: 13157771.6976\n",
      "  Gradient X: mean=-0.2077, std=2.8829\n",
      "  Gradient W: mean=-0.1506, std=2.9961\n",
      "Epoch 150/200\n",
      "  Reconstruction Loss: 13151930.8628\n",
      "  Regularization Loss: 5513.0578\n",
      "  Total Loss: 13157443.9206\n",
      "  Gradient X: mean=0.2080, std=2.8829\n",
      "  Gradient W: mean=0.1505, std=2.9961\n",
      "Epoch 151/200\n",
      "  Reconstruction Loss: 13152168.7814\n",
      "  Regularization Loss: 5444.2704\n",
      "  Total Loss: 13157613.0518\n",
      "  Gradient X: mean=-0.2077, std=2.8829\n",
      "  Gradient W: mean=-0.1506, std=2.9961\n",
      "Epoch 152/200\n",
      "  Reconstruction Loss: 13151775.2443\n",
      "  Regularization Loss: 5514.2561\n",
      "  Total Loss: 13157289.5004\n",
      "  Gradient X: mean=0.2080, std=2.8829\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 153/200\n",
      "  Reconstruction Loss: 13152011.6829\n",
      "  Regularization Loss: 5445.4732\n",
      "  Total Loss: 13157457.1562\n",
      "  Gradient X: mean=-0.2077, std=2.8829\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 154/200\n",
      "  Reconstruction Loss: 13151624.0609\n",
      "  Regularization Loss: 5515.4671\n",
      "  Total Loss: 13157139.5280\n",
      "  Gradient X: mean=0.2080, std=2.8829\n",
      "  Gradient W: mean=0.1507, std=2.9961\n",
      "Epoch 155/200\n",
      "  Reconstruction Loss: 13151857.0446\n",
      "  Regularization Loss: 5446.6890\n",
      "  Total Loss: 13157303.7336\n",
      "  Gradient X: mean=-0.2077, std=2.8829\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 156/200\n",
      "  Reconstruction Loss: 13151474.7296\n",
      "  Regularization Loss: 5516.6896\n",
      "  Total Loss: 13156991.4192\n",
      "  Gradient X: mean=0.2080, std=2.8829\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 157/200\n",
      "  Reconstruction Loss: 13151703.2900\n",
      "  Regularization Loss: 5447.9163\n",
      "  Total Loss: 13157151.2063\n",
      "  Gradient X: mean=-0.2077, std=2.8830\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 158/200\n",
      "  Reconstruction Loss: 13151327.6577\n",
      "  Regularization Loss: 5517.9242\n",
      "  Total Loss: 13156845.5820\n",
      "  Gradient X: mean=0.2080, std=2.8830\n",
      "  Gradient W: mean=0.1507, std=2.9962\n",
      "Epoch 159/200\n",
      "  Reconstruction Loss: 13151555.0226\n",
      "  Regularization Loss: 5449.1551\n",
      "  Total Loss: 13157004.1777\n",
      "  Gradient X: mean=-0.2077, std=2.8830\n",
      "  Gradient W: mean=-0.1508, std=2.9961\n",
      "Epoch 160/200\n",
      "  Reconstruction Loss: 13151131.5198\n",
      "  Regularization Loss: 5519.1766\n",
      "  Total Loss: 13156650.6964\n",
      "  Gradient X: mean=0.2081, std=2.8830\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 161/200\n",
      "  Reconstruction Loss: 13151401.8139\n",
      "  Regularization Loss: 5450.4126\n",
      "  Total Loss: 13156852.2265\n",
      "  Gradient X: mean=-0.2077, std=2.8830\n",
      "  Gradient W: mean=-0.1508, std=2.9961\n",
      "Epoch 162/200\n",
      "  Reconstruction Loss: 13150976.3518\n",
      "  Regularization Loss: 5520.4407\n",
      "  Total Loss: 13156496.7925\n",
      "  Gradient X: mean=0.2081, std=2.8830\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 163/200\n",
      "  Reconstruction Loss: 13151250.8320\n",
      "  Regularization Loss: 5451.6792\n",
      "  Total Loss: 13156702.5112\n",
      "  Gradient X: mean=-0.2077, std=2.8830\n",
      "  Gradient W: mean=-0.1508, std=2.9961\n",
      "Epoch 164/200\n",
      "  Reconstruction Loss: 13150795.8225\n",
      "  Regularization Loss: 5521.7184\n",
      "  Total Loss: 13156317.5409\n",
      "  Gradient X: mean=0.2081, std=2.8830\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 165/200\n",
      "  Reconstruction Loss: 13151101.2953\n",
      "  Regularization Loss: 5452.9605\n",
      "  Total Loss: 13156554.2558\n",
      "  Gradient X: mean=-0.2077, std=2.8830\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 166/200\n",
      "  Reconstruction Loss: 13150649.6711\n",
      "  Regularization Loss: 5523.0059\n",
      "  Total Loss: 13156172.6771\n",
      "  Gradient X: mean=0.2081, std=2.8830\n",
      "  Gradient W: mean=0.1507, std=2.9962\n",
      "Epoch 167/200\n",
      "  Reconstruction Loss: 13150954.9943\n",
      "  Regularization Loss: 5454.2500\n",
      "  Total Loss: 13156409.2443\n",
      "  Gradient X: mean=-0.2077, std=2.8831\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 168/200\n",
      "  Reconstruction Loss: 13150505.2344\n",
      "  Regularization Loss: 5524.3017\n",
      "  Total Loss: 13156029.5361\n",
      "  Gradient X: mean=0.2081, std=2.8831\n",
      "  Gradient W: mean=0.1507, std=2.9961\n",
      "Epoch 169/200\n",
      "  Reconstruction Loss: 13150807.8750\n",
      "  Regularization Loss: 5455.5492\n",
      "  Total Loss: 13156263.4242\n",
      "  Gradient X: mean=-0.2077, std=2.8831\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 170/200\n",
      "  Reconstruction Loss: 13150353.6364\n",
      "  Regularization Loss: 5525.6085\n",
      "  Total Loss: 13155879.2449\n",
      "  Gradient X: mean=0.2081, std=2.8831\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 171/200\n",
      "  Reconstruction Loss: 13150663.3940\n",
      "  Regularization Loss: 5456.8596\n",
      "  Total Loss: 13156120.2536\n",
      "  Gradient X: mean=-0.2077, std=2.8831\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 172/200\n",
      "  Reconstruction Loss: 13150200.4868\n",
      "  Regularization Loss: 5526.9281\n",
      "  Total Loss: 13155727.4148\n",
      "  Gradient X: mean=0.2081, std=2.8831\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 173/200\n",
      "  Reconstruction Loss: 13150516.7046\n",
      "  Regularization Loss: 5458.1832\n",
      "  Total Loss: 13155974.8877\n",
      "  Gradient X: mean=-0.2077, std=2.8831\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 174/200\n",
      "  Reconstruction Loss: 13150055.5677\n",
      "  Regularization Loss: 5528.2582\n",
      "  Total Loss: 13155583.8259\n",
      "  Gradient X: mean=0.2081, std=2.8831\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 175/200\n",
      "  Reconstruction Loss: 13150369.8700\n",
      "  Regularization Loss: 5459.5170\n",
      "  Total Loss: 13155829.3871\n",
      "  Gradient X: mean=-0.2077, std=2.8831\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 176/200\n",
      "  Reconstruction Loss: 13149914.3503\n",
      "  Regularization Loss: 5529.5988\n",
      "  Total Loss: 13155443.9491\n",
      "  Gradient X: mean=0.2081, std=2.8831\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 177/200\n",
      "  Reconstruction Loss: 13150228.9454\n",
      "  Regularization Loss: 5460.8608\n",
      "  Total Loss: 13155689.8061\n",
      "  Gradient X: mean=-0.2077, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 178/200\n",
      "  Reconstruction Loss: 13149774.2043\n",
      "  Regularization Loss: 5530.9496\n",
      "  Total Loss: 13155305.1539\n",
      "  Gradient X: mean=0.2081, std=2.8831\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 179/200\n",
      "  Reconstruction Loss: 13150087.1570\n",
      "  Regularization Loss: 5462.2151\n",
      "  Total Loss: 13155549.3720\n",
      "  Gradient X: mean=-0.2077, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 180/200\n",
      "  Reconstruction Loss: 13149634.1471\n",
      "  Regularization Loss: 5532.3111\n",
      "  Total Loss: 13155166.4582\n",
      "  Gradient X: mean=0.2081, std=2.8832\n",
      "  Gradient W: mean=0.1507, std=2.9962\n",
      "Epoch 181/200\n",
      "  Reconstruction Loss: 13149948.1225\n",
      "  Regularization Loss: 5463.5797\n",
      "  Total Loss: 13155411.7023\n",
      "  Gradient X: mean=-0.2077, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 182/200\n",
      "  Reconstruction Loss: 13149493.3858\n",
      "  Regularization Loss: 5533.6825\n",
      "  Total Loss: 13155027.0683\n",
      "  Gradient X: mean=0.2081, std=2.8832\n",
      "  Gradient W: mean=0.1507, std=2.9961\n",
      "Epoch 183/200\n",
      "  Reconstruction Loss: 13149809.7033\n",
      "  Regularization Loss: 5464.9554\n",
      "  Total Loss: 13155274.6587\n",
      "  Gradient X: mean=-0.2078, std=2.8832\n",
      "  Gradient W: mean=-0.1508, std=2.9961\n",
      "Epoch 184/200\n",
      "  Reconstruction Loss: 13149298.0080\n",
      "  Regularization Loss: 5535.0750\n",
      "  Total Loss: 13154833.0830\n",
      "  Gradient X: mean=0.2082, std=2.8832\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 185/200\n",
      "  Reconstruction Loss: 13149665.5980\n",
      "  Regularization Loss: 5466.3559\n",
      "  Total Loss: 13155131.9539\n",
      "  Gradient X: mean=-0.2077, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 186/200\n",
      "  Reconstruction Loss: 13149155.8986\n",
      "  Regularization Loss: 5536.4764\n",
      "  Total Loss: 13154692.3750\n",
      "  Gradient X: mean=0.2082, std=2.8832\n",
      "  Gradient W: mean=0.1505, std=2.9961\n",
      "Epoch 187/200\n",
      "  Reconstruction Loss: 13149485.5635\n",
      "  Regularization Loss: 5467.7646\n",
      "  Total Loss: 13154953.3282\n",
      "  Gradient X: mean=-0.2077, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 188/200\n",
      "  Reconstruction Loss: 13149018.9248\n",
      "  Regularization Loss: 5537.8901\n",
      "  Total Loss: 13154556.8149\n",
      "  Gradient X: mean=0.2082, std=2.8832\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 189/200\n",
      "  Reconstruction Loss: 13149341.0209\n",
      "  Regularization Loss: 5469.1792\n",
      "  Total Loss: 13154810.2000\n",
      "  Gradient X: mean=-0.2077, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 190/200\n",
      "  Reconstruction Loss: 13148883.0044\n",
      "  Regularization Loss: 5539.3098\n",
      "  Total Loss: 13154422.3142\n",
      "  Gradient X: mean=0.2082, std=2.8832\n",
      "  Gradient W: mean=0.1506, std=2.9962\n",
      "Epoch 191/200\n",
      "  Reconstruction Loss: 13149203.0361\n",
      "  Regularization Loss: 5470.6013\n",
      "  Total Loss: 13154673.6374\n",
      "  Gradient X: mean=-0.2078, std=2.8832\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 192/200\n",
      "  Reconstruction Loss: 13148745.3610\n",
      "  Regularization Loss: 5540.7381\n",
      "  Total Loss: 13154286.0991\n",
      "  Gradient X: mean=0.2081, std=2.8832\n",
      "  Gradient W: mean=0.1507, std=2.9961\n",
      "Epoch 193/200\n",
      "  Reconstruction Loss: 13149066.2225\n",
      "  Regularization Loss: 5472.0319\n",
      "  Total Loss: 13154538.2544\n",
      "  Gradient X: mean=-0.2078, std=2.8833\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 194/200\n",
      "  Reconstruction Loss: 13148612.6337\n",
      "  Regularization Loss: 5542.1739\n",
      "  Total Loss: 13154154.8076\n",
      "  Gradient X: mean=0.2082, std=2.8832\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 195/200\n",
      "  Reconstruction Loss: 13148931.5175\n",
      "  Regularization Loss: 5473.4712\n",
      "  Total Loss: 13154404.9887\n",
      "  Gradient X: mean=-0.2078, std=2.8833\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 196/200\n",
      "  Reconstruction Loss: 13148480.5393\n",
      "  Regularization Loss: 5543.6179\n",
      "  Total Loss: 13154024.1571\n",
      "  Gradient X: mean=0.2082, std=2.8833\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 197/200\n",
      "  Reconstruction Loss: 13148798.0782\n",
      "  Regularization Loss: 5474.9190\n",
      "  Total Loss: 13154272.9972\n",
      "  Gradient X: mean=-0.2078, std=2.8833\n",
      "  Gradient W: mean=-0.1507, std=2.9962\n",
      "Epoch 198/200\n",
      "  Reconstruction Loss: 13148347.4896\n",
      "  Regularization Loss: 5545.0714\n",
      "  Total Loss: 13153892.5611\n",
      "  Gradient X: mean=0.2082, std=2.8833\n",
      "  Gradient W: mean=0.1506, std=2.9961\n",
      "Epoch 199/200\n",
      "  Reconstruction Loss: 13148652.9597\n",
      "  Regularization Loss: 5476.3784\n",
      "  Total Loss: 13154129.3380\n",
      "  Gradient X: mean=-0.2078, std=2.8833\n",
      "  Gradient W: mean=-0.1507, std=2.9961\n",
      "Epoch 200/200\n",
      "  Reconstruction Loss: 13148209.4442\n",
      "  Regularization Loss: 5546.5372\n",
      "  Total Loss: 13153755.9813\n",
      "  Gradient X: mean=0.2082, std=2.8833\n",
      "  Gradient W: mean=0.1506, std=2.9962\n"
     ]
    }
   ],
   "source": [
    "#4. Start SGD loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the predicted matrix\n",
    "    R_hat = X @ W.T\n",
    "\n",
    "    # Compute the error matrix for observed entries only\n",
    "    E = np.multiply(mask, R - R_hat)\n",
    "\n",
    "    # Compute gradients with regularization\n",
    "    grad_X = -E @ W + reg_lambda * X\n",
    "    grad_W = -E.T @ X + reg_lambda * W\n",
    "\n",
    "    # Apply gradient clipping to avoid exploding updates\n",
    "    grad_X = np.clip(grad_X, -gradient_clip, gradient_clip)\n",
    "    grad_W = np.clip(grad_W, -gradient_clip, gradient_clip)\n",
    "\n",
    "    # Update X and W\n",
    "    X -= learning_rate * grad_X\n",
    "    W -= learning_rate * grad_W\n",
    "\n",
    "    # Compute the total loss\n",
    "    reconstruction_loss = np.sum(np.multiply(mask, E) ** 2)\n",
    "    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n",
    "    total_loss = reconstruction_loss + regularization_loss\n",
    "\n",
    "    # Append the total loss to the history\n",
    "    loss_history.append(total_loss)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n",
    "    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Check for NaN or Inf values in X or W\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(W)) or np.any(np.isinf(X)) or np.any(np.isinf(W)):\n",
    "        print(\"Numerical instability detected. Terminating training.\")\n",
    "        break\n",
    "\n",
    "    # Debugging: Check mean and std of gradients\n",
    "    print(f\"  Gradient X: mean={np.mean(grad_X):.4f}, std={np.std(grad_X):.4f}\")\n",
    "    print(f\"  Gradient W: mean={np.mean(grad_W):.4f}, std={np.std(grad_W):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing (User-Based)\n",
    "The function is tested with a sample user to generate personalized recommendations (User-Based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for User 774868:\n",
      "                                                Name  Year  Estimated_Rating  \\\n",
      "0      Lord of the Rings: The Fellowship of the Ring  2001          0.924213   \n",
      "1  Pirates of the Caribbean: The Curse of the Bla...  2003          0.757490   \n",
      "2                               Bend It Like Beckham  2002          0.646989   \n",
      "3                                  Princess Mononoke  1997          0.637968   \n",
      "4                          Finding Nemo (Widescreen)  2003          0.636507   \n",
      "\n",
      "   Movie_ID  \n",
      "0      2452  \n",
      "1      1905  \n",
      "2      1470  \n",
      "3       473  \n",
      "4      3962  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the recommendation function\n",
    "user_id_to_test = 774868 #1331154  # Change as needed\n",
    "num_recommendations = 5\n",
    "\n",
    "try:\n",
    "    user_based_recommendations = recommend_movies_user_based(user_id_to_test, sparse_user_item, num_recommendations=num_recommendations)\n",
    "    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test}:\")\n",
    "    print(user_based_recommendations[['Name', 'Year', 'Estimated_Rating', 'Movie_ID']])\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Based Collaborative Filtering\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **Item-User Matrix Creation**: Transpose the user-item matrix $M$ to create a new matrix $M' = M^T$, where rows represent movies and columns represent users. Missing ratings are filled with zeros.\n",
    "  $$ M'[i, u] = M[u, i] = r_{u,i} \\in R$$\n",
    "- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format to optimize memory usage.\n",
    "  $$M'_{\\{\\text{sparse}\\}} = \\text{csr\\_matrix}(M')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transpose the user-item matrix\n",
    "item_user_matrix = user_item_matrix.T\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix\n",
    "sparse_item_user = csr_matrix(item_user_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collaborative Filtering Model (Item-Based)\n",
    "We use the `scikit-learn` library to implement a KNN-based model:\n",
    "\n",
    "1. **Similarity Metric**: Cosine similarity is used to identify movies with similar rating patterns. The formula for cosine similarity between two movies $i$ and $j$ is:\n",
    "   $$\n",
    "   \\text{sim}(i, j) = \\frac{\\vec r_i \\cdot \\vec r_j}{\\|\\vec r_i\\| \\cdot \\|\\vec r_j\\|}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\vec r_i$ and $\\vec r_j$ are column vectors of user ratings for movies $i$ and $j$ respectively (the $i$-th and $j$-th rows of the item-user matrix $M'$).\n",
    "   - $\\cdot$ represents the dot product.\n",
    "\n",
    "2. **Nearest Neighbors**: The model identifies the top $k$ nearest neighbors for each movie based on similarity scores:\n",
    "   $$\\large\n",
    "   \\mathcal{N}_i^k = \\argmax_{I'_u \\subseteq I_u \\setminus \\{i\\} \\land |I'_u| = k} \\sum_{j \\in I'_u} \\text{sim}(i, j)\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\mathcal N_i^k$ is the set of the top $k$ nearest neighbors (in the user $u$ rated movies set) for movie $i$.\n",
    "   - $I'_u$ is a subset, of size $k$, of items rated by user $u$ that excludes movie $i$, obviously.\n",
    "   - $k = 10$ (default value in this implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the item-based collaborative filtering model\n",
    "model_knn_item = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10)\n",
    "\n",
    "# Create the sparse matrix for the item-based model\n",
    "sparse_item_user = csr_matrix(item_user_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recommendation Function (Item-Based)\n",
    "\n",
    "In the item-based collaborative filtering approach, the recommendation function works as follows:\n",
    "\n",
    "Predict movie ratings for the target user by aggregating ratings from similar movies. For a given user $u$ and an item $i$ not yet rated by $u$, the predicted rating is calculated as:\n",
    "   $$\\large\n",
    "   I^k_u = \\{\\hat{r}_{u,i} \\mid i \\in I \\setminus I_u \\setminus \\{i\\}\\} \\quad \\text{where} \\quad\n",
    "   \\hat{r}_{u,i} = \\frac{\\sum_{j \\in \\mathcal N_i^k} \\text{sim}(i, j) \\cdot r_{u,j}}{\\sum_{j \\in \\mathcal N_i^k} \\text{sim}(i, j)}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\hat{r}_{u,i}$ is the predicted rating for user $u$ on item $i$.\n",
    "   - $r_{u,j}$ is the rating of user $u$ for a similar item $j \\in \\mathcal N_i^k$.\n",
    "   - $\\text{sim}(i, j)$ is the similarity between items $i$ and $j$.\n",
    "   - $\\mathcal N_i^k$ is the set of the top $k$ most similar items to $i$.\n",
    "   - $I^k_u$ is the set of predicted ratings for user $u$ on items not yet rated by $u$.\n",
    "\n",
    "Thus, for each movie not rated by the target user, the model calculates a weighted average of similar movies' ratings based on their similarity scores. The predicted ratings are then sorted and the top are returned.\n",
    "\n",
    "This method leverages the user's own preferences and the similarity relationships between movies, providing relevant and personalized suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Recommendation Function (Item-Based)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def recommend_movies_item_based(user_id, train_matrix, num_recommendations=5):\n",
    "    \"\"\"Recommend movies using item-based collaborative filtering for a given user.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "\n",
    "    # Get the user's ratings\n",
    "    user_ratings = item_user_matrix.loc[:, user_id].to_numpy()\n",
    "    rated_movies = np.where(user_ratings > 0)[0]  # Indices of movies the user has rated\n",
    "\n",
    "    if len(rated_movies) == 0:\n",
    "        raise ValueError(\"User has not rated any movies\")\n",
    "\n",
    "    # Initialize a dictionary to store weighted scores\n",
    "    movie_scores = {}\n",
    "\n",
    "    # Fit the model with the sparse item-user matrix limited to the rated movies\n",
    "    sparse_item_user_u = train_matrix[rated_movies, :]\n",
    "    model_knn_item.fit(sparse_item_user_u)\n",
    "\n",
    "    # Get unseen movies\n",
    "    not_rated_movies = item_user_matrix.index.difference(item_user_matrix.index[rated_movies])\n",
    "\n",
    "    # Iterate over all movies not rated by the user\n",
    "    for movie_id in tqdm(not_rated_movies, \n",
    "                         desc=\"Processing Movies\", unit=\"movie\"):\n",
    "        # Get the vector for the movie\n",
    "        movie_idx = item_user_matrix.index.get_loc(movie_id)\n",
    "        movie_vector = train_matrix[movie_idx, :]\n",
    "\n",
    "        # Find nearest neighbors for the movie\n",
    "        distances, indices = model_knn_item.kneighbors(movie_vector, n_neighbors=10)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "        # Get the indexes of the similar movies\n",
    "        similar_movies = rated_movies[indices.flatten()]\n",
    "\n",
    "        # Calculate the weighted average score for the movie using similarity scores\n",
    "        user_ratings_for_similar = user_ratings[similar_movies]\n",
    "        movie_scores[movie_id] = np.dot(similarity_scores, user_ratings_for_similar) / 10 #similarity_scores.sum()\n",
    "\n",
    "    # Sort movies by aggregated score\n",
    "    recommended_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)[:num_recommendations]\n",
    "\n",
    "    # Normalize the scores\n",
    "    recommended_movies = [(movie_id, score) for movie_id, score in recommended_movies]\n",
    "\n",
    "    # Map movie IDs to names\n",
    "    recommended_movies_df = pd.DataFrame(recommended_movies, columns=['Movie_ID', 'Estimated_Rating'])\n",
    "    recommendations = recommended_movies_df.merge(movies, on='Movie_ID')[['Movie_ID', 'Name', 'Year', 'Estimated_Rating']]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Recommendation Function (Item-Based)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def recommend_movies_item_based(user_id, train_matrix, num_recommendations=5):\n",
    "    \"\"\"Recommend movies using item-based collaborative filtering for a given user.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "\n",
    "    # Get the user's ratings\n",
    "    user_ratings = item_user_matrix.loc[:, user_id].to_numpy()\n",
    "    rated_movies = np.where(user_ratings > 0)[0]  # Indices of movies the user has rated\n",
    "\n",
    "    if len(rated_movies) == 0:\n",
    "        raise ValueError(\"User has not rated any movies\")\n",
    "\n",
    "    # Initialize a dictionary to store weighted scores\n",
    "    movie_scores = {}\n",
    "\n",
    "    # Fit the model with the sparse item-user matrix limited to the rated movies\n",
    "    sparse_item_user_u = train_matrix[rated_movies, :]\n",
    "    model_knn_item.fit(sparse_item_user_u)\n",
    "\n",
    "    # Get unseen movies\n",
    "    not_rated_movies = item_user_matrix.index.difference(item_user_matrix.index[rated_movies])\n",
    "\n",
    "    # Iterate over all movies not rated by the user\n",
    "    for movie_id in tqdm(not_rated_movies, \n",
    "                         desc=\"Processing Movies\", unit=\"movie\"):\n",
    "        # Get the vector for the movie\n",
    "        movie_idx = item_user_matrix.index.get_loc(movie_id)\n",
    "        movie_vector = train_matrix[movie_idx, :]\n",
    "\n",
    "        # Find nearest neighbors for the movie\n",
    "        distances, indices = model_knn_item.kneighbors(movie_vector, n_neighbors=10)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "        # Get the indexes of the similar movies\n",
    "        similar_movies = rated_movies[indices.flatten()]\n",
    "\n",
    "        # Calculate the weighted average score for the movie using similarity scores\n",
    "        user_ratings_for_similar = user_ratings[similar_movies]\n",
    "        movie_scores[movie_id]    = []\n",
    "        movie_scores[movie_id].append(np.dot(similarity_scores, user_ratings_for_similar))\n",
    "\n",
    "        # Store the similarity sum for normalization\n",
    "        movie_scores[movie_id].append(similarity_scores.sum())\n",
    "\n",
    "    # Sort movies by aggregated score\n",
    "    recommended_movies = sorted(movie_scores.items(), key=lambda x: x[1][0], reverse=True)[:num_recommendations]\n",
    "\n",
    "    # Normalize the scores\n",
    "    recommended_movies = [(movie_id, score[0] / score[1]) for movie_id, score in recommended_movies]\n",
    "\n",
    "    # Map movie IDs to names\n",
    "    recommended_movies_df = pd.DataFrame(recommended_movies, columns=['Movie_ID', 'Estimated_Rating'])\n",
    "    recommendations = recommended_movies_df.merge(movies, on='Movie_ID')[['Movie_ID', 'Name', 'Year', 'Estimated_Rating']]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing\n",
    "The function is tested with a sample user to generate personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Movies: 100%|██████████| 1331/1331 [00:10<00:00, 132.73movie/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for User 774868 (Item-Based):\n",
      "                                                Name  Year  Estimated_Rating  \\\n",
      "0  Pirates of the Caribbean: The Curse of the Bla...  2003          4.816355   \n",
      "1      Lord of the Rings: The Fellowship of the Ring  2001          4.809336   \n",
      "2                                    The Sixth Sense  1999          4.808670   \n",
      "3                                    American Beauty  1999          4.761287   \n",
      "4                          Finding Nemo (Widescreen)  2003          4.817077   \n",
      "\n",
      "   Movie_ID  \n",
      "0      1905  \n",
      "1      2452  \n",
      "2      4306  \n",
      "3       571  \n",
      "4      3962  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the recommendation function\n",
    "user_id_to_test = 774868#1331154  # Change as needed\n",
    "num_recommendations = 5\n",
    "\n",
    "try:\n",
    "    item_based_recommendations = recommend_movies_item_based(user_id_to_test, sparse_item_user, num_recommendations=num_recommendations)\n",
    "    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test} (Item-Based):\")\n",
    "    print(item_based_recommendations[['Name', 'Year', 'Estimated_Rating', 'Movie_ID']])\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation of the Model\n",
    "\n",
    "To assess the effectiveness of the collaborative filtering approach, we perform a train-test split on the ratings data. The evaluation process includes:\n",
    "\n",
    "1. **Train-Test Split**:  \n",
    "   - 80% of the data is used for training the model.\n",
    "   - 20% of the data is reserved for testing.\n",
    "\n",
    "2. **Predictions and Metrics**:  \n",
    "   - For each user in the test set, the model predicts ratings for movies based on the nearest neighbors identified in the training data.\n",
    "   - **Mean Absolute Error (MAE)** is calculated as the primary metric to evaluate prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset...Done\n",
      "Creating train and test matrices...Done\n",
      "Evaluating the recommender system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100%|██████████| 3413/3413 [21:01<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Precision@50: 0.0000, Recall@50: 0.0000\n",
      "Precision@k: 0.0000, Recall@k: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluation (Optional)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split the ratings data\n",
    "print(\"Splitting the dataset...\", end=\"\")\n",
    "train_data, test_data = train_test_split(\n",
    "    ratings, \n",
    "    test_size=0.0002, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"Ok\")\n",
    "\n",
    "# Create train and test user-item matrices\n",
    "print(\"Creating train and test matrices...\", end=\"\")\n",
    "user_item_matrix = ratings.pivot(index='User_ID', columns='Movie_ID', values='Rating').fillna(0)\n",
    "sparse_user_item = csr_matrix(user_item_matrix.values)\n",
    "item_user_matrix = user_item_matrix.T\n",
    "sparse_item_user = csr_matrix(item_user_matrix.values)\n",
    "test_matrix  = test_data #.pivot(index='User_ID', columns='Movie_ID', values='Rating').fillna(0)\n",
    "print(\"Ok\")\n",
    "\n",
    "# Test the recommender\n",
    "def evaluate_recommender():\n",
    "    \"\"\"\n",
    "    Evaluate the user-based collaborative filtering recommender system.\n",
    "    Computes Precision@k and Recall@k for each user in the test set.\n",
    "    \"\"\"\n",
    "    k = 50  # Top-k recommendations to evaluate\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    print(\"Evaluating the recommender system...\")\n",
    "\n",
    "    # Iterate over each user in the test set\n",
    "    for user_id in tqdm(test_data['User_ID'].unique(), desc=\"Users evaluated\"):\n",
    "        # Extract the user's test ratings\n",
    "        user_test_ratings = test_data[test_data['User_ID'] == user_id]\n",
    "\n",
    "        # Identify relevant items (rated by the user in the test set)\n",
    "        relevant_items = set(user_test_ratings['Movie_ID'])\n",
    "\n",
    "        # Skip users with no interactions in the test set\n",
    "        if len(relevant_items) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get recommendations for the user\n",
    "        try:\n",
    "            recommendations = recommend_movies_user_based(user_id, sparse_user_item, num_recommendations=k)\n",
    "        except ValueError:\n",
    "            # Skip if the user is not in the training dataset\n",
    "            continue\n",
    "\n",
    "        # Extract the recommended items\n",
    "        recommended_items = set(recommendations['Movie_ID'].values)\n",
    "\n",
    "        # Compute true positives (intersection of relevant and recommended items)\n",
    "        true_positives = relevant_items & recommended_items\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = len(true_positives) / len(recommended_items) if recommended_items else 0\n",
    "        recall = len(true_positives) / len(relevant_items) if relevant_items else 0\n",
    "\n",
    "        # Append results for this user\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Compute average precision and recall across all users\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "\n",
    "    print(f\"Evaluation complete. Precision@{k}: {avg_precision:.4f}, Recall@{k}: {avg_recall:.4f}\")\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "# Run the evaluation\n",
    "precision, recall = evaluate_recommender()\n",
    "print(f\"Precision@k: {precision:.4f}, Recall@k: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrated the implementation of a collaborative filtering approach for recommending movies. Key takeaways include:\n",
    "\n",
    "- The model effectively utilizes user similarity to make recommendations, as shown by the ability to generate relevant suggestions for a sample user.\n",
    "- The **MAE metric** provides a reliable evaluation of the model's predictive accuracy.\n",
    "- While collaborative filtering is powerful, it faces challenges such as:\n",
    "  - **Cold Start Problem**: Difficulty in recommending movies for new users or items.\n",
    "  - **Data Sparsity**: Limited interactions in the dataset can affect similarity computations.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "To address the limitations, potential enhancements include:\n",
    "\n",
    "- Implementing hybrid recommendation systems that combine collaborative and content-based filtering.\n",
    "- Exploring matrix factorization techniques (e.g., Singular Value Decomposition).\n",
    "- Integrating deep learning-based recommendation methods.\n",
    "\n",
    "This collaborative filtering approach forms a solid foundation for building scalable and effective recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
