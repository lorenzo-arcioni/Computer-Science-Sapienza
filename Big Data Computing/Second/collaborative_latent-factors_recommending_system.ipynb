{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Collaborative Latent-Factors-Based Filtering for Movie Recommendations (Incomplete)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The latent factor approach in recommendation systems utilizes matrix factorization techniques to uncover hidden patterns in user-item interactions. These methods predict user preferences by mapping both users and items to a shared latent space where their interactions can be represented by their proximity or alignment. Latent factor models, such as Singular Value Decomposition (SVD), are widely used in this context.\n",
    "\n",
    "Key features of the latent factor approach:\n",
    "- Captures underlying relationships between users and items.\n",
    "- Handles sparse datasets effectively by reducing dimensionality.\n",
    "- Improves scalability compared to neighborhood-based methods.\n",
    "\n",
    "In this notebook, we will explore the latent factor approach to build a movie recommendation system using matrix factorization.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "The latent factor approach works by:\n",
    "- Represent users and items in a shared lower dimensional latent space (ie, as a vector of latent factors).\n",
    "- Such vectros are inferred (ie, learned) from the observed ratings.\n",
    "- High correlation between user and item latent factors indicates a possible recomendation.\n",
    "- Map both users and items to the latent space and then predict ratings based on the inner product in the latent space.\n",
    "\n",
    "So formally we have:\n",
    "- $\\vec x_u \\in R^d$ is the latent factor vector for user $u$. Each $\\vec x_u[k] \\in R$ measure the extent of interest user $u$ has in items exhibiting latent factor $k$.\n",
    "- $\\vec w_i \\in R^d$ is the latent factor vector for item $i$. Each $\\vec w_i[k] \\in R$ measure the extent of interest item $i$ has in users exhibiting latent factor $k$.\n",
    "\n",
    "Essentially, $d$ hidden features to describe both users and items.\n",
    "\n",
    "Thus, $r_{u,i}$ is the rating given by user $u$ to item $i$ and $\\hat{r}_{u,i} = \\vec x_u \\cdot \\vec w_i = \\sum_{k=1}^d \\vec x_u[k] \\cdot \\vec w_i[k]$ is the predicted rating for user $u$ and item $i$.\n",
    "\n",
    "The problem is to approximate the user-item matrix $R \\in \\mathbb R^{m \\times n}$ with the product of a user latent factor matrix $X \\in \\mathbb R^{m \\times d}$ and an item latent factor matrix $W^T \\in \\mathbb R^{d \\times n}$. So\n",
    "\n",
    "$$\n",
    "R \\approx X \\cdot W^T.\n",
    "$$\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "We use two datasets for this analysis:\n",
    "\n",
    "1. **Movies Dataset**:\n",
    "   - `Movie_ID`: Unique identifier for each movie.\n",
    "   - `Title`: Name of the movie.\n",
    "   - `Year`: Year the movie was released.\n",
    "\n",
    "2. **Ratings Dataset**:\n",
    "   - `User_ID`: Unique identifier for each user.\n",
    "   - `Movie_ID`: Identifier for the movie rated by the user.\n",
    "   - `Rating`: Numeric rating provided by the user (e.g., on a scale of 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the dataset\n",
    "ratings = pd.read_csv(\"./data/Netflix_Dataset_Rating.csv\")  # Columns: User_ID, Rating, Movie_ID\n",
    "movies  = pd.read_csv(\"./data/Netflix_Dataset_Movie.csv\")    # Columns: Movie_ID, Year, Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "- $U = \\{u_1, u_2, \\dots, u_n\\}$ is the set of users.\n",
    "- $U_i = \\{u \\in U \\mid r_{u,i} \\neq 0\\}$ is the set of users who have rated item $i$\n",
    "- $I = \\{i_1, i_2, \\dots, i_m\\}$ is the set of items.\n",
    "- $I_u = \\{i \\in I \\mid r_{u,i} \\neq 0\\}$ is the set of items rated by user $u$\n",
    "- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n",
    "- $r_{u,i}$ is the rating given by user $u$ for item $i$ (equal to 0 if not rated).\n",
    "- $D = \\{(u_j, i_j)\\}_{j=1}^{N}$ is the set of user-item pairs (our dataset).\n",
    "- $I_D = \\{i \\in I \\mid \\exists (u, i) \\in D\\}$ is the set of items in the dataset.\n",
    "- $U_D = \\{u \\in U \\mid \\exists (u, i) \\in D\\}$ is the set of users in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **User-Item Matrix Creation**: Convert the ratings dataset into a user-item matrix, where rows represent users and columns represent movies. Missing ratings are filled with zeros. Each rating is represented by a number from 1 to 5.\n",
    "  $$ M[u, i] = r_{u,i} \\in R$$\n",
    "  Where:\n",
    "  - $u \\in U$ is the set of users.\n",
    "  - $i \\in I$ is the set of movies.\n",
    "  - $r_{u,i}$ is the rating given by user $u$ for movie $i$.\n",
    "\n",
    "- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format for memory optimization:\n",
    "  $$M_{\\{\\text{sparse}\\}} = \\text{sparse}(M)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a user-item matrix\n",
    "user_item_matrix = ratings.pivot(index='User_ID', columns='Movie_ID', values='Rating')\n",
    "\n",
    "# Fill missing values with 0 (can use NaN for some algorithms)\n",
    "user_item_matrix.fillna(0, inplace=True) # It is not the case for this dataset\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determine the Loss Function and Computing Its Gradient\n",
    "\n",
    "Assuming we have access to the dataset $D$ of observed atings, the matrix $R$ is partially known and filled with those observations. To actully learn the latent factors, we need to choose a loss function to optimize. In our case, we choose squared error (SE):\n",
    "\n",
    "$$\n",
    "L(X, W) = \\frac{1}{2} \\left[ \\sum_{(u, i) \\in D} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda (\\sum_{u \\in U_D} \\|\\vec x_u\\|^2 + \\sum_{i \\in I_D} \\|\\vec w_i\\|^2)\\right]\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$X^*, W^* = \\argmin_{X, W} \\ L(X, W).$$\n",
    "\n",
    "#### Loss Function\n",
    "The loss function in, matrix notation, is defined in terms of matrices as:\n",
    "$$\n",
    "L(X, W) = \\frac{1}{2} \\left[ \\| R - X W^T \\|_F^2 + \\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right) \\right],\n",
    "$$\n",
    "where:\n",
    "- $R \\in \\mathbb{R}^{m \\times n}$ is the observed rating matrix, with $R_{u,i} = r_{u,i}$ if user $u$ has rated item $i$, and 0 otherwise.\n",
    "- $X \\in \\mathbb{R}^{m \\times d}$ represents the user latent factors (each row corresponds to a user vector $X_u$).\n",
    "- $W \\in \\mathbb{R}^{n \\times d}$ represents the item latent factors (each row corresponds to an item vector $W_i$).\n",
    "- $\\| \\cdot \\|_F$ is the Frobenius norm.\n",
    "\n",
    "The prediction matrix is:\n",
    "$$\n",
    "\\hat{R} = X W^T.\n",
    "$$\n",
    "\n",
    "The loss consists of:\n",
    "1. The reconstruction error:\n",
    "$$\n",
    "\\| R - X W^T \\|_F^2 = \\sum_{(u, i) \\in D} (r_{u,i} - X_u W_i^T)^2.\n",
    "$$\n",
    "2. The regularization terms:\n",
    "$$\n",
    "\\lambda \\left( \\|X\\|_F^2 + \\|W\\|_F^2 \\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Computing the Gradients\n",
    "\n",
    "##### Gradient with respect to $X$\n",
    "\n",
    "1. Differentiate the reconstruction error term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X} \\frac{1}{2} \\| R - X W^T \\|_F^2 = -(R - X W^T) W.\n",
    "$$\n",
    "\n",
    "2. Differentiate the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X} \\frac{\\lambda}{2} \\|X\\|_F^2 = \\lambda X.\n",
    "$$\n",
    "\n",
    "3. Combine the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = -(R - X W^T) W + \\lambda X.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Gradient with respect to $W$\n",
    "\n",
    "1. Differentiate the reconstruction error term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\frac{1}{2} \\| R - X W^T \\|_F^2 = -(R - X W^T)^T X.\n",
    "$$\n",
    "\n",
    "2. Differentiate the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\frac{\\lambda}{2} \\|W\\|_F^2 = \\lambda W.\n",
    "$$\n",
    "\n",
    "3. Combine the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = -(R - X W^T)^T X + \\lambda W.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setting up the model parameters\n",
    "\n",
    "# Hyperparameters\n",
    "num_factors = 100  # Number of latent factors (k)\n",
    "learning_rate = 0.01  # Learning rate (eta)\n",
    "reg_lambda = 0.1  # Regularization term (lambda)\n",
    "num_epochs = 30  # Number of epochs\n",
    "gradient_clip = 2.0  # Gradient clipping threshold\n",
    "\n",
    "# Dimensions of the user-item matrix\n",
    "num_users, num_items = user_item_matrix.shape\n",
    "\n",
    "# Initialize latent factor matrices X and W with small random values\n",
    "X = np.random.normal(scale=0.01, size=(num_users, num_factors))\n",
    "W = np.random.normal(scale=0.01, size=(num_items, num_factors))\n",
    "\n",
    "# Create a mask for observed entries in R\n",
    "R = user_item_matrix.values\n",
    "mask = R > 0  # Boolean mask for observed entries\n",
    "\n",
    "# List to store loss values for plotting\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimize the Loss Function with Stochastic Gradient Descent\n",
    "\n",
    "In order to optimize the loss function, we use Stochastic Gradient Descent (SGD).\n",
    "\n",
    "#### Explanation of the SGD Algorithm (Matrix Form)\n",
    "\n",
    "##### 1. Initialization\n",
    "- Matrices $X$ (users' latent factors) and $W$ (items' latent factors) are initialized randomly with small values.\n",
    "- $X \\in \\mathbb{R}^{m \\times d}$, where $m$ is the number of users and $d$ is the number of latent factors.\n",
    "- $W \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of items.\n",
    "\n",
    "##### 2. Gradient Computation\n",
    "- Define the prediction matrix:\n",
    "  $$\n",
    "  \\hat{R} = X W^T\n",
    "  $$\n",
    "- Compute the error matrix (only for observed entries in $R$):\n",
    "  $$\n",
    "  E = \\begin{cases}\n",
    "  R_{ui} > 0 \\quad R_{ui} - \\hat{R}_{ui}\\\\\n",
    "  0 \\quad \\text{otherwise}\n",
    "  \\end{cases},\n",
    "  $$\n",
    "  where $E_{ui} = 0$ for unobserved entries of $R$.\n",
    "\n",
    "- Gradients for $X$ and $W$:\n",
    "  $$\n",
    "  \\nabla_X = - E W + \\lambda X\n",
    "  $$\n",
    "  $$\n",
    "  \\nabla_W = - E^T X + \\lambda W\n",
    "  $$\n",
    "\n",
    "##### 3. Updates\n",
    "- Update the latent factor matrices $X$ and $W$ simultaneously:\n",
    "  $$\n",
    "  X \\leftarrow X - \\eta \\nabla_X\n",
    "  $$\n",
    "  $$\n",
    "  W \\leftarrow W - \\eta \\nabla_W\n",
    "  $$\n",
    "- Here, $\\eta$ is the learning rate.\n",
    "\n",
    "##### 4. Loss Tracking\n",
    "- The total loss for each epoch combines the squared error and the regularization terms:\n",
    "  $$\n",
    "  L = \\| R - X W^T \\|_F^2 + \\lambda (\\|X\\|_F^2 + \\|W\\|_F^2)\n",
    "  $$\n",
    "- This tracks the reconstruction error and ensures that the latent factor matrices do not grow too large (controlled by the regularization term).\n",
    "\n",
    "##### 5. Optimization Loop\n",
    "- Repeat the following steps for a fixed number of epochs or until the loss converges:\n",
    "  1. Compute the error matrix $E$.\n",
    "  2. Compute the gradients $\\nabla_X$ and $\\nabla_W$ using matrix operations.\n",
    "  3. Update $X$ and $W$ using the gradients.\n",
    "  4. Track and print the loss for each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "##### Notes\n",
    "- This implementation only updates $X$ and $W$ for the observed entries of $R$ using matrix masking.\n",
    "- The hyperparameters ($\\eta$, $d$, and $\\lambda$) should be tuned based on the dataset for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  Reconstruction Loss: 243096336.8193\n",
      "  Regularization Loss: 173.5566\n",
      "  Total Loss: 243096510.3759\n",
      "  Gradient X: mean=0.0138, std=0.4234\n",
      "  Gradient W: mean=0.0188, std=1.5754\n",
      "Epoch 2/30\n",
      "  Reconstruction Loss: 243025143.4859\n",
      "  Regularization Loss: 608.8258\n",
      "  Total Loss: 243025752.3117\n",
      "  Gradient X: mean=-0.0392, std=1.6836\n",
      "  Gradient W: mean=0.0388, std=1.9757\n",
      "Epoch 3/30\n",
      "  Reconstruction Loss: 240346544.7414\n",
      "  Regularization Loss: 1459.0490\n",
      "  Total Loss: 240348003.7904\n",
      "  Gradient X: mean=0.1167, std=1.9251\n",
      "  Gradient W: mean=0.0283, std=1.9979\n",
      "Epoch 4/30\n",
      "  Reconstruction Loss: 231143324.2500\n",
      "  Regularization Loss: 3232.8502\n",
      "  Total Loss: 231146557.1002\n",
      "  Gradient X: mean=0.0463, std=1.9037\n",
      "  Gradient W: mean=0.2535, std=1.9813\n",
      "Epoch 5/30\n",
      "  Reconstruction Loss: 216493572.6411\n",
      "  Regularization Loss: 5783.1455\n",
      "  Total Loss: 216499355.7866\n",
      "  Gradient X: mean=0.2463, std=1.9535\n",
      "  Gradient W: mean=0.0878, std=1.9973\n",
      "Epoch 6/30\n",
      "  Reconstruction Loss: 195127498.2908\n",
      "  Regularization Loss: 9331.2985\n",
      "  Total Loss: 195136829.5892\n",
      "  Gradient X: mean=0.1135, std=1.9434\n",
      "  Gradient W: mean=0.2596, std=1.9816\n",
      "Epoch 7/30\n",
      "  Reconstruction Loss: 168712167.8613\n",
      "  Regularization Loss: 13810.4451\n",
      "  Total Loss: 168725978.3065\n",
      "  Gradient X: mean=0.2347, std=1.9641\n",
      "  Gradient W: mean=0.1829, std=1.9910\n",
      "Epoch 8/30\n",
      "  Reconstruction Loss: 138019206.6171\n",
      "  Regularization Loss: 19319.8045\n",
      "  Total Loss: 138038526.4216\n",
      "  Gradient X: mean=0.2018, std=1.9454\n",
      "  Gradient W: mean=0.2111, std=1.9880\n",
      "Epoch 9/30\n",
      "  Reconstruction Loss: 105512687.5220\n",
      "  Regularization Loss: 25854.4009\n",
      "  Total Loss: 105538541.9229\n",
      "  Gradient X: mean=0.1830, std=1.9684\n",
      "  Gradient W: mean=0.2582, std=1.9827\n",
      "Epoch 10/30\n",
      "  Reconstruction Loss: 73606920.2563\n",
      "  Regularization Loss: 33449.5789\n",
      "  Total Loss: 73640369.8352\n",
      "  Gradient X: mean=0.2403, std=1.9364\n",
      "  Gradient W: mean=0.1923, std=1.9902\n",
      "Epoch 11/30\n",
      "  Reconstruction Loss: 45810349.4605\n",
      "  Regularization Loss: 42097.3560\n",
      "  Total Loss: 45852446.8165\n",
      "  Gradient X: mean=0.2243, std=1.9464\n",
      "  Gradient W: mean=0.2264, std=1.9859\n",
      "Epoch 12/30\n",
      "  Reconstruction Loss: 26050023.6690\n",
      "  Regularization Loss: 50982.7977\n",
      "  Total Loss: 26101006.4667\n",
      "  Gradient X: mean=0.2066, std=1.9018\n",
      "  Gradient W: mean=0.2003, std=1.9863\n",
      "Epoch 13/30\n",
      "  Reconstruction Loss: 17881277.1937\n",
      "  Regularization Loss: 51794.4724\n",
      "  Total Loss: 17933071.6661\n",
      "  Gradient X: mean=0.0241, std=1.7768\n",
      "  Gradient W: mean=-0.0207, std=1.9932\n",
      "Epoch 14/30\n",
      "  Reconstruction Loss: 15399124.0356\n",
      "  Regularization Loss: 52397.7036\n",
      "  Total Loss: 15451521.7392\n",
      "  Gradient X: mean=0.0102, std=1.6360\n",
      "  Gradient W: mean=0.0300, std=1.9909\n",
      "Epoch 15/30\n",
      "  Reconstruction Loss: 15151343.7818\n",
      "  Regularization Loss: 51905.5904\n",
      "  Total Loss: 15203249.3723\n",
      "  Gradient X: mean=-0.0093, std=1.6811\n",
      "  Gradient W: mean=-0.0338, std=1.9924\n",
      "Epoch 16/30\n",
      "  Reconstruction Loss: 15034587.7836\n",
      "  Regularization Loss: 53027.3244\n",
      "  Total Loss: 15087615.1080\n",
      "  Gradient X: mean=0.0154, std=1.7648\n",
      "  Gradient W: mean=0.0102, std=1.9958\n",
      "Epoch 17/30\n",
      "  Reconstruction Loss: 15034180.0652\n",
      "  Regularization Loss: 51980.7765\n",
      "  Total Loss: 15086160.8416\n",
      "  Gradient X: mean=-0.0174, std=1.8166\n",
      "  Gradient W: mean=-0.0419, std=1.9968\n",
      "Epoch 18/30\n",
      "  Reconstruction Loss: 14876976.1766\n",
      "  Regularization Loss: 53503.5225\n",
      "  Total Loss: 14930479.6991\n",
      "  Gradient X: mean=0.0204, std=1.8421\n",
      "  Gradient W: mean=0.0099, std=1.9972\n",
      "Epoch 19/30\n",
      "  Reconstruction Loss: 14819776.1583\n",
      "  Regularization Loss: 52333.9571\n",
      "  Total Loss: 14872110.1154\n",
      "  Gradient X: mean=-0.0172, std=1.8608\n",
      "  Gradient W: mean=-0.0395, std=1.9973\n",
      "Epoch 20/30\n",
      "  Reconstruction Loss: 14638390.5913\n",
      "  Regularization Loss: 54044.6126\n",
      "  Total Loss: 14692435.2039\n",
      "  Gradient X: mean=0.0212, std=1.8694\n",
      "  Gradient W: mean=0.0096, std=1.9974\n",
      "Epoch 21/30\n",
      "  Reconstruction Loss: 14554503.0340\n",
      "  Regularization Loss: 52871.9162\n",
      "  Total Loss: 14607374.9502\n",
      "  Gradient X: mean=-0.0148, std=1.8774\n",
      "  Gradient W: mean=-0.0395, std=1.9971\n",
      "Epoch 22/30\n",
      "  Reconstruction Loss: 14369264.4370\n",
      "  Regularization Loss: 54684.2447\n",
      "  Total Loss: 14423948.6817\n",
      "  Gradient X: mean=0.0199, std=1.8798\n",
      "  Gradient W: mean=0.0070, std=1.9975\n",
      "Epoch 23/30\n",
      "  Reconstruction Loss: 14280414.6061\n",
      "  Regularization Loss: 53525.1401\n",
      "  Total Loss: 14333939.7462\n",
      "  Gradient X: mean=-0.0116, std=1.8838\n",
      "  Gradient W: mean=-0.0404, std=1.9974\n",
      "Epoch 24/30\n",
      "  Reconstruction Loss: 14107399.2119\n",
      "  Regularization Loss: 55388.1271\n",
      "  Total Loss: 14162787.3390\n",
      "  Gradient X: mean=0.0176, std=1.8832\n",
      "  Gradient W: mean=0.0057, std=1.9975\n",
      "Epoch 25/30\n",
      "  Reconstruction Loss: 14023885.3412\n",
      "  Regularization Loss: 54233.9125\n",
      "  Total Loss: 14078119.2537\n",
      "  Gradient X: mean=-0.0078, std=1.8867\n",
      "  Gradient W: mean=-0.0414, std=1.9972\n",
      "Epoch 26/30\n",
      "  Reconstruction Loss: 13869279.3199\n",
      "  Regularization Loss: 56110.2552\n",
      "  Total Loss: 13925389.5750\n",
      "  Gradient X: mean=0.0152, std=1.8852\n",
      "  Gradient W: mean=0.0047, std=1.9976\n",
      "Epoch 27/30\n",
      "  Reconstruction Loss: 13793138.1456\n",
      "  Regularization Loss: 54958.0876\n",
      "  Total Loss: 13848096.2331\n",
      "  Gradient X: mean=-0.0049, std=1.8890\n",
      "  Gradient W: mean=-0.0421, std=1.9971\n",
      "Epoch 28/30\n",
      "  Reconstruction Loss: 13654131.4916\n",
      "  Regularization Loss: 56839.2799\n",
      "  Total Loss: 13710970.7715\n",
      "  Gradient X: mean=0.0125, std=1.8873\n",
      "  Gradient W: mean=0.0040, std=1.9977\n",
      "Epoch 29/30\n",
      "  Reconstruction Loss: 13580828.4218\n",
      "  Regularization Loss: 55694.2259\n",
      "  Total Loss: 13636522.6477\n",
      "  Gradient X: mean=-0.0026, std=1.8910\n",
      "  Gradient W: mean=-0.0424, std=1.9969\n",
      "Epoch 30/30\n",
      "  Reconstruction Loss: 13454377.7474\n",
      "  Regularization Loss: 57584.2980\n",
      "  Total Loss: 13511962.0455\n",
      "  Gradient X: mean=0.0105, std=1.8891\n",
      "  Gradient W: mean=0.0031, std=1.9975\n"
     ]
    }
   ],
   "source": [
    "#4. Start SGD loop\n",
    "\n",
    "# Initialize R_hat as the predicted matrix\n",
    "R_hat = np.ndarray(shape=(num_users, num_items))\n",
    "\n",
    "# Perform SGD for a number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the predicted matrix\n",
    "    R_hat = X @ W.T\n",
    "\n",
    "    # Compute the error matrix for observed entries only\n",
    "    E = np.multiply(mask, R - R_hat)\n",
    "\n",
    "    # Compute gradients with regularization\n",
    "    grad_X = -E @ W + reg_lambda * X\n",
    "    grad_W = -E.T @ X + reg_lambda * W\n",
    "\n",
    "    # Apply gradient clipping to avoid exploding updates\n",
    "    grad_X = np.clip(grad_X, -gradient_clip, gradient_clip)\n",
    "    grad_W = np.clip(grad_W, -gradient_clip, gradient_clip)\n",
    "\n",
    "    # Update X and W\n",
    "    X -= learning_rate * grad_X\n",
    "    W -= learning_rate * grad_W\n",
    "\n",
    "    # Compute the total loss\n",
    "    reconstruction_loss = np.sum(np.multiply(mask, E) ** 2)\n",
    "    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)\n",
    "    total_loss = reconstruction_loss + regularization_loss\n",
    "\n",
    "    # Append the total loss to the history\n",
    "    loss_history.append(total_loss)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Reconstruction Loss: {reconstruction_loss:.4f}\")\n",
    "    print(f\"  Regularization Loss: {regularization_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Check for NaN or Inf values in X or W\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(W)) or np.any(np.isinf(X)) or np.any(np.isinf(W)):\n",
    "        print(\"Numerical instability detected. Terminating training.\")\n",
    "        break\n",
    "\n",
    "    # Debugging: Check mean and std of gradients\n",
    "    print(f\"  Gradient X: mean={np.mean(grad_X):.4f}, std={np.std(grad_X):.4f}\")\n",
    "    print(f\"  Gradient W: mean={np.mean(grad_W):.4f}, std={np.std(grad_W):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, R_hat, user_item_matrix, movies, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommends movies for a given user based on the predicted matrix R_hat.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: ID of the user to whom the recommendations will be made.\n",
    "    - R_hat: Predicted user-item matrix (num_users x num_items).\n",
    "    - user_item_matrix: Original user-item matrix (Pandas DataFrame) with ratings.\n",
    "    - movies: DataFrame containing movie details (Movie_ID, Name, Year).\n",
    "    - top_n: Number of recommendations to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - recommendations: DataFrame containing the top_n recommended movies.\n",
    "    \"\"\"\n",
    "    # Map user_id to the corresponding index in R_hat\n",
    "    user_index = user_item_matrix.index.get_loc(user_id)\n",
    "    \n",
    "    # Get the predicted ratings for the user\n",
    "    predicted_ratings = R_hat[user_index]\n",
    "\n",
    "    # Get the user's original ratings\n",
    "    original_ratings = user_item_matrix.loc[user_id]\n",
    "\n",
    "    # Find movies that the user has not rated (those with a rating of 0)\n",
    "    unrated_movies = original_ratings[original_ratings == 0].index\n",
    "\n",
    "    # Map the unrated movies to the correct columns in R_hat\n",
    "    unrated_predictions = {\n",
    "        movie_id: predicted_ratings[user_item_matrix.columns.get_loc(movie_id)]\n",
    "        for movie_id in unrated_movies\n",
    "    }\n",
    "\n",
    "    # Sort the predicted ratings for unrated movies in descending order\n",
    "    sorted_predictions = sorted(unrated_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top_n movie IDs\n",
    "    top_movie_ids = [movie_id for movie_id, _ in sorted_predictions[:top_n]]\n",
    "\n",
    "    # Retrieve movie details for the top_n recommendations\n",
    "    recommendations = movies[movies['Movie_ID'].isin(top_movie_ids)]\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid the SettingWithCopyWarning\n",
    "    recommendations = recommendations.copy()\n",
    "    \n",
    "    # Add the Predicted_Rating column\n",
    "    recommendations['Predicted_Rating'] = [unrated_predictions[movie_id] for movie_id in recommendations['Movie_ID']]\n",
    "    \n",
    "    # Sort recommendations by predicted rating (optional, for clarity)\n",
    "    recommendations = recommendations.sort_values(by='Predicted_Rating', ascending=False)\n",
    "\n",
    "    return recommendations[['Movie_ID', 'Name', 'Year', 'Predicted_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing\n",
    "The function is tested with a sample user to generate personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for User 774868:\n",
      "                                       Name  Year  Predicted_Rating  Movie_ID\n",
      "2113                                Firefly  2002          5.917396      2114\n",
      "3045      The Simpsons: Treehouse of Horror  1990          5.825943      3046\n",
      "3443  Family Guy: Freakin' Sweet Collection  2004          5.824890      3444\n",
      "4237                              Inu-Yasha  2000          5.817704      4238\n",
      "2056     Buffy the Vampire Slayer: Season 6  2001          5.816484      2057\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the recommendation function\n",
    "user_id_to_test = 774868  # Cambia con un ID utente valido nel dataset\n",
    "num_recommendations = 5   # Numero di raccomandazioni desiderate\n",
    "\n",
    "try:\n",
    "    # Esegui la funzione di raccomandazione\n",
    "    recommendations = recommend_movies(user_id_to_test, R_hat, user_item_matrix, movies, top_n=num_recommendations)\n",
    "    \n",
    "    # Mostra i risultati\n",
    "    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test}:\")\n",
    "    print(recommendations[['Name', 'Year', 'Predicted_Rating', 'Movie_ID']])\n",
    "except KeyError as e:\n",
    "    print(f\"Error: User ID {user_id_to_test} not found in the dataset.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the user-item matrix into training and testing sets...Done.\n",
      "Converting the train matrix to a sparse matrix...Done.\n",
      "Initializing X and W matrices...Done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to split the user-item matrix into train and test sets\n",
    "def train_test_split_matrix(user_item_matrix, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the user-item matrix into training and testing matrices.\n",
    "\n",
    "    Parameters:\n",
    "    - user_item_matrix: Pandas DataFrame containing the user-item ratings.\n",
    "    - test_size: Proportion of the data to use for testing.\n",
    "    - random_state: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_matrix: Training matrix with some ratings removed (set to 0).\n",
    "    - test_matrix: Testing matrix with only the removed ratings (everything else is 0).\n",
    "    \"\"\"\n",
    "    # Flatten the matrix to get non-zero indices\n",
    "    non_zero_indices = np.array(user_item_matrix[user_item_matrix > 0].stack().index)\n",
    "    \n",
    "    # Split indices into train and test sets\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        non_zero_indices, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create boolean masks for train and test indices\n",
    "    train_mask = np.zeros(user_item_matrix.shape, dtype=bool)\n",
    "    test_mask = np.zeros(user_item_matrix.shape, dtype=bool)\n",
    "    \n",
    "    for user_id, movie_id in train_indices:\n",
    "        train_mask[user_item_matrix.index.get_loc(user_id), user_item_matrix.columns.get_loc(movie_id)] = True\n",
    "    \n",
    "    for user_id, movie_id in test_indices:\n",
    "        test_mask[user_item_matrix.index.get_loc(user_id), user_item_matrix.columns.get_loc(movie_id)] = True\n",
    "\n",
    "    # Create train and test matrices using the masks\n",
    "    train_matrix = user_item_matrix.values.copy()\n",
    "    test_matrix = user_item_matrix.values.copy()\n",
    "    \n",
    "    train_matrix[test_mask] = 0  # Set test indices to 0 in the train matrix\n",
    "    test_matrix[train_mask] = 0  # Set train indices to 0 in the test matrix\n",
    "\n",
    "    return pd.DataFrame(train_matrix, index=user_item_matrix.index, columns=user_item_matrix.columns), \\\n",
    "           pd.DataFrame(test_matrix, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "\n",
    "# Split the user-item matrix into training and testing sets\n",
    "print(\"Splitting the user-item matrix into training and testing sets...\", end=\"\")\n",
    "train_matrix, test_matrix = train_test_split_matrix(user_item_matrix)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Convert train matrix to sparse representation for SGD\n",
    "print(\"Converting the train matrix to a sparse matrix...\", end=\"\")\n",
    "sparse_train_matrix = csr_matrix(train_matrix.values)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Step 1: Initialize X and W matrices\n",
    "print(\"Initializing X and W matrices...\", end=\"\")\n",
    "num_users, num_items = train_matrix.shape\n",
    "X = np.random.normal(scale=1.0 / num_factors, size=(num_users, num_factors))\n",
    "W = np.random.normal(scale=1.0 / num_factors, size=(num_items, num_factors))\n",
    "print(\"Done.\")\n",
    "\n",
    "# Step 2: Train the model\n",
    "mask_train = train_matrix > 0  # Mask for observed entries in the train matrix\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the predicted matrix\n",
    "    R_hat_train = X @ W.T\n",
    "\n",
    "    # Compute the error matrix for observed entries only\n",
    "    E_train = np.multiply(mask_train, train_matrix.values - R_hat_train)\n",
    "\n",
    "    # Compute gradients with regularization\n",
    "    grad_X = -E_train @ W + reg_lambda * X\n",
    "    grad_W = -E_train.T @ X + reg_lambda * W\n",
    "\n",
    "    # Apply gradient clipping\n",
    "    grad_X = np.clip(grad_X, -gradient_clip, gradient_clip)\n",
    "    grad_W = np.clip(grad_W, -gradient_clip, gradient_clip)\n",
    "\n",
    "    # Update X and W\n",
    "    X -= learning_rate * grad_X\n",
    "    W -= learning_rate * grad_W\n",
    "\n",
    "    # Compute the total loss (ensure it is a scalar value)\n",
    "    reconstruction_loss = np.sum(np.multiply(mask_train, E_train) ** 2)  # Scalar\n",
    "    regularization_loss = reg_lambda * (np.linalg.norm(X, 'fro') ** 2 + np.linalg.norm(W, 'fro') ** 2)  # Scalar\n",
    "    total_loss = reconstruction_loss + regularization_loss  # Scalar\n",
    "    train_losses.append(total_loss)\n",
    "\n",
    "    # Print the epoch progress and loss\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Step 3: Evaluate the model on the test set\n",
    "# Predict the ratings for the entire matrix\n",
    "R_hat = X @ W.T\n",
    "\n",
    "# Extract the predicted ratings for the test set\n",
    "mask_test = test_matrix > 0  # Mask for observed entries in the test matrix\n",
    "E_test = np.multiply(mask_test, test_matrix.values - R_hat)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) for the test set\n",
    "test_mse = mean_squared_error(test_matrix.values[mask_test], R_hat[mask_test])\n",
    "print(f\"\\nTest MSE: {test_mse:.4f}\")\n",
    "\n",
    "# Step 4: Plot the training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
