{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering for Movie Recommendations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Collaborative filtering is a popular approach in recommendation systems that leverages user-item interactions to predict user preferences. There are two main types of collaborative filtering:\n",
    "\n",
    "- **User-Based Collaborative Filtering**: Recommends items based on the preferences of users who are similar to the target user.\n",
    "- **Item-Based Collaborative Filtering**: Suggests items that are similar to those the user has already rated positively.\n",
    "\n",
    "In this notebook, we will explore both approaches for movie recommendations.\n",
    "\n",
    "## Dataset Description\n",
    "We use two datasets for this analysis:\n",
    "1. **Movies Dataset**:\n",
    "   - `Movie_ID`: Unique identifier for each movie.\n",
    "   - `Name`: Title of the movie.\n",
    "   - `Year`: Release year of the movie.\n",
    "\n",
    "2. **Ratings Dataset**:\n",
    "   - `User_ID`: Unique identifier for each user.\n",
    "   - `Movie_ID`: Identifier for the movie rated.\n",
    "   - `Rating`: Numeric rating given by the user (e.g., on a scale of 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the dataset\n",
    "ratings = pd.read_csv(\"./data/Netflix_Dataset_Rating.csv\")  # Columns: User_ID, Rating, Movie_ID\n",
    "movies  = pd.read_csv(\"./data/Netflix_Dataset_Movie.csv\")    # Columns: Movie_ID, Year, Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "- $U = \\{u_1, u_2, \\dots, u_n\\}$ is the set of users.\n",
    "- $U_i = \\{u \\in U \\mid r_{u,i} \\neq 0\\}$ is the set of users who have rated item $i$\n",
    "- $I = \\{i_1, i_2, \\dots, i_m\\}$ is the set of items.\n",
    "- $I_u = \\{i \\in I \\mid r_{u,i} \\neq 0\\}$ is the set of items rated by user $u$\n",
    "- $R = \\{0, 1, \\dots, 5\\} \\lor R = [0, 1]$ is the set of ratings.\n",
    "- $r_{u,i}$ is the rating given by user $u$ for item $i$ (equal to 0 if not rated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **User-Item Matrix Creation**: Convert the ratings dataset into a user-item matrix, where rows represent users and columns represent movies. Missing ratings are filled with zeros. Each rating is represented by a number from 1 to 5.\n",
    "  $$ M[u, i] = r_{u,i} \\in R$$\n",
    "  Where:\n",
    "  - $u \\in U$ is the set of users.\n",
    "  - $i \\in I$ is the set of movies.\n",
    "  - $r_{u,i}$ is the rating given by user $u$ for movie $i$.\n",
    "\n",
    "- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format for memory optimization:\n",
    "  $$M_{\\{\\text{sparse}\\}} = \\text{sparse}(M)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a user-item matrix\n",
    "user_item_matrix = ratings.pivot(index='User_ID', columns='Movie_ID', values='Rating')\n",
    "\n",
    "# Fill missing values with 0 (can use NaN for some algorithms)\n",
    "user_item_matrix.fillna(0, inplace=True) # It is not the case for this dataset\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collaborative Filtering Model\n",
    "We use the `scikit-learn` library to implement a KNN-based model:\n",
    "\n",
    "1. **Similarity Metric**: Cosine similarity is used to identify users with similar rating patterns. The formula for cosine similarity between two users $u$ and $v$ is:\n",
    "   $$\n",
    "   \\text{sim}(u, v) = \\frac{\\vec r_u \\cdot \\vec r_v}{\\|\\vec r_u\\| \\cdot \\|\\vec r_v\\|}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\vec r_u$ and $\\vec r_v$ are column vectors of ratings for users $u$ and $v$ respectively (the $u$-th and $v$-th rows of the user-item matrix).\n",
    "   - $\\cdot$ represents the dot product.\n",
    "\n",
    "2. **Nearest Neighbors**: The model identifies the top $k$ nearest neighbors for each user, based on the similarity scores:\n",
    "   $$\\large\n",
    "   \\mathcal{N}_u^k = \\argmax_{U' \\subseteq U \\setminus \\{u\\} \\land |U'| = k} \\sum_{v \\in U'} \\text{sim}(u, v)\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\mathcal N_u^k$ is the set of the top $k$ nearest neighbors for user $u$.\n",
    "   - $U'$ is a subset of users excluding $u$.\n",
    "   - $k = 10$ (default value in this implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the collaborative filtering model\n",
    "model_knn_user = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recommendation Function (User-Based)\n",
    "\n",
    "In the user-based collaborative filtering approach, the recommendation function operates as follows:\n",
    "\n",
    "1. **Get all relevant items**: The set of items rated by the top $k$ nearest neighbors for user $u$:\n",
    "$$\n",
    "I^k = \\{i \\in I \\setminus I_u \\mid \\exists v \\in \\mathcal N_u^k : r_{v,i} \\neq 0\\}\n",
    "$$\n",
    "Where:\n",
    "- $\\mathcal N_u^k$ is the set of the top $k$ nearest neighbors for user $u$.\n",
    "- $I^k$ is the set of items rated by the top $k$ nearest neighbors for user $u$ and not already rated by $u$.\n",
    "- $k = 10$ (default value in this implementation).\n",
    "\n",
    "1. **Prediction Formula**: For a given user $u$, the set of predicted ratings for items $i \\in I^k$ is calculated as:\n",
    "   $$\\large\n",
    "   I^k_u = \\{\\hat{r}_{u,i} \\mid i \\in I^k\\} \\quad \\text{where} \\quad\n",
    "   \\hat{r}_{u,i} = \\frac{\\sum_{v \\in \\mathcal N_u^k \\land i \\in I^k} \\text{sim}(u, v) \\cdot r_{v,i}}{\\sum_{v \\in \\mathcal N_u^k} \\text{sim}(u, v)}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\hat{r}_{u,i}$ is the predicted rating for user $u$ on item $i$.\n",
    "   - $r_{v,i}$ is the rating of neighbor $v \\in \\mathcal N_u^k$ for item $i$.\n",
    "   - $\\text{sim}(u, v)$ is the similarity between users $u$ and $v$.\n",
    "\n",
    "This combination of steps allows the model to efficiently generate user-based collaborative filtering recommendations using nearest neighbors. This approach works well in cases where users with similar preferences exist in the dataset. For example, if two users have rated several movies similarly, the model assumes they will likely share preferences for other movies as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define a rating and a recommendation function\n",
    "def recommend_movies_user_based(user_id, train_matrix, num_recommendations=5):\n",
    "    \"\"\"Recommend movies using user-based collaborative filtering.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "\n",
    "    # Get the user vector\n",
    "    user_idx = user_item_matrix.index.get_loc(user_id)\n",
    "    user_vector = train_matrix[user_idx]\n",
    "\n",
    "    # Train the model\n",
    "    model_knn_user.fit(train_matrix)\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    distances, indices = model_knn_user.kneighbors(user_vector, n_neighbors=10 + 1)  # +1 to exclude the user itself\n",
    "\n",
    "    # Filter out the user itself (distance = 0)\n",
    "    neighbors = indices.flatten()[1:]\n",
    "    similarity_scores = 1 - distances.flatten()[1:]  # Convert distances to similarity scores\n",
    "\n",
    "    # Get movies rated by similar users and calculate the weighted average rating\n",
    "    similar_users_ratings = user_item_matrix.iloc[neighbors]\n",
    "    similar_users_ratings = similar_users_ratings.loc[:, user_item_matrix.loc[user_id] == 0]  # Exclude movies rated by the user\n",
    "    weighted_ratings = (similar_users_ratings.T * similarity_scores).T  # Apply weights\n",
    "    mean_ratings = weighted_ratings.sum(axis=0) / similarity_scores.sum()\n",
    "\n",
    "    # Sort by rating and get the top recommendations\n",
    "    recommended_movies = mean_ratings.sort_values(ascending=False).head(num_recommendations)\n",
    "\n",
    "    # Map movie IDs to names\n",
    "    recommended_movies = recommended_movies.reset_index()\n",
    "    recommended_movies.columns = ['Movie_ID', 'Estimated_Rating']\n",
    "    recommendations = recommended_movies.merge(movies, on='Movie_ID')[['Movie_ID', 'Name', 'Year', 'Estimated_Rating']]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing (User-Based)\n",
    "The function is tested with a sample user to generate personalized recommendations (User-Based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for User 774868:\n",
      "                                                Name  Year  Estimated_Rating  \\\n",
      "0      Lord of the Rings: The Fellowship of the Ring  2001          3.916104   \n",
      "1  Pirates of the Caribbean: The Curse of the Bla...  2003          3.209660   \n",
      "2                               Bend It Like Beckham  2002          2.741444   \n",
      "3                                  Princess Mononoke  1997          2.703218   \n",
      "4                          Finding Nemo (Widescreen)  2003          2.697026   \n",
      "\n",
      "   Movie_ID  \n",
      "0      2452  \n",
      "1      1905  \n",
      "2      1470  \n",
      "3       473  \n",
      "4      3962  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the recommendation function\n",
    "user_id_to_test = 774868 #1331154  # Change as needed\n",
    "num_recommendations = 5\n",
    "\n",
    "try:\n",
    "    user_based_recommendations = recommend_movies_user_based(user_id_to_test, sparse_user_item, num_recommendations=num_recommendations)\n",
    "    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test}:\")\n",
    "    print(user_based_recommendations[['Name', 'Year', 'Estimated_Rating', 'Movie_ID']])\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Based Collaborative Filtering\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **Item-User Matrix Creation**: Transpose the user-item matrix $M$ to create a new matrix $M' = M^T$, where rows represent movies and columns represent users. Missing ratings are filled with zeros.\n",
    "  $$ M'[i, u] = M[u, i] = r_{u,i} \\in R$$\n",
    "- **Sparse Matrix Conversion**: The dense matrix is converted to a sparse format to optimize memory usage.\n",
    "  $$M'_{\\{\\text{sparse}\\}} = \\text{csr\\_matrix}(M')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transpose the user-item matrix\n",
    "item_user_matrix = user_item_matrix.T\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix\n",
    "sparse_item_user = csr_matrix(item_user_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collaborative Filtering Model (Item-Based)\n",
    "We use the `scikit-learn` library to implement a KNN-based model:\n",
    "\n",
    "1. **Similarity Metric**: Cosine similarity is used to identify movies with similar rating patterns. The formula for cosine similarity between two movies $i$ and $j$ is:\n",
    "   $$\n",
    "   \\text{sim}(i, j) = \\frac{\\vec r_i \\cdot \\vec r_j}{\\|\\vec r_i\\| \\cdot \\|\\vec r_j\\|}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\vec r_i$ and $\\vec r_j$ are column vectors of user ratings for movies $i$ and $j$ respectively (the $i$-th and $j$-th rows of the item-user matrix $M'$).\n",
    "   - $\\cdot$ represents the dot product.\n",
    "\n",
    "2. **Nearest Neighbors**: The model identifies the top $k$ nearest neighbors for each movie based on similarity scores:\n",
    "   $$\\large\n",
    "   \\mathcal{N}_i^k = \\argmax_{I'_u \\subseteq I_u \\setminus \\{i\\} \\land |I'_u| = k} \\sum_{j \\in I'_u} \\text{sim}(i, j)\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\mathcal N_i^k$ is the set of the top $k$ nearest neighbors (in the user $u$ rated movies set) for movie $i$.\n",
    "   - $I'_u$ is a subset, of size $k$, of items rated by user $u$ that excludes movie $i$, obviously.\n",
    "   - $k = 10$ (default value in this implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the item-based collaborative filtering model\n",
    "model_knn_item = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10)\n",
    "\n",
    "# Create the sparse matrix for the item-based model\n",
    "sparse_item_user = csr_matrix(item_user_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recommendation Function (Item-Based)\n",
    "\n",
    "In the item-based collaborative filtering approach, the recommendation function works as follows:\n",
    "\n",
    "Predict movie ratings for the target user by aggregating ratings from similar movies. For a given user $u$ and an item $i$ not yet rated by $u$, the predicted rating is calculated as:\n",
    "   $$\\large\n",
    "   I^k_u = \\{\\hat{r}_{u,i} \\mid i \\in I \\setminus I_u \\setminus \\{i\\}\\} \\quad \\text{where} \\quad\n",
    "   \\hat{r}_{u,i} = \\frac{\\sum_{j \\in \\mathcal N_i^k} \\text{sim}(i, j) \\cdot r_{u,j}}{\\sum_{j \\in \\mathcal N_i^k} \\text{sim}(i, j)}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\hat{r}_{u,i}$ is the predicted rating for user $u$ on item $i$.\n",
    "   - $r_{u,j}$ is the rating of user $u$ for a similar item $j \\in \\mathcal N_i^k$.\n",
    "   - $\\text{sim}(i, j)$ is the similarity between items $i$ and $j$.\n",
    "   - $\\mathcal N_i^k$ is the set of the top $k$ most similar items to $i$.\n",
    "   - $I^k_u$ is the set of predicted ratings for user $u$ on items not yet rated by $u$.\n",
    "\n",
    "Thus, for each movie not rated by the target user, the model calculates a weighted average of similar movies' ratings based on their similarity scores. The predicted ratings are then sorted and the top are returned.\n",
    "\n",
    "This method leverages the user's own preferences and the similarity relationships between movies, providing relevant and personalized suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Recommendation Function (Item-Based)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def recommend_movies_item_based(user_id, train_matrix, num_recommendations=5):\n",
    "    \"\"\"Recommend movies using item-based collaborative filtering for a given user.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "\n",
    "    # Get the user's ratings\n",
    "    user_ratings = item_user_matrix.loc[:, user_id].to_numpy()\n",
    "    rated_movies = np.where(user_ratings > 0)[0]  # Indices of movies the user has rated\n",
    "\n",
    "    if len(rated_movies) == 0:\n",
    "        raise ValueError(\"User has not rated any movies\")\n",
    "\n",
    "    # Initialize a dictionary to store weighted scores\n",
    "    movie_scores = {}\n",
    "\n",
    "    # Fit the model with the sparse item-user matrix limited to the rated movies\n",
    "    sparse_item_user_u = train_matrix[rated_movies, :]\n",
    "    model_knn_item.fit(sparse_item_user_u)\n",
    "\n",
    "    # Get unseen movies\n",
    "    not_rated_movies = item_user_matrix.index.difference(item_user_matrix.index[rated_movies])\n",
    "\n",
    "    # Iterate over all movies not rated by the user\n",
    "    for movie_id in tqdm(not_rated_movies, \n",
    "                         desc=\"Processing Movies\", unit=\"movie\"):\n",
    "        # Get the vector for the movie\n",
    "        movie_idx = item_user_matrix.index.get_loc(movie_id)\n",
    "        movie_vector = train_matrix[movie_idx, :]\n",
    "\n",
    "        # Find nearest neighbors for the movie\n",
    "        distances, indices = model_knn_item.kneighbors(movie_vector, n_neighbors=10)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "        # Get the indexes of the similar movies\n",
    "        similar_movies = rated_movies[indices.flatten()]\n",
    "\n",
    "        # Calculate the weighted average score for the movie using similarity scores\n",
    "        user_ratings_for_similar = user_ratings[similar_movies]\n",
    "        movie_scores[movie_id] = np.dot(similarity_scores, user_ratings_for_similar) / similarity_scores.sum()\n",
    "\n",
    "    # Sort movies by aggregated score\n",
    "    recommended_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)[:num_recommendations]\n",
    "\n",
    "    # Normalize the scores\n",
    "    recommended_movies = [(movie_id, score) for movie_id, score in recommended_movies]\n",
    "\n",
    "    # Map movie IDs to names\n",
    "    recommended_movies_df = pd.DataFrame(recommended_movies, columns=['Movie_ID', 'Estimated_Rating'])\n",
    "    recommendations = recommended_movies_df.merge(movies, on='Movie_ID')[['Movie_ID', 'Name', 'Year', 'Estimated_Rating']]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Recommendation Function (Item-Based)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def recommend_movies_item_based(user_id, train_matrix, num_recommendations=5):\n",
    "    \"\"\"Recommend movies using item-based collaborative filtering for a given user.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "\n",
    "    # Get the user's ratings\n",
    "    user_ratings = item_user_matrix.loc[:, user_id].to_numpy()\n",
    "    rated_movies = np.where(user_ratings > 0)[0]  # Indices of movies the user has rated\n",
    "\n",
    "    if len(rated_movies) == 0:\n",
    "        raise ValueError(\"User has not rated any movies\")\n",
    "\n",
    "    # Initialize a dictionary to store weighted scores\n",
    "    movie_scores = {}\n",
    "\n",
    "    # Fit the model with the sparse item-user matrix limited to the rated movies\n",
    "    sparse_item_user_u = train_matrix[rated_movies, :]\n",
    "    model_knn_item.fit(sparse_item_user_u)\n",
    "\n",
    "    # Get unseen movies\n",
    "    not_rated_movies = item_user_matrix.index.difference(item_user_matrix.index[rated_movies])\n",
    "\n",
    "    # Iterate over all movies not rated by the user\n",
    "    for movie_id in tqdm(not_rated_movies, \n",
    "                         desc=\"Processing Movies\", unit=\"movie\"):\n",
    "        # Get the vector for the movie\n",
    "        movie_idx = item_user_matrix.index.get_loc(movie_id)\n",
    "        movie_vector = train_matrix[movie_idx, :]\n",
    "\n",
    "        # Find nearest neighbors for the movie\n",
    "        distances, indices = model_knn_item.kneighbors(movie_vector, n_neighbors=10)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "        # Get the indexes of the similar movies\n",
    "        similar_movies = rated_movies[indices.flatten()]\n",
    "\n",
    "        # Calculate the weighted average score for the movie using similarity scores\n",
    "        user_ratings_for_similar = user_ratings[similar_movies]\n",
    "        movie_scores[movie_id]    = []\n",
    "        movie_scores[movie_id].append(np.dot(similarity_scores, user_ratings_for_similar))\n",
    "\n",
    "        # Store the similarity sum for normalization\n",
    "        movie_scores[movie_id].append(similarity_scores.sum())\n",
    "\n",
    "    # Sort movies by aggregated score\n",
    "    recommended_movies = sorted(movie_scores.items(), key=lambda x: x[1][0], reverse=True)[:num_recommendations]\n",
    "\n",
    "    # Normalize the scores\n",
    "    recommended_movies = [(movie_id, score[0] / score[1]) for movie_id, score in recommended_movies]\n",
    "\n",
    "    # Map movie IDs to names\n",
    "    recommended_movies_df = pd.DataFrame(recommended_movies, columns=['Movie_ID', 'Estimated_Rating'])\n",
    "    recommendations = recommended_movies_df.merge(movies, on='Movie_ID')[['Movie_ID', 'Name', 'Year', 'Estimated_Rating']]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing\n",
    "The function is tested with a sample user to generate personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Movies: 100%|██████████| 1331/1331 [00:09<00:00, 136.26movie/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for User 774868 (Item-Based):\n",
      "                                                Name  Year  Estimated_Rating  \\\n",
      "0  Pirates of the Caribbean: The Curse of the Bla...  2003          4.816355   \n",
      "1      Lord of the Rings: The Fellowship of the Ring  2001          4.809336   \n",
      "2                                    The Sixth Sense  1999          4.808670   \n",
      "3                                    American Beauty  1999          4.761287   \n",
      "4                          Finding Nemo (Widescreen)  2003          4.817077   \n",
      "\n",
      "   Movie_ID  \n",
      "0      1905  \n",
      "1      2452  \n",
      "2      4306  \n",
      "3       571  \n",
      "4      3962  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the recommendation function\n",
    "user_id_to_test = 774868#1331154  # Change as needed\n",
    "num_recommendations = 5\n",
    "\n",
    "try:\n",
    "    item_based_recommendations = recommend_movies_item_based(user_id_to_test, sparse_item_user, num_recommendations=num_recommendations)\n",
    "    print(f\"Top {num_recommendations} movie recommendations for User {user_id_to_test} (Item-Based):\")\n",
    "    print(item_based_recommendations[['Name', 'Year', 'Estimated_Rating', 'Movie_ID']])\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation of the Model\n",
    "\n",
    "To assess the effectiveness of the collaborative filtering approach, we perform a train-test split on the ratings data. The evaluation process includes:\n",
    "\n",
    "1. **Train-Test Split**:  \n",
    "   - 80% of the data is used for training the model.\n",
    "   - 20% of the data is reserved for testing.\n",
    "\n",
    "2. **Predictions and Metrics**:  \n",
    "   - For each user in the test set, the model predicts ratings for movies based on the nearest neighbors identified in the training data.\n",
    "   - **Mean Absolute Error (MAE)** is calculated as the primary metric to evaluate prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_single(user_id, movie_id, train_matrix):\n",
    "    \"\"\"Estimate the rating for a specific user and movie using user-based collaborative filtering.\"\"\"\n",
    "\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "    \n",
    "    # Get the user vector\n",
    "    user_idx = user_item_matrix.index.get_loc(user_id)\n",
    "    user_vector = sparse_user_item[user_idx]\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    distances, indices = model_knn_user.kneighbors(user_vector, n_neighbors=10)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "    # Get the vector for the movie\n",
    "    movie_idx = user_item_matrix.index.get_loc(movie_id)\n",
    "    movie_vector = train_matrix[movie_idx, :]\n",
    "\n",
    "    # Find nearest neighbors for the movie\n",
    "    distances, indices = model_knn_user.kneighbors(movie_vector, n_neighbors=10)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "    # Get the indexes of the similar users\n",
    "    similar_users = indices.flatten()\n",
    "\n",
    "    # Calculate the weighted average score for the movie using similarity scores\n",
    "    user_ratings_for_similar = user_item_matrix.iloc[similar_users].to_numpy()[:, movie_idx]\n",
    "    weighted_ratings = (user_ratings_for_similar * similarity_scores).sum() / similarity_scores.sum()\n",
    "\n",
    "    return weighted_ratings\n",
    "\n",
    "def item_based_single(user_id, movie_id, train_matrix):\n",
    "    \"\"\"Estimate the rating for a specific user and movie using item-based collaborative filtering.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        raise ValueError(\"User ID not found in the dataset\")\n",
    "    \n",
    "    # Get the user's ratings\n",
    "    user_ratings = item_user_matrix.loc[:, user_id].to_numpy()\n",
    "    rated_movies = np.where(user_ratings > 0)[0]  # Indices of movies the user has rated\n",
    "\n",
    "    if len(rated_movies) == 0:\n",
    "        raise ValueError(\"User has not rated any movies\")\n",
    "\n",
    "    # Fit the model with the sparse item-user matrix limited to the rated movies\n",
    "    sparse_item_user_u = train_matrix[rated_movies, :]\n",
    "    model_knn_item.fit(sparse_item_user_u)\n",
    "\n",
    "    # Get the vector for the movie\n",
    "    movie_idx = item_user_matrix.index.get_loc(movie_id)\n",
    "    movie_vector = train_matrix[movie_idx, :]\n",
    "\n",
    "    # Find nearest neighbors for the movie\n",
    "    distances, indices = model_knn_item.kneighbors(movie_vector, n_neighbors=10)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = 1 - distances.flatten()\n",
    "\n",
    "    # Get the indexes of the similar movies\n",
    "    similar_movies = rated_movies[indices.flatten()]\n",
    "\n",
    "    # Calculate the weighted average score for the movie using similarity scores\n",
    "    user_ratings_for_similar = user_ratings[similar_movies]\n",
    "\n",
    "    return np.dot(similarity_scores, user_ratings_for_similar) / similarity_scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset...Done\n",
      "Creating train and test matrices...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating User-Based Model:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607403\n",
      "Cia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoot Mean Squared Error (RMSE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Example Evaluation\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecommend_movies_user_based\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_user_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-Based\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m evaluate_model(recommend_movies_item_based, sparse_item_user, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItem-Based\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[126], line 40\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(recommendation_function, sparse_train_matrix, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m movie_id \u001b[38;5;129;01min\u001b[39;00m recommended_movie_ids:\n\u001b[1;32m     39\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(recommendations\u001b[38;5;241m.\u001b[39mloc[recommendations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMovie_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m movie_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstimated_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 40\u001b[0m         true_ratings\u001b[38;5;241m.\u001b[39mappend(\u001b[43muser_test_ratings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmovie_id\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     42\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(true_ratings, predictions)\n\u001b[1;32m     43\u001b[0m rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(true_ratings, predictions, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluation (Optional)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split the ratings data\n",
    "print(\"Splitting the dataset...\", end=\"\")\n",
    "train_data, test_data = train_test_split(\n",
    "    ratings[:10000], # Filter the first 100000 rows  \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"Done\")\n",
    "\n",
    "# Create train and test user-item matrices\n",
    "print(\"Creating train and test matrices...\", end=\"\")\n",
    "user_item_matrix = ratings[:10000].pivot(index='User_ID', columns='Movie_ID', values='Rating').fillna(0)\n",
    "sparse_user_item = csr_matrix(user_item_matrix.values)\n",
    "item_user_matrix = user_item_matrix.T\n",
    "sparse_item_user = csr_matrix(item_user_matrix.values)\n",
    "test_matrix  = test_data#.pivot(index='User_ID', columns='Movie_ID', values='Rating').fillna(0)\n",
    "print(\"Done\")\n",
    "\n",
    "def evaluate_model(recommendation_function, sparse_train_matrix, name='Model'):\n",
    "    \"\"\"Evaluate a recommendation function using MAE and RMSE.\"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    true_ratings = []\n",
    "\n",
    "    for user_id in tqdm(test_matrix.User_ID, desc=\"Evaluating \" + name + \" Model\"):\n",
    "        print(user_id)\n",
    "        recommendations = recommendation_function(user_id, sparse_train_matrix, num_recommendations=10)\n",
    "        recommended_movie_ids = recommendations['Movie_ID']\n",
    "        user_test_ratings = test_matrix[test_matrix['User_ID'] == user_id].set_index('Movie_ID')['Rating']\n",
    "\n",
    "        print(\"Cia\")\n",
    "\n",
    "        for movie_id in recommended_movie_ids:\n",
    "            predictions.append(recommendations.loc[recommendations['Movie_ID'] == movie_id, 'Estimated_Rating'].values[0])\n",
    "            true_ratings.append(user_test_ratings[movie_id])\n",
    "            \n",
    "    mae = mean_absolute_error(true_ratings, predictions)\n",
    "    rmse = mean_squared_error(true_ratings, predictions, squared=False)\n",
    "\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Example Evaluation\n",
    "evaluate_model(recommend_movies_user_based, sparse_user_item, name='User-Based')\n",
    "evaluate_model(recommend_movies_item_based, sparse_item_user, name='Item-Based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "607403 in user_item_matrix.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrated the implementation of a collaborative filtering approach for recommending movies. Key takeaways include:\n",
    "\n",
    "- The model effectively utilizes user similarity to make recommendations, as shown by the ability to generate relevant suggestions for a sample user.\n",
    "- The **MAE metric** provides a reliable evaluation of the model's predictive accuracy.\n",
    "- While collaborative filtering is powerful, it faces challenges such as:\n",
    "  - **Cold Start Problem**: Difficulty in recommending movies for new users or items.\n",
    "  - **Data Sparsity**: Limited interactions in the dataset can affect similarity computations.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "To address the limitations, potential enhancements include:\n",
    "\n",
    "- Implementing hybrid recommendation systems that combine collaborative and content-based filtering.\n",
    "- Exploring matrix factorization techniques (e.g., Singular Value Decomposition).\n",
    "- Integrating deep learning-based recommendation methods.\n",
    "\n",
    "This collaborative filtering approach forms a solid foundation for building scalable and effective recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
